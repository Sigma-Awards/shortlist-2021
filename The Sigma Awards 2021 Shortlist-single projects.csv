Country,Publishing organisations,Organisation size,Project title,Tags,Technologies tools used,A short description of the project,What was the impact of the project,What tools techniques technologies did you use and how did you use them,What was the hardest part of this project What should the jury know to better understand what you did and why it should be selected ,What can other journalists learn from this project,Project link 1,Project link 2,Project link 3,Project link 4,Project link 5,Project link 6,Project link 7,Project link 8,Who made this project,Short biography/ies,Publication date
El Salvador,"elsalvador.com, El Diario de Hoy",Small,Rostros y voces del conflicto,"Investigation, Documentary, Database, Video, Map, Crime, Human rights","Scraping, Google Sheets, CSV, Python","     Faces and voices of the conflict is a digital memorial that, after the creation of two own databases on war crimes and serious human rights violations that occurred in the eighties, presents them on an interactive map together with videos of testimonies of victims and survivors.   The ""victims"" and ""events"" databases were constructed after consulting the editions of four newspapers published between 1972 and 1992 and include crimes against humanity and war crimes attributed to the army and the guerrillas.    Publications from organizations related to the victims, guerrilla and the army were also consulted.  "," Faces and voices of the conflict is a living project, constantly updated. After its launch, dozens of users wrote to us to share more information or to make small corrections to the published information. This has not served to contact new sources and publish new stories.  Another impact has been that, by being published by a media outlet that had a very right-wing position during the armed conflict, it has attracted the attention of readers and users who generally do not read and do not think about the human rights violated during the war of the 80's.  Politicians, relatives of victims of the army or the guerrillas, academics and human rights defenders have welcomed this data project.  From its launch on September 16th to Janyary 16th, the site had 155,542 page views and amassed 104,879 users.  After a general amnesty law was declared unconstitutional and the door was opened for war crimes committed in the 1980s to be brought to trial, the issue of restorative justice and historical memory has risen to prominence on the national agenda. Faces and Voices of Conflict has brought information never before collected in one place to that debate."," A team of five people reviewed four different print newspapers from 1972 to 1992. The information we found on human rights violations and possible crimes against humanity and war crimes, we aggregated into Google spreadsheets. To this information was added the one that was in a monument in honor of the victims of the war in the Cuscatlán park in San Salvador.   In Cuscatlan Park there is a monument which contains a list of 30,000 names of those who died in El Salvador's Civil War between 1980 and 1992. A El Diario de Hoy photographer took a high resolution photo of every element of the monument.   After that, using the OpenCV library we applied a a threshold treatment to every image and then used a tool named Tesseract to recognize the names.   Them, using a Python programming language library named Pillow, every picture was cutted and the results systematized in CSV files, which were united for storing.    We also use Flourish Studio to present the data from the ""victims"" database in a table where users can search for their relatives by typing their name."," The hardest part of the job was turning that data into history. It was very difficult to contact relatives of the victims or survivors of massacres willing to give their testimonies before a video camera. For our part, we prepare to address the victims and to accept when they identify some part of the story that they do not want to share.  The pandemic and the 80-day quarantine in El Salvador made this job even more difficult."," What other journalists can learn from this project is how the planning and work of a multidisciplinary team with a different vision of an event can enrich and carry out a journalistically ambitious task. Building a database with the serious human rights violations, crimes against humanity and war crimes committed by the State or by the guerrillas between 1972 and 1992 seemed an impossible task. But as a team it was decided what information needed to be collected and where to look for it. The editor sought the support of interns whom she trained to collaborate in the review of hundreds of pages of newspapers from which the information that was added to the databases was extracted.  The work could not have been carried out without this help. It was vital and important to have a methodology for external partners to join in the data collection task. The project started in June 2019 with planning; Data collection started in November 2019 and concluded in March 2020. Then came reviewing, cleaning and data analysis. For each task there was a team leader. Simultaneously, based on the data, relatives of victims were contacted to collect their testimonies and present them in videos and podcasts that are presented and added to the site as they occur.Finally, what other journalists can learn from this project is the need to have at least one person who knows a programming language like Python for data extraction and cleaning.",https://rostrosyvocesdelconflicto.elsalvador.com/,https://www.facebook.com/watch/?v=648892162732791,https://www.facebook.com/watch/?v=1715078788668696,,,,,,"Lilian Martínez, Karla Arévalo, Xenia González Oliva, Carlos Palomo, Vladimir Bonilla, Claudia Zaldaña"," Lilian Martínez (47) is a journalist with more than 20 years of experience. Working in her field since 1998. She joined the editorial staff of El Diario de Hoy in June 2001. Since then, she has developed different positions in the newspaper. In August 2017, she became the editor of the Data Unit of El Diario de Hoy, from where, together with young journalists, she assumed the task of not only doing data journalism but also of combining this with the stories and the people who live in their own flesh what the data reveal. It is there, in this Data Unit, where the project ""Conflict Faces and Voices"" is gestated, an ambitious work that aims to compile the human rights violations, war crimes and crimes against humanity committed between 1972 and 1992 in El Salvador.  Karla Arévalo (26) is a data journalist in the Data Unit of El Diario de Hoy in El Salvador. She writes reports about homicidal violence and environment. She works with open data or request information through the Law of Access to Public Information. In addition to cleaning, analyzing and disseminating these stories, she also uses different formats to present the stories to readers. On April of this year, she documented the hydrological crisis in the western El Salvador. Arévalo is graduated with a degree in Communication Sciences, specializing in journalism, from the Dr. José Matías Delgado University in El Salvador.  Xenia González Oliva (29) Xenia González Oliva is a journalist based in El Salvador. Currently, she worked on El Diario de Hoy’s data unit three years, where she has published stories relating to health issues including teen pregnancies, the sexual abuse of children, and chronic renal failure in the country. She participated in ICFJ’s Professional Fellows program and supported a project published in ProPublica.  Carlos Eduardo Palomo (26) is a salvadoran engineer who has experience working as a data analyst and designing web visualizations. He has a background in statistics, business intelligence and computer networks.  Carlos worked in the Data Unit inside El Diario de Hoy, one of the principals newspaper in El Salvador. While he was in El Diario de Hoy, he centered his activities in developing algorithms to digitalize data contained in physical media and using statistical techniques to analyze that data. ",16 Sep 2020
Netherlands,"Pointer (KRO-NCRV), De Monitor (KRO-NCRV), Follow The Money",Small,The Real Estate Books of the German Occupiers,"Investigation, Explainer, Long-form, Documentary, Open data, Fact-checking, OSINT, Illustration, Infographics, Chart, Video, Map","Animation, D3.js, Canvas, Microsoft Excel, Google Sheets, CSV, R, RStudio, OpenStreetMap","During World War II, the german occupiers made a registration of stolen Jewish real estate in the Netherlands. These books were digitalized recently. Pointer researched more than 7.000 transactions of disowned and sold Jewish property. We're telling the stories of persons who came back from the war and found their homes occupied by the new owners. A lot of people ea We found that a lot of municipalities never researched their own shady past. More than 7.000 transactions of disowned and sold property could be researched. We also want to submit De Straat Die Niet Meer Bestaat as a separate"," Because of our research, municipalities are starting to research their own role in the real estate theft. Before our research, just 3 had concluded their research. Thanks to our publications, we're at 33 (of the possible 226) municipalities. These researches will conclude somewhere in 2021, and more municipalities are ready to follow their example. These investigations can lead to excuses by city councils, and monetary compensations to Jewish organisations and individuals.   We're telling these stories in the 75th year of the Dutch liberation. The second world war seems a thing far in the past, but we've made these stories relevant by looking at the current consequences of this theft. A lot of Jewish people are still looking for closure of their family past. A lot of companies made a lot of money from these transactions. And municipalities still aren't recognizing their own role in these transactions. In some cases, they asked the Jews coming back from concentration camps of hiding for overdue taxes on their homes. Municipalities sometimes bought stolen property, and afterwards frustrated the recovery of the property to the rightful owners.   We made the data sets public for anyone to look into. This led to a lot of tips from relatives, people living in these houses and historians. Our research led to at least 77 other publications, because we gave them the data to do their own investigations. We made 2 tv episodes, made somewhere near 20 publications, 1 interactive map and 1 interactive longread De Straat Die Niet Meer Bestaat (translated: The Street That No Longer Exists), which is now submitted to be archived by the Washington Holocaust Museum. We've provided translations of the key publications in the url's (Google Drive with Google Translated texts). "," We started with the data set in Google Spreadsheets and Excel. The stories we found, we're researched with several national, local and online archives. We searched through old newspaper articles to find personal information, and we got access to archives which are hard to get into (because of sensitive information on war criminals). We also used old city maps of Amsterdam from around the war.   We made our interactive map with OpenStreetMap and a DMS in Google Spreadsheets. When one of the adresses has been located (because not all adresses still exist), we can change this location in Google Spreadsheets, which automatically updates our map.   Our interactive longread was made mobile-first, and combines video, animation, illustrations, photos a poem and text to tell the story about the Joden Houttuinen. It was developed in Vue, a JavaScript framework. The video was edited in After Effects, and the design made in Illustrator. The back-end is a Google Docs document where the journalists inserted content. In combination of ArchieML and a custom script, the reporters were able to update the project in production. For the exact locations and factchecking of photos, we used a lot of geolocation tricks (OSINT). "," When we started our investigation, the hardest part was to make an 75 year old database relevant in 2020. What stories haven't been told in the past decades? This required a lot of preliminary investigation, and we decided to depart from the municipalities. They can still be held accountable for not researching their own past. We could tell a lot of stories around that premise.   After that, we had to find out where the interesting stories can be found in a database of more than 7.000 transactions. Our preliminary research helped a lot, and eventually every row in this spreadsheet is an interesting story to tell. We always tried to find key persons in our stories which could be a symbol for a larger group: victims, buyers, notaries, real estate agents, companies, etc. It's hard to make a good story from just one row of data.   We used all sorts of archives to make substantial stories. It's really important to factcheck every column in the spreadsheet, because the original documents were handwritten. So finding more evidence who the real buyer is, for what price is was sold, etc. was a key part in our investigations. We all wrote up our main lessons in a making-of article, so other people can do their own research with our tips. "," What we learned, was that even the second world war (a part of our history that caused countless books, articles and movies) can be fuel for journalistic stories that are still relevant and important. Our combination of data journalism, archival research and classic journalism created an extensive investigation that was featured online in articles, an interactive map and our interactive longread, and two tv episodes. We gave the data personality and emotion.   Sharing the data and activating other journalists was important for our impact. Other regional and local journalists could go to work with our data. And because they made beautiful stories with the data, more municipalities felt the pressure of doing their own research. We also collaborated with our colleagues of De Monitor (KRO-NCRV) and Follow The Money on several parts during this investigation. Journalists working together on a data journalism project: that's just awesome, and more newsrooms should consider opening up their shop to make information even more relevant. ",https://pointer.kro-ncrv.nl/artikelen/vastgoedboeken-van-de-duitse-bezetter,https://pointer.kro-ncrv.nl/artikelen/verkaufsbucher-administratief-boekwerk-als-startpunt-voor-aangrijpende-oorlogsverhalen,https://pointer.kro-ncrv.nl/artikelen/de-straat-die-niet-meer-bestaat,https://pointer.kro-ncrv.nl/datasets/de-dataset-waarmee-alles-begint-verkaufsbucher,https://pointer.kro-ncrv.nl/artikelen/hier-staan-van-joden-onteigende-panden-bij-jou-in-de-buurt,https://pointer.kro-ncrv.nl/artikelen/terugkijken-de-verdwenen-joodse-huizen,https://drive.google.com/drive/folders/1lt4PUbYgEbLX1J2x9NT-JC02JnserMwW?usp=sharing,,"Jerry Vermanen, Thomas Mulder, Peter Keizer, Anoek Hofkens, Thomas de Beus, Dirk Mostert, Marije Rooze, Joris Heijkant, Tanne van Bree, Wendy van der Waal, Wouter Hoek, Rene Sommer, Marlies v.d. Meent, Miranda Grit, Anne-Mae van Tilburg, Stefan Vermeulen", Pointer is a data journalism platform of the public broadcast corporation KRO-NCRV in the Netherlands. We make stories based on data and OSINT. ,22 May 2020
France,"Disclose, Mediapart, Le Poulpe, Brut, France Culture, France 2",Small,Inside Lactalis,"Investigation, Long-form, Multiple-newsroom collaboration, Documentary, Database, Open data, OSINT, Podcast/radio, Infographics, Satellite images, Environment, Agriculture","Animation, Drone, Json, Microsoft Excel, Google Sheets, CSV"," Disclose has spent a year investigating the Lactalis ‘system’, involving numerous interviews and the study of hundreds of administrative and legal documents. This lengthy research reveals for the first time the extent of the group’s questionable practices, and also a certain sentiment of impunity that appears to be present within the multinational; failings in the field of food safety, massive pollution of rivers, the dissimulation of information, failings in control mechanisms, large-scale tax evasion and the hunting of whistleblowers. "," The project have a hudge impact in France, first because Disclose published with 5 partners media on radio (3 podcast broadcast on public radio France Culture ), online, and on TV ( a documentary broadcast on public channel France 2). The publication raised an audience of more thant 3 millions people in France. To further the public’s right to information, Disclose has published online all of the documents obtained, and which are listed under the plants concerned. We also created an interactive map who was use by several local media to continue our work. Two environmental NGO use the data provide online by Disclose to initiate legal prosecution for pollution against Lactalis. One NGO who fight against tax evasion used the financial documents published by Disclose to initiate prosecution against Lactalis. And one of the main farmers' unions held Lactalis to account following our revelations. The fishing and river protection association wrote to the Minister of Environment to denounce the serious environmental damage caused by Lactalis. One month after our revelation, the Ministry of Environment announced the creation of a new criminal offense on ecocides for private companies responsible for pollution and damage to the biotope. Last impact, parliamentarians demanded that Lactalis repay the public subsidies received by the State. Regional councils decide not to pay subsidies to Lactalis in regions where factories were accused of polluting rivers.                                             "," Disclose use several tool to map the factoris of Lactalis, who were hidden because not register like Lactalis factories. For this we use data of the registry of commerce to find the sharholder of the factory, we were able to find 60 factories in France own by Lactalis. After find these factories, we use satellite imagery to map the coordinate GPS and discover that all factories were based close to a river. Then, we send FOIA's request for each factories to the state authority responsible for environmental control and the authority responsible for sanitary control of factories to obtained documents that was documents that had to be public but were not disclosed by the State. We received hundred of documents from 2010 to 2020 : environmental and sanitary reports, the non-compliance orders of factories on environmental and health laws taken by the State authorities. We created our own data to analyze and categorize the information obtained on each factories. We read all the documents and put the result in a excel document, which allowed us to create data on the number of river pollution by 37 Lactalis factories over 10 years, the number of non-compliance with requests for compliance by the environmental protection authority, the impacts on the aquatic environment. We also ask the administration to obtained the analyzes carried out by the factories themselves on the pollutants discharged into the rivers. We cross-appraised the data obtained on the level of pollutants released, which enabled us to know what type of pollutants were released, in what quantity and if above the threshold authorized by environmental laws.Finally, we examined 113 documents which concerned the yearly accounts of companies owned by Lactalis and which appear to indicate a system of tax evasion to Luxembourg.                                                 "," The group Lactalis do not published documents about environmental issue, it's a very secret company and it was really hard to find information about these factory. We therefore decide to use the FOIA request on the documents that Lactalis is legally obliged to send to the State. But in France, public documents are ofen not disclose by administration at all level. On the state's Opendata website, the data is mostly very old and not up to date. Public administrations in France regularly oppose FOIA requests from citizens and journalists, or simply do not respond. So it was really difficulte to obtained this document, and Disclose was help by a pro-bono lawyer. When they finally send documents, they are often unorganized, or in very heavy excel form with thousands of data mixed up.Basing ourselves on <a href=""https://www.legifrance.gouv.fr/codes/id/LEGIARTI000006832922/2013-12-10/"" rel=""noopener"" target=""_blank"">Article L124-3  of France’s environmental law code, and Article 1 of the July 17th 1978 law regarding access to administrative documents, we addressed a total of 66 requests for public information concerning Lactalis production plants, which were submitted to 34 prefectures (local state administration centres) across France. Following the refusals by administrations to communicate the requested information, Disclose appealed against their non-cooperation on 22 occasions before the French mediating body for access to administrative documents, the  Commission d’accès aux documents administratifs  (CADA) - the FOIA authoritiy. This resulted in 16 rulings by FOIA authoritiy  in favour of Disclose. Disclose addressed requests to five regional councils, four regional agencies of the food, agriculture and forestry administration (Draaf), and six regional public water management agencies, for details of subsidies granted to the group’s production plants. Resulting from an exchange of around 350 emails with public administrative services. "," This investigative show how you can investigate on a company who hidden informations about is environmental impact with public data. This investigative shows how you can investigate a company that is hiding information about its environmental impact only with public data. It helps to understand how the use of public data such as business registers, environmental reports, public administration controls, can be sufficient to demonstrate the negative impact of industrial activities. That also show how FOIA's request can be use to have a lot informations. Disclose obtained and studied several thousand documents that included prefectural decrees, sanitary inspection reports, and data concerning products evacuated into watercourses by Lactalis plants.     ",https://lactalistoxique.disclose.ngo/en,https://www.theguardian.com/environment/2020/oct/19/french-dairy-giant-accused-of-polluting-countrys-famous-rivers-for-years,https://lequotidien.lu/economie/lactalis-aurait-echappe-au-fisc-via-le-luxembourg/,https://vimeo.com/471680608,https://www.franceculture.fr/emissions/series/enquete-lait-toxique,https://www.facebook.com/watch/?v=682411136029881,https://www.lemonde.fr/planete/article/2020/10/22/le-groupe-laitier-lactalis-accuse-de-ne-pas-respecter-le-code-de-l-environnement_6056932_3244.html,,"Marianne Kerfriden, Mathias Destal, Inès Léraud, Geoffrey Livolsi"," Marianne Kerfriden, is an investigative journalist and film documentary who ofen work on agricultural topics, living in Nantes.    Inès Léraud, is an investigative journalist on radio, who has doing long-term stories on environmental and social impact of agribusiness in France.   Mathias Destal is an investigative journalist and cofounder of the non-profit newsroom Disclose   Geoffrey Livolsi is an investigative journalist and cofounder of the non-profit newsroom Disclose ",18 Oct 2020
Brazil,G1,Big,"States buy 7 thousand respirators, but less than half are delivered; value of each equipment varies from R$ 40,000 to R$ 226,000 in Brazil","Investigation, Database, Open data, Politics, Health","Google Sheets, R"," This is an unprecedented and exclusive investigation carried out in Brazil on one of the most important equipment used during the pandemic of the new coronavirus.   With the data, it was possible to reveal that less than half of the respirators purchased by the states of the country were delivered at the time of peak disease.   The report exposed irregularities, unnecessary and unaudited spending of public money, as well as evidence of corruption. "," The work also became a story in Jornal Nacional, the main one on TV in the country. Its content gained national repercussion. The report shed light on a latent problem, giving a panorama hitherto unexplored. With this, new investigations were carried out on the purchase of equipments in the country.   The survey shows that:   1. states purchased 6,998 pulmonary respirators during the Covid-19 pandemic, but only 3,088 were delivered - less than half the equipment (44%)   2. the average amount paid for a respirator ranged from R$ 40,000 to R$ 226,000 in the country. That is, a respirator cost up to five times more than another "," The data were collected from the health departments of the 26 states and the Federal District. Requests were made to the press offices and through the Access to Information Law. Requests were also made to all Public Prosecution Offices in each state, via advisory and via AIL, to learn about the investigations in progress. In all, 108 requisitions were required (two for each government secretary and two for each body of the Court of Auditors) to obtain all the information and cross it. "," The entire investigation took more than a month and a half, as long as data collection, interviews and analyzes were carried out. Obtaining the data was, without a doubt, the most complicated part, since they stripped down a failed system and pointed out several signs of corruption involving public money. "," The investigation shows the importance and strength of the Access to Information Law. The data, hitherto unknown to the public, surfaced, exposing a serious problem and once again showing the power of journalism. ",https://g1.globo.com/bemestar/coronavirus/noticia/2020/06/26/estados-compram-7-mil-respiradores-mas-menos-da-metade-e-entregue-valor-de-cada-equipamento-varia-de-r-40-mil-a-r-226-mil-no-pais.ghtml,https://globoplay.globo.com/v/8656248/,,,,,,,"Clara Velasco, Gabriela Caesar, Thiago Reis", G1 won the Data Journalism Awards with the Violence Monitor and has done several other projects in the area. ,26 Jun 2020
Brazil,"Agência Lupa, Google News Initiative",Small,No Epicentro (At the epicenter),"Solutions journalism, Database, Open data, News application, Map, Health",Python,"   No epicentro  is a data visualization tool created with the aim of alerting to the amazing numbers of Covid-19 deaths in Brazil. Up until 2021 January 5th, over 196,000 people had died due to Covid‑19 in the country. But it can be difficult to visualize it so we thought: what if all these deaths had happened near you? Since major Covid‑19 outbreaks happened in metropolitan areas, many Brazilians don’t see the effects of the disease in their daily lives. This simulation was created to make the dimension of our losses easier to understand. It was replicated by ""The Washington Post"". "," The project was published on July 24th. One week after its launch, it had more than 210,000 page views. Among those, 178,000 were unique users. But the most remarkable indicator is that these people spent, on average, 10 minutes and 54 seconds browsing the website. This is way above market average. Also, people really dove into the story: for every 100 people who began the narrative experience, 71 reached the end of the narrative. Also, by publishing ""No Epicentro"", Agência Lupa added a new ​​activity - data visualization projects - in its business portfolio.   One of the biggest TV celebrities in Brazil, Luciano Huck, tweeted about the awareness the tool was able to bring and invited all his 13,1 million followers to experience ""No epicentro"". The tool was reported in the biggest brazilian national daily, ""Folha de S,. Paulo"" (<a href=""https://www1.folha.uol.com.br/equilibrioesaude/2020/07/plataforma-mostra-o-que-ocorreria-se-a-pandemia-de-covid-19-estivesse-concentrada-na-sua-vizinhanca.shtml"">https://www1.folha.uol.com.br/equilibrioesaude/2020/07/plataforma-mostra-o-que-ocorreria-se-a-pandemia-de-covid-19-estivesse-concentrada-na-sua-vizinhanca.shtml ).   The tool was also reported on prime time TV show Fantastico, biggest audience on Sunday television in the country: <a href=""https://g1.globo.com/fantastico/noticia/2020/08/09/brasil-chega-aos-100-mil-mortos-por-covid-aplicativo-dimensiona-devastacao-da-doenca.ghtml"">https://g1.globo.com/fantastico/noticia/2020/08/09/brasil-chega-aos-100-mil-mortos-por-covid-aplicativo-dimensiona-devastacao-da-doenca.ghtml .   It also aired on CNN Brasil: <a href=""https://www.cnnbrasil.com.br/tecnologia/2020/07/24/plataforma-dimensiona-mortes-por-covid-19-no-pais-a-partir-de-dados-locais"">https://www.cnnbrasil.com.br/tecnologia/2020/07/24/plataforma-dimensiona-mortes-por-covid-19-no-pais-a-partir-de-dados-locais    And as said before, in November 2020 ""No epicentro"" was replicated by the american national daily ""The Washington Post"" with the name ""At the epicenter"" (https://www.washingtonpost.com/graphics/2020/national/coronavirus-deaths-neighborhood/).   Finally, ""No epicentro"" was also awarded ""Best Visualization Tool"" at WANIFRA LATAM DiGITAL AWARDS 2020 (https://events.wan-ifra.org/events/2020-latam-digital-media-awards/content/4667) and will compete with other visualizaton tool created all around the world this year.  "," Regarding the nerdier aspects of the project, we used Python to develop the project's backend. Most computations use the geo-spatial analysis package GeoPandas and other related libraries, such as Shapely. For the front-end, we use Mapbox and a Javascript library called turf.js.   But an important point was the issues we faced while choosing the best dataset for the job. The most critical database would be the representation of Brazil's population distribution. All calculations would be made from that and there were two options, both from the Brazilian Institute of Geography and Statistics (IBGE). The most detailed of those is the statistical grid, a division of the Brazilian territory in a series of 200m² rectangles in urban areas and 1km² in rural areas. The other alternative were the census tracts. They are usually smaller in very populated areas and larger in areas with less population density. At first, it seemed that the better choice was to use the most detailed data possible, but the first tests with the statistical grid already showed performance problems, so we opted for the census tracts.    The second database was the number of deaths by Covid-19 in Brazil. However, the disclosure of these figures by public authorities has been confusing since the beginning of the pandemic. In the first weeks, it was erratic and not very granular. Then, it underwent methodological changes that undermined confidence in official information. To circumvent this, independent entities started to collect and publish these statistics. We chose the survey of Brasil.io, a team of about 40 volunteers who compiled daily cases and deaths in each city in the country since March 2020.    The project was under GNU and Creative Commons licenses, which allowed the content to be reproduced. ""No epicentro"" is also open source. "," The whole awareness/perception task was hard. On March 16, 2020, the President of Brazil, Jair Bolsonaro, said that the country would not ""overate"" the new coronavirus and that there was ""hysteria"" regarding Covid-19. On that day, the country registered 34 cases of contamination by SARS-Cov-2, the virus that causes the disease. The following day, March 17, Brazil saw its first death by Covid-19 – a retired doorman, Manoel Messias Freitas Filho, 62, who lived in São Paulo. From then on, the numbers of deaths and registered cases of contamination by the new coronavirus became routine for the Brazilians, occupying a good part of national Journalism. As the days passed, those numbers grew in geometric progression, so that the words ""oversize"" and ""hysteria"", from May on, lost their meaning completely. In September, more than 1000 Brazilians were dying each day.  In addition, with misinformation spread and social isolation measures being partially adhered to, it became more difficult to see, in fact, what the new coronavirus was causing. Was it a real pandemic or a hoax? It seemed that ""other people"" were dying, not me. The numbers were faceless. How to show the human cost of the disease beyond misinformation, hours of boredom and sidewalks half empty? With this in mind, the team at ""No epicentro"" started to work on references that could bring familiarity to the theme and could place the user at the center of this experience. For that, is there anything more familiar than the place where we live? Although this data did not reflect the reality as a whole, placing all the victims of the new coronavirus ""at the side"" of the user was a way of bringing the tragedy closer to the perception of each one. "," The most important lesson we learned - and other journalists can learn from the process by reading ""the making of No epicentro"", published in Medium (https://medium.com/datavizbr/como-fizemos-o-mapa-interativo-que-te-coloca-no-epicentro-da-epidemia-de-covid-19-no-brasil-4ce949a9183b) and in GitHub (https://github.com/noepicentro/) - was how valuable a multidisciplinary team is when we talk about digital journalism. Three professionals specialized in data journalism and visual narratives were responsible for the development of the project: the designer Vinicius Sueiro, the journalist-programmer Rodrigo Menegat and the developer Tiago Maranhão. Gilberto Scofield Jr., Director of Strategy and Business at Agência Lupa, Natalia Leal, Content Director at Lupa, and Marco Túlio Pires, Google News Lab Lead in Brazil, were also part of the team, everybody coordinated by one of the best data-visualization professionals in the world, Alberto Cairo, from University of Miami.   We also learned that good communication and openness to constructive criticism is key for achieving positive outcomes. When the team was set up, it seemed like all the development tasks were very compartmentalized and could be performed individually: to us, it was really possible that everyone involved sat at their computers, did their parts and just talked to each other when assembling the parts.    Luckily, the routine was different. The development team itself talked almost non-stop in a WhatsApp group chat which was particularly active late at night, with conversations about all dimensions of the content. The extended team, with professionals from Lupa and Google, met weekly on video conferences to discuss the product. Even though each professional had specific demands, everyone's fingerprints are on every piece of the material.    In the end, the team worked under a strict division of labor, but with freedom to make suggestions on topics distant from each person's responsibilities. The first half of the equation ensured productivity and efficiency. The second ensured creativity and critical sense.  ",https://piaui.folha.uol.com.br/lupa/epicentro/,https://www.washingtonpost.com/graphics/2020/national/coronavirus-deaths-neighborhood/,https://events.wan-ifra.org/events/2020-latam-digital-media-awards/content/4667,https://www.youtube.com/watch?v=7PMj8A9lDvI&t=2s,https://www.cnnbrasil.com.br/tecnologia/2020/07/24/plataforma-dimensiona-mortes-por-covid-19-no-pais-a-partir-de-dados-locais,https://g1.globo.com/fantastico/noticia/2020/08/09/brasil-chega-aos-100-mil-mortos-por-covid-aplicativo-dimensiona-devastacao-da-doenca.ghtml,https://www1.folha.uol.com.br/equilibrioesaude/2020/07/plataforma-mostra-o-que-ocorreria-se-a-pandemia-de-covid-19-estivesse-concentrada-na-sua-vizinhanca.shtml,,"Gilberto Scofield Junior, Natalia Leal, Vinicius Sueiro, Tiago Maranhão, Rodrigo Menegat, Marco Tulio Pires, Alberto Cairo"," GILBERTO SCOFIELD JR.  is a communication executive and Business and Strategy Director at Agência Lupa.   NATALIA LEAL is a journalist and content editor at Agência Lupa.   RODRIGO MENEGAT is a freelancer data driven journalist.   VINICIUS SUEIRO is a former lead information designer helping organizations use design & technology for positive social impact.   TIAGO MARANHÃO is a freelancer journalist-engineer and developer in Brasília.   ALBERTO CAIRO is a journalist and designer, and the Knight Chair in Visual Journalism at the School of Communication of the University of Miami (UM).    MARCO TULIO PIRES is the coordinator of Google News Lab in Brazil. ",24 Jul 2020
Nigeria,Dataphyte Nigeria,Small,"₦35m for Face Mask, ₦15m for Liquid Soap, How 5 Federal MDAs Mismanaged ₦1.69 Billion COVID19 Fund","Investigation, Open data, Infographics, Corruption","Microsoft Excel, Google Sheets"," The story revealed how an agency of Government, the Federal Road Safety Commission (FRSC) inflated the price of hand sanitizer (500ml) for its COVID-19 procurement deal worth $15,000 (₦5.6 million). Other agencies also spent thrice of the money (39.33 million) for the supply of face masks (Supply & Delivery of Face Mask for LGA Training participants. Delivery at NSCS, Abuja). "," After the data report, Nigerians on social media demanded an investigation and probe into the outrageous contracts and inflated prices of Personal Protective Equipment awarded by five federal agencies of government. ", I used Open Data Portals of the federal government to achieve the story and visualisation tool. ," The hardest part was confirming some of the procurement items as COVID-19 lockdown impacted the story. Instead of letting the story go, I connected with friends across cities to contact suppliers and sellers of medical protectives.     ", Journalists from Africa and Nigeria need to start making using the government open portal platforms for impactive and investigations. These will strengthen governance and government procurement processes. It helps save cost and improve accountability. ,https://www.dataphyte.com/covid19nigeria/%e2%82%a635m-for-face-mask-%e2%82%a615m-for-liquid-soap-how-5-federal-mdas-mismanaged-%e2%82%a61-69-billion-covid19-fund/,https://www.dataphyte.com/economy/11-years-and-counting-as-nigeria-waits-on-her-5-billion-check-from-nnpc/,https://www.dataphyte.com/development/%e2%82%a6350-billion-sukuk-issuances-and-the-missing-transparency-on-nigerias-borrowing/,https://www.dataphyte.com/economy/nigerians-kick-over-outrageous-covid-19-spendings-frsc-defends-procurement-other-mdas-keep-mum/,https://www.dataphyte.com/economy/covid19-20-states-cut-2020-budget-but-what-data-aided-the-decision/,,,,Aderemi Ojekunle," Aderemi Ojekunle reports on Health, Finance, and Development. He is currently a Data Journalist with Dataphyte, a media research and data analytics organization, in Abuja, Nigeria’s capital city. He is a graduate of Public Administration from Obafemi Awolowo University and a member of the Nigerian Chartered Institute of Personnel Management (ACIPM).   He is also a Bloomberg Media Initiative trained business reporter and has combined professional experience spanning 5 years.   Before joining Dataphyte, Aderemi was a Senior Reporter with Business Insider Africa and had been a reporter at Nigerian Bulletin, Business Am Newspaper, as well as Campus reporter with Nigerian Tribune and The Nation Newspapers.   On different occasions, he worked with Non-Governmental Organizations including Young Volunteers for Environment (YVE/JVE), an international organization, advocating for climate change, clean energy, and a green environment.   Reach him via:   Twitter: RemmyAlex   LinkedIn: <a href=""https://www.linkedin.com/in/remmyalex/"">https://www.linkedin.com/in/remmyalex/    <a href=""https://muckrack.com/aderemi-ojekunle"">https://muckrack.com/aderemi-ojekunle      ",27 Jul 2020
Brazil,CNN Brasil,Big,Revealing chloroquine's background in Brazil during the pandemic,"Investigation, Breaking news","Microsoft Excel, Google Sheets, CSV","In a serie of articles, I have revealed huge problems in chloroquine's production in Brazil, made by the Army. I have obtained several documents and data using Access to Information Law that proved that: 1 - the chloroquine was overpriced; 2 - the army knew it but decided to buy it anyway; 3 - the army helped a company to win the bid by giving them privilleged information; 4 - the ammount produced was much higher than the demand and now there is a stock of 400,000 pills; 5 - The Army recognized that they produced chloroquine without scientific evidence", Tribunal de Contas da União (TCU) and Procuradoria-Geral da República (PGR) opened investigations against the Ministry of Defense and the Army itself after the stories were released; congresssmen asked for  CPI (inquiry comission). The case is still under investigation - https://www.cnnbrasil.com.br/nacional/2020/09/16/deputados-pedem-explicacao-sobre-preco-da-cloroquina-assunto-deve-ir-a-cpi / https://www.cnnbrasil.com.br/nacional/2020/09/17/pgr-vai-apurar-suposto-crime-de-responsabilidade-de-ministros-da-defesa-e-saude ," Access to information law, osint research and Google Spreadsheet ", The hardest part was to scrape data from Diário  Oficial da União (Federal Gazette) to see every single purchase of chloroquine in the past and then compare it to the last purchase. ," When a public authority is defending something with no benefits to the population, it is very possible that there is some scheme or corruption involved. President Jair Bolsonaro made a lot of publicity about Chloroquine even knowing that it has no proved benefits against Covid-19. ",https://www.cnnbrasil.com.br/nacional/2020/09/15/exclusivo-sem-contestar-exercito-paga-quase-triplo-por-insumo-da-cloroquina,https://www.cnnbrasil.com.br/nacional/2020/10/28/fornecedora-de-cloroquina-do-exercito-foi-consultada-um-mes-antes-de-concorrente,https://www.cnnbrasil.com.br/nacional/2020/09/17/pgr-vai-apurar-suposto-crime-de-responsabilidade-de-ministros-da-defesa-e-saude,https://fiquemsabendo.com.br/saude/cloroquina-exercito-tcu/,https://www.cnnbrasil.com.br/nacional/2020/10/15/departamento-do-exercito-questionou-preco-inflado-de-cloroquina-antes-de-compra,https://www.cnnbrasil.com.br/nacional/2020/11/16/sem-demanda-nos-estados-400-mil-comprimidos-de-cloroquina-encalham-no-exercito,,,"Luiz Fernando Toledo, Daniel Mota, José Brito and Vital Neto"," Luiz Fernando Toledo is a brazilian investigative journalist currently based in Brazil, researcher at Reuters Institute for the Study of Journalism, director of Brazilian Association of Investigative Journalism (Abraji) and brazilian editor for Organized Crime and Corruption Reporting Project (OCCRP).       Fiquem Sabendo is a data agency focused on obtaining public data and documents using Access to Information Law (Lei de Acesso à Informação or LAI). Our main goal is to make LAI mainstream among Brazilian people to improve democracy. ",15 Sep 2020
South Africa,News24,Small,Targeting AGU Team C: How Zane Kilian tracked anti-gang cop Charl Kinnear and his team. An investigation by News24.,"Investigation, Explainer, Long-form, Breaking news, Database, Illustration, Infographics, Video, Map, Corruption, Crime, Gun violence, Human rights",Microsoft Excel," A policeman leading a gang task-force was assassinated and, in the weeks that followed, our investigation uncovered evidence that an underworld figure had used illegal cellphone technology to track his movements for months.   A dataset revealed how a pliant PI became an assassin’s hidden hand, his illegal surveillance tied to other attempted murders. Our project unearthed evidence that hundreds of people, including high ranking police officials, politicians, and rival gangsters were spied on.   What may be South Africa’s most extensive private surveillance scandal – with network operators surreptitiously selling access to their systems – was exposed by a murder.       "," The project – in long form and using data visualizations showing the real locations of hundreds of people who were surveilled – followed a narrative of how a failed sports star turned to the criminal underworld and became a spy.   Our investigation put paid to Zane Kilian’s public denials that he didn’t know who he was tracking, and uncovered evidence of a vast history of illegal surveillance, monitoring the movements of more than 600 people.   Access to the tracking technology – meant to be the preserve of law enforcement – had quietly been sold on the open market to hundreds of people, with our project showing the scale of privacy breach, which in some cases had deadly consequences.   Our investigative reporting revealed how mobile networks reactively shut access to their systems, which were effectively hijacked for this shadowy espionage for years without detection. Operators have launched high level investigations into the breach, and News24 is using freedom of information legislation to gain access.   Anti-surveillance experts have called for network operators, the legal custodian of their customers’ data, to face a parliamentary inquiry. The country’s Information Regulator has launched their own investigation into potential breaches of privacy laws.    Police top brass have also instituted an investigation into how this system had been infiltrated by criminal operators, and effectively used a life-saving technology to plan an assassination and other attacks. "," The team utilised free cellphone directory application TrueCaller to identify a vast majority of the 5 000 plus records of cell phones that were tracked, and used other databases to further verify identifies. The visualisaitons were built using the 3D Map function on Microsoft Excel, and the website was built using Shorthand.     The visualisations were made possible with the GPS coordinates already contained in the database provided to us, but it required the use of Power Queries and numerous format edits to make the data usable for the 3D Map function. More sophisticated software was considered but ultimately, we chose to use tools that were readily available on a majority of personal and work computers around the world.  "," This project was not without its challenges, and likely the most significant hurdle to overcome was processing the raw data and putting a name and face to each of the thousands of cell phone numbers, with two reporters to do this.    We obtained an Excel spreadsheet which had been populated by owners of the illegal software platform, which had been handed over the police. The dataset provided key insight into every cell phone PI Zane Kilian illegally surveilled over a period of 17 months, and the corresponding GPS coordinates of the precise moment their phones were “pinged”, but they were still just faceless numbers.   We turned to call identification app Truecaller, which uses end-user agreements to generate a global bank of contacts on their servers and allows a user to input a number in a search, the result returning the user’s crowdsourced name [as it had been stored in the contact books of others]. There were two obstacles to surmount, firstly one is only allowed 20 number searches per day and the second is that results, because of crowdsourcing, lacked accuracy.   For six weeks, our team of two waded through the dataset, using Truecaller as an online divining rod. When numbers were matched to names, publicly available document searches and interviews were used to confirm the personal details of the survielled.     Once 5308 numbers had been identified to the best possible extent, freely available mapping software was used to develop patterns and networks, identifying key people who were being tracked by nefarious forces.     A not uncommon yet inherent hinderance is the personal danger when engaging criminal actors, who in this instance, have tentacles which have a considerable reach, even into the ranks of the police. "," Our project was compiled a small investigative team consisting of just two journalists, who between them divided ours spent in the field, processing data, and conducting interviews, all while covering other investigative beats.   Operating within the media landscape in South Africa requires prudential management of resources, and to ameliorate this, we turned to solutions which were free. Using credit traces to identify thousands of individual cell numbers would have notched up massive costs, thus TrueCaller became our code book.      The data visualisations for the story were created using tools readily available on nearly all computers in the world. It’s not difficult, once you have the information, to experiment and create similar maps.   We were industrious in producing this project, a valuable tenet in African newsrooms which are often small and underfunded. This concept, one which is open to adaption for use in other projects, would be advantageous for African newsrooms to produce world-class data investigations. ",https://specialprojects.news24.com/zane-kilian-charl-kinnear-cellphone-tracking/index.html,https://www.news24.com/news24/video/southafrica/news/watch-explained-see-how-zane-kilian-tracked-charl-kinnear-and-600-others-20201215,https://www.news24.com/news24/southafrica/investigations/watch-killing-kinnear-the-spy-in-your-pocket-and-how-it-was-used-to-track-slain-anti-gang-unit-cop-20201107,https://www.news24.com/news24/southafrica/investigations/killing-kinnear-the-other-men-who-were-tracking-the-veteran-cop-20210109,https://www.news24.com/news24/southafrica/investigations/exclusive-mtn-vodacom-cut-off-cellphone-spies-after-top-cop-kinnears-murder-20201017,,,,"Jeff Wicks, Kyle Cowan, Alet Law, Sharlene Roodt"," Kyle Cowan: Cowan started his journalism career in 2013, working for community newspapers, before joining the The Times newspaper in a national reporting role before moving to News24, South Africa’s largest online news publication, as an investigative reporter in June 2017. He was named the joint winner of the 2018 Taco Kuiper Award for Investigative Journalism for his reporting on white-collar corruption, work that earned him another national award in 2019.  Previous accolades include awards received for breaking news and multimedia reporting.   Jeff Wicks: Wicks, alongside Cowan, is an investigative journalist at News24. He has worked at the Sunday Times, the Natal Witness and the Sunday Tribune as a senior reporter, and holds regional Vodacom Journalist of the Year awards for financial and multi-platform reporting.     Alet Shaw: Law is the newsletter and engagement editor at News24 and former opinions editor. She holds a PhD in political communication from the University of Cape Town and is the editor of the recently published book,  Should we go? To emigrate or not: 21 voices speak their mind .    Sharlene Roodt: The 33-year-old journalist is News24's Multimedia Editor. She has written for The Media Magazine and Wits Business School Journal, as well as Beeld newspaper. In 2014 she began her career as a multimedia journalist for Netwerk 24.  ",13 Dec 2020
South Africa,"Oxpeckers Investigative Environmental Journalism, Internews' Earth Journalism Network, The Third Pole, The Frontier Manipur, Haluan and Ekuatorial",Small,#WildEye Asia: Mapping wildlife crime,"Investigation, Explainer, Solutions journalism, Cross-border, Multiple-newsroom collaboration, Database, Open data, Crowdsourcing, Infographics, Map, Environment, Corruption, Crime","Personalisation, Scraping, D3.js, QGIS, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, OpenStreetMap, Python"," #WildEye Asia is a pioneering geo-journalism tool that provides access to data on wildlife trafficking. It maps information on seizures, arrests, court cases and convictions. The platform also hosts a dossier of investigative reporting. #WildEye Asia was created out of a sense of urgency when Covid-19 hit, and the question of what law enforcement and legal systems are doing about illegal wildlife trade became a concern for everyone. Until now, there has been no single place to access information on efforts to crack down on wildlife crime. #WildEye addresses this gap by tracking and sharing data on justice in action. "," We have seen significant success in making wildlife trafficking data publicly accessible. By shining a light on the scale of illegal wildlife trade taking place, we have sparked the interest of other journalists, law enforcement, and organisations monitoring and investigating wildlife crime. We have mapped over 900 incidents of seizures, arrests, court cases and convictions on the #WildEye Asia map. These span the entire continent and record information on more than 100 different endangered species.    We have also contributed to the narrative around Covid-19 and its links to illegal wildlife trade. When the urgent need to tackle wildlife trade that is illegal, unsustainable and poses threats to human health and biodiversity conservation became apparent, resources like #WildEye highlighted the need for urgent action.    We have formed ongoing relationships with organisations across the globe, including wildlife monitoring agency Traffic, the Global Initiative against Transnational Organized Crime, USAID, the World Wildlife Fund, the Wildlife Conservation Society, the Environmental Investigation Agency, the Freeland Foundation and Investigate Earth, among others. We have also interacted with law enforcement agencies, including Interpol, the Wildlife Justice Commission and local law enforcement.    A total of 16 in-depth, data-driven investigations have been published in China, India, Pakistan, Nepal, Indonesia and Vietnam. These have been viewed and shared, along with the #WildEye map, hundreds of thousands of times. Topics include Chinese courts' leniency towards pangolin offenders; insurgents linked to India's rhino poaching syndicates; traditional Chinese medicine in wildlife; and using new Indonesian law to nab wildlife smugglers.    We have also hosted eight webinars between May and December 2020, with a total of 416 participants from all over the world. These included training on how and why to use #WildEye; investigating wildlife trafficking and conservation reporting; digitisation for anti-corruption and Covid-19 and the environmental crisis. These webinars are available on #WildEye. "," #WildEye Asia was built by developers using an open-source platform called Mapbox. This was customised to fit in with Oxpeckers' style, and to suit our needs as data journalists.    #WildEye Asia's main feature is a map of the continent showing where law enforcement agencies and legal systems have been involved in action against wildlife trafficking. Each case is identified by an icon that signifies either a seizure, an arrest, a court case or a conviction.    The tool includes a search function to help users filter information and find topics of interest. If you want to learn about the illegal trade of pangolin scales, for instance, simply type ""pangolin"" or ""scales"" in the search box and you will get a host of results covering incidents involving pangolin scale smuggling.    We also have an alert system, which allows users to subscribe to receive updates on either an area or a specific case. Each time the data is updated, you will receive an email with this information. Use the buttons to subscribe and unsubscribe on the map. This way, you do not need to search manually for updated information, and can rely on #WildEye to do this for you.    Data is uploaded on Mapbox via a Google spreadsheet that is updated on a weekly basis, sometimes more frequently. Methods of data collection range from scraping social media and news sites to working with reports and datasets provided by monitoring organisations. We convert bulky reports and complex datasets into spreadsheets that can be analysed and added to #WildEye. By engaging with organisations such as Traffic, the Wildlife Justice Commission, the Environmental Investigation Agency, the Global Initiative against Transnational Organized Crime, the Environmental Reporting Collective and the Indonesian Data Journalism Network, we have been able to access some of this data more easily. "," Our biggest challenge has always been accessing data, mainly because #WildEye is the first platform to collect, collate, analyse and make public data on legal processes relating to illicit wildlife trade in Asia.    We always knew that we were creating a tool that maps and makes the data public, but quickly came to understand why we were doing this when we hit multiple roadblocks and had to fight to get that information - in some cases, the fight isn't even nearly over.    This is what led us to working so closely with local journalists and locally-based monitoring organisations. These people and the work they do are crucial to our understanding, involvement and accessing of often sensitive information. We have worked with over a dozen locally-based journalists to produce data-driven investigations that highlight issues related to wildlife trafficking in China, Indonesia, India, Nepal and Vietnam.  "," #WildEye is a fantastic customisation of open-source technology, which we hope encourages others – especially journalists – to test similar methods of creating, collating and visualizing large datasets of their own. We have been vocal about the challenges we faced throughout the creation of this tool during in-person interactions, on panels and in webinars, and hope that others learn from our experiences. Making important data sets look good and easily accessible does not have to be difficult, and #WildEye is a prime example of this, within the context of data journalism. We have also shown how to turn data into a compelling and important environmental story. By training journalists how to work the tool, we have published numerous investigations that use #WildEye data to tell their stories, lending our voice to issues around the law, health, safety, corruption and illicit financial flow. We see these stories as demonstrating what newsworthy and good quality data journalism looks like. ",https://oxpeckers.org/wildeyemap-asia/,https://earthjournalism.net/special-reports/wildeye,https://oxpeckers.org/2020/08/talking-trafficking-in-asia/,https://earthjournalism.net/video-highlight/how-to-become-a-top-wildlife-journalist,https://oxpeckers.org/2020/08/indias-rhino-poaching-syndicates/,https://oxpeckers.org/2020/05/chinese-court-cases/,https://oxpeckers.org/2020/10/wildlife-has-no-part-in-tcm/,,"Fiona Macleod, James Fahn, Roxanne Joseph, Sara Schonhardt, Mark Hartman, Tristan Mathiesen, Wan Ulfa Nur Zuhra, Richa Syal and Andiswa Matikinca"," #WildEye is a project by Oxpeckers Investigative Environmental Journalism (Oxpeckers), supported by the Earth Journalism Network (EJN). Oxpeckers is Africa's first journalistic investigation unit focusing on environmental issues. It combines traditional investigative reporting with data analysis and geo-mapping tools to expose eco-offences and track organised criminal syndicates.    EJN was developed by Internews to enable journalists from developing countries to cover the environment more effectively. It is a global network with reporters and outlets in every region of the world. It trains journalists to cover a variety of issues, develops innovative environmental news sites and produces content for local media.     Artman Designs is a digital design agency geared towards graphic design, web design, app design, UX design and social media management. ",4 Mar 2020
United States,CBS Chicago,Big,Left in the Dark,"Investigation, Explainer, Solutions journalism, Long-form, Chart, Video, Map, Crime",Python," Published about five years after the Chicago Police Department first began issuing body cameras to its officers,  Left in the Dark  explores how the policies surrounding those cameras have broken down at nearly every level, and how those breakdowns have exacerbated the trauma inflicted by police misconduct. "," In response to our reporting, the Chicago Inspector General’s office said they would take steps to ensure the body cameras are seen as “more than high-tech vest ornaments,” and would focus on body camera usage in a follow-up inquiry to a 2019 report it published, which, in part, discussed issues with CPD’s body camera policy, including misuse of cameras.   We asked Chicago Mayor Lori Lightfoot and CPD Superintendent David Brown what steps they’d take following our reporting. Neither responded directly to our questions, and numerous interview requests to both officials were ignored.   A spokesperson for Mayor Lightfoot did say the city would “create a strengthened plan to ensure compliance among all officers with bodyworn [sic] cameras, including more detailed aspects of compliance such as the extent to which officers properly record interactions with the City’s residents.” The spokesperson didn’t provide specifics, so we will continue to report on those purported changes in the future.   In addition, during the summer of 2020, while we were reporting this story, two CPD officers who weren’t wearing body cameras shot a man in the back. In response, CPD announced officers on its specialized “community safety team” would finally be equipped with body cameras. Our focus now is to investigate how those changes play out. "," The data analysis was done in Python, particularly using geospatial tools like Geopandas and the Folium mapping library to visually examine geographic disparities in the data we obtained.    To help piece together how often officers do activate their cameras when required, we used data on more than 340,000 routine investigatory stops beginning in 2018, after every patrol officer was equipped with a camera. At the time we reported the story, hundreds of officers on specialized tactical teams still weren’t required to wear cameras, so we used daily officer assignment data to rule out officers assigned to one of those teams, and found one in ten stops weren’t recorded. When factoring in officers who are on those teams, the number jumps to one in five.   We mapped that data down to the police beat — a small geographic subdivision, typically less than a square mile — used by CPD. It shows stark disparities in the use of body cameras, particularly when including stops made by officers assigned to tactical teams, who primarily work in predominantly Black and brown communities. While almost 100 percent of stops in the predominantly white North Side were recorded, less than 60 percent of stops were recorded in some parts of the South and West sides. "," The biggest challenge with the data was determining if an officer had been given a camera at the time of the stop. While officers not equipped with cameras are part of the problem, our focus was on investigatory stops by officers who had cameras but didn’t use them properly.   We knew all patrol officers had cameras, but those on specialized teams didn’t, so we sought to determine if an officer was on one of those teams at the time of each stop. We obtained data showing the unit assignments of every CPD officer every day and cross-cross-referenced the officer’s identities and dates of the stops to determine if they were on one of those teams at the time of the stop.   In addition, we faced significant issues obtaining records in nearly every FOIA request we filed for this story.   In our requests to CPD's accountability organization, the Civilian Office of Police Accountability (COPA), for video of the one particular stop of a man we featured heavily in the story, COPA initially failed to release the dashcam video, and improperly redacted multiple portions of the body camera video. They only corrected the redactions when we realized they accidentally left one portion unredacted which was redacted in another officer’s camera.   CPD vigorously fought the release of internal audit reports on its body camera program that we requested. After we initially requested them in June 2019, police denied the request, saying they were exempt from release because they were materials gathered in the course of an audit. We refiled the request, and, after nearly four months, CPD denied the request again. We then appealed the request to the Illinois Attorney General, who agreed the records were in fact subject to release. "," The takeaway from our experience reporting this story is the value combining records reporting and data reporting can bring.   On its own, the data we uncovered revealed significant disparities between communities in the likelihood an incident will be recorded — a finding that would have been significant even without the records we obtained. Likewise, the internal audit reports we fought so hard to get revealed not only that there was a problem, but that CPD knew about it for years, often documenting the same problem month after month.   Like our data, that would have been a story on its own. But the conjunction of the two yielded an even more powerful narrative: CPD knew there were problems with officers misusing cameras, and with the accountability systems set up to ensure officers use cameras properly. Yet, as far as we can tell (and as far as CPD has told us), they've made little meaningful progress. The consequences of that lack of accountability are the disparities we saw in our data.  ",https://chicago.cbslocal.com/leftinthedark,,,,,,,,"Samah Assad, Christopher Hacker, Dave Savini"," Samah Assad is an investigative journalist with WBBM/CBS Chicago. Her work focuses on exposing systemic failures in policing, sexual assault, racial disparities, taxpayer waste and government corruption. She specializes in data analysis, public records and digital interactivity. Her impactful reporting has been recognized for numerous awards including the George Foster Peabody, Edward R. Murrow, Investigative Reporters & Editors (IRE), and Emmy awards.   Christopher Hacker is a data journalist at CBS Chicago, where he supports investigative staff with complex data analysis, FOIA requests and design of project-specific newsgathering tools.    Dave Savini is a Peabody, duPont, Murrow, IRE and NABJ Award winning investigative reporter at CBS2 Chicago where he has held that position since 2004.  Savini’s career in Chicago began in 1993 at NBC5 Chicago with the UNIT-5 investigative team  .    His awards include 20 Emmys, the prestigious national Alfred I. duPont award, two national Edward R. Murrow awards, 2019 national IRE award, 2019 George Foster Peabody award, and the 2020 Emmy for our series “Unwarranted” exposing wrong police raids and trauma to innocent children and families. He was also honored with the  2020 Emmy for Best Single Investigative Report “The Anjanette Young Story.”  Voted best reporter by the Illinois Associated Press 5 times.  ",15 Nov 2020
Taiwan,READr,Small,The visible virus : COVID-19 Disinformaiton,"Investigation, Explainer, Cross-border, Quiz/game, Database, Open data, Fact-checking, Infographics, Chart, Politics","Scraping, Json, Adobe Creative Suite, Google Sheets, CSV, R, RStudio, Node.js"," Covid-19 has claimed 1.93 million lives. Not only the virus but also disinformation spreads globally and causes harm, and it even travels faster than the virus, kills people and increases the risk of racial discrimination.   READr analyzes more than 5,000 fact-checking reports and gain insights into the status and trends of the infodemic.  "," This is the first complete investigative report in the world that analyzed the disinformation about COVID-19. We analyzed more than 5,000 fact-checking reports written by fact-checking organizations in the world to find out the characteristics of disinformation spread in different countries. Not only data analysis, we also interviewed fact checkers from various countries, and they shared the actual harm caused by these disinformation. We also verified the influence of these fake messages in tens of millions of tweets from Twitter, and found that disinformation in the ""good news"" category is more likely to spread, which was different from the sensational one. "," In the part of data analysis, we used web-crawlers to get the fact-checking reports of IFCN(The International Fact-Checking Network), and used R to analyze the data. Manual classification of disinformation was the most time-consuming part of this project. We used TF-IDF word segmentation to help reporter find the same type of disinformation. Also, we used some functions of Google Spreadsheet, like translation, to minimize the time spent on manual classification.   For the data visualization, we use Adobe Illustrator to design graphs and use Flourish to build interactive charts. We use Vue.js and vue-i18n to implement multi-language web page. "," When the reporter analyzed more than 5,000 fact-checking reports, we found out that these disinformations were sometimes so ridiculious, and even rearrangement the combination of the terms will be a new disinformation, but some people still believe them. We wanted to convey this feeling to users, because the users may only received one or two disinformation, but the whole world was facing the impact of huge number of disinformation at the same time. With the help of web designers and engineers, we used ""slot machines"" to convey this feeling. Users could simply ""create a disinformation"" through click the bottom on the slot machine. Users could experienced that in the process of playing, even if the disinformations you create were absurd, they may be actual disinformations. And you only need to change the combination of terms to generate a new disinformation.   While the epidemic was still developing, that is, when there was hardly any academic resrearch (only a few weeks before our report published, a report contains 225 disinformation sample studies released by the Reuters Institute fot the Study of Journalism). To do this, the newsroom had to assume the responsibility of research, and this is exactly what data journalism should do. "," When users reading the report, they not only absorb new knowledge, the format of the report could also convey more feelings to users. We should grasp every second when users enter the webpage of the report, so that they could get something from every mouse-click or stay. ",https://www.readr.tw/project/covid19-disinformation/en,,,,,,,,"Yu-Ju Lee , Yi-chian Chen , Tan Hsueh-Yung , Meg Wu , Yu-Chung Cheng's team at National Chengchi University"," READr is not just a data newsroom, it is also a digital innovation team. The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, we still try hard to have an indicative impact on the development of data journalism in Taiwan.   We always hope to make breakthroughs in every topic.Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",24 Jul 2020
Taiwan,READr,Small,How China use Twitter as a weapon of COVID-19 propaganda,"Investigation, Explainer, Cross-border, Open data, Chart, Politics","R, RStudio, Python"," COVID-19 epidemic first broke out in China in early 2020.   In order to maintain its international image, the Chinese diplomatic group began to publish positive posts on Twitter, from optimistic and active anti-epidemic experience to donating materials to other countries.  We analyze the posts of the embassies' accounts, which is in the list monitored by the American think tank ""German Marshall Foundation"".We found that the diplomatic team uses the official accounts of Spokespersons as the main combat force to actively respond to external critics; other members, including embassies and ambassadors abroad, strengthen the influence on public opinion by retweeting. "," This report is the first one to analyze the original data which the Chinese diplomatic group has published since the beginning of the COVID-19 epidemic released from Twitter. Almost all the media in Taiwan only quoted from Twitter’s press release, while ignoring the batch of data they released. After further analysis, we found more interesting facts. For example, with the control of the domestic epidemic, the strategy of propaganda has changed several times, from the beginning of the fight against the epidemic with grateful and positive attitude, to emphasize China’s positive contribution to the world to fight against the epidemic. However, China rarely announced how the medical system was out of control.  Moreover, they spent a lot of effort arguing with the U.S. where the epidemic started to spread and which country should be responsible for it. "," We have a Python script to fetch the Twitter data by calling the Twitter API. And make the segments by Jieba, a Python library for CJK segmentation. After that, we have the term frequency analytics by TF/IDF to identify the keywords of each article. "," During operation, we processed hundreds of thousands of tweets. To visualize the huge amount of data, and uncover the meaning behind them is difficult, but very interesting. Compared with almost all the media only quoting Twitter's released news, we are the first media to analyze their data. "," To people who are interested in public opinion analysis, the report shows how to analyze data from a number of social networking sites quickly and find their interaction patterns and associate with each other. To people who are concerned about how the Chinese government handles the epidemic crisis, the report attempts to provide an image about how China defended the legitimacy of the regime by using the influence of public opinion in its diplomatic group. ",https://www.readr.tw/post/2440,,,,,,,,"Pei-Yu Chen , Yi-Chian Chen, Yu-Ju Lee, Hsin-Chan Chieh, Yu-Chung Cheng's team at National Chengchi University"," READr is not just a data newsroom, it is also a digital innovation team. The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, we still try hard to have an indicative impact on the development of data journalism in Taiwan.   We always hope to make breakthroughs in every topic.Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",3 Jun 2020
Taiwan,READr,Small,Which Politician Set You as An Advertising Target?,"Investigation, Explainer, Quiz/game, Database, Open data, Illustration, Infographics, Chart, Elections, Politics","CSV, R, RStudio, Python"," Before the election, we created a simulator by thousands real political advertisements. That allow users to simulate which political figures you as anadvertising target by setting conditions of different ages, genders, and regions. "," In response to Taiwan’s presidential election, Facebook opened its advertising database in Taiwan in August 2019, collecting and disclosing all users’ advertisements on Facebook, and later actively required ""political advertisements"" to disclose the target and amount of advertisements. We analyzed the data of advertising database that began in October 2019, and further analyzed the target of the official fanpage of legislator candidates in 2020. From thousands of data, we found that ""middle-aged"" and ""male"" were the main target of this election. But there were also special candidates whose main target was female or who were not in their election districts.   Although the amount of advertising on Facebook is already considerable, Facebook advertising is not yet the main battlefield of election. For some candidates whose electoral district in suburbs.""The air battles"" on Facebook were relatively calm, showing how to combine online ""air battles"" and face-to-face ""land battles"" with voters was still a complex question. "," Since the Facebook public their ads data with API, we have a Python script to get all of the data. And all of the data are combined in a csv file and the we can analytics the ads by R. So we can know the GAs for each advertisings. We design a simple web game for the users to let them know what ads they are targeted by the Vue.js. "," Although the advertising data are public, we used the form of a simulator let users understand the information more easily, also established the relationship between the topic and themselves, and increased the accessibility of the topic. In addition, although the amount of advertising on Facebook was already considerable, in Taiwan, candidates still had to rely on local networks and physical election activities. We learned about this through interviews and found out the reasons why some candidates did not use internet as a main propaganda channel. "," Through a interesting way to present a report, users could read the topic more easily and feel that it is related to themselves. ",https://www.readr.tw/project/political-post,,,,,,,,"Liu Tzu-Wei, Chen Yi-Chian, Chang Zhao Wen, Ronny Wang, Lee Yu Ju, Chien Hsin-chan"," READr is not just a data newsroom, it is also a digital innovation team. The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, we still try hard to have an indicative impact on the development of data journalism in Taiwan.   We always hope to make breakthroughs in every topic.Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",7 Jan 2020
South Korea,The Kyunghyang Shinmun (daily newspaper),Big,Three workers a day are never back from work for good,"Investigation, Multiple-newsroom collaboration, Database, Infographics, Chart, Map, Employment, Human rights","Scraping, D3.js, JQuery, Json, Google Sheets, CSV, Python"," In Korea, three workers die in an accident at an industrial site a day. In order to effectively inform this horrible reality to the people, we decided to investigate how many people are dying for a certain period and show all of the death. we investigated and analyzed 1,305 accident reports. And then we reported articles that 1,748 workers were dead during a year and 10 months. We made an archive of how each person died into website, so readers could check it out. And we filled the front page of the newspaper only with over 1,000 dead workers' name. "," Since the report, many experts and civic groups have mentioned our reports in newspaper columns, statements, and social media. Kim Hoon, a famous Korean novelist who has usually spoken out about brutality of industrial accidents, sent a special contribution to our newspaper and we published it as a part of our article series. Political sphere and labor groups have also issued statements after our reports. At the time, the Democratic Party of Korea, the Bareunmirae Party, and the Justice Party mentioned the report and said they felt responsibility. They urged the government and the National Assembly and companies to take action to reduce accident. The Korean Confederation of Trade Unions and the Federation of Korean Trade Unions, the two largest trade union organizations in Korea, also commented on our reports.   In the wake of our reports, many press have released similar reports that show the seriousness of industrial accidents. Like us, more articles have been written by analyzing accident reports. Interest in the deaths of workers has increased in journalism of Korea.   The punishment of industrial accidents in Korea was very weak. After the report, Minister of Labor Lee Jae-gap said in an interview with our paper that he would ask the Supreme court to raise the sentencing guidelines for punishment of industrial accidents . As mentioned in the interview, Minister Lee met with Kim Young-ran, chairman of the Supreme Court's Sentencing Committee, in June last year and asked for adjustment of sentencing guidelines. A month later, the Supreme Court's Sentencing Committee began revising the sentencing guidelines, and in January this year, it announced the sentencing guidelines to drastically increase the sentences of crimes violating the Industrial Safety and Health Act. "," We inputed all data into the Google Spread Sheet by hand. And then analyzed using the pivot table. We used Python for more detailed analysis. For example, to find out which conglomerates the industrial accidents are affiliated with, we created a program using Python that contrasts the workplace where industrial accident occurs with name of the affiliates. Python also were used analyzing what causes disasters and how many disasters have similar patterns.   We made an archive of how each person died into website as interactive news, which acts as an archive. It was created using html5/css3 and jquery and javascript. Using the D3.js library, 2,000 dead workers were visualized as small icon of human figures. The colors and forms of icon's shapes were expressed differently depending on the type of cause of the accident so that users could see that 50% of the death were caused by falls at a glance. Users also can categorize dead workers by age, proficiency, industry, size of business, accident type.   We visualized the distribution of daily deaths by heat map based on D3.js. death place. we marked the place of death on the map using Leaflet library.     "," The hardest part was collecting and organizing data. In order to understand the current status of industrial accidents, we requested information disclosure to the Ministry of Labor and obtained a list of accident reports from 2016 to September 2019. And we received 1,305 accident investigation reports that written form last year to September this year with the help of the Democratic Party of Korea's lawmaker Han Jung-ae's office. Because reports were in PDF file format, so we manually inputted them all into the spreadsheet. It took about a month and a half to complete data input. Refining data also wasn't easy because the list received from information disclosure, the list received through the lawmaker's office, and the list extracted from accident report were different. Until the day before the report, we had been revised the number of deaths.   It was the first time in Korea that an article compiled and analyzed all industrial accidents that occurred during a certain period of time. we also published the newspaper that filled front page with the dead worker's name, age, and cause of the accident. It was evaluated as an unconventional attempt. It was rough, but it had a great effect as an infographic.   Three people a day, more than 1,000 a year, die, but people don't know this very well. Our reports reminded many people of such a terrible reality once again.     "," The Old media is in crisis in these days amid the distrust of the media and the emergence of new media. Out report showed the possibility of what role traditional media can play by revealing structural problems and providing insights through extensive data collection and analysis in industrial accidents, a traditional field of reporting.   The combination of paper infographic and digital interactive news was first attempt in Korea. The subscription rate for newspapers is decreasing day by day. Many people think that newspaper pages are already old. However, newspapers are 'the biggest media' you can carry around with your hands. I think the experiencing of the properties of this medium deeply would impressed on current readers. We published the entire front page filled with a list of deaths, using the properties of this media. It delivered effectively messages to readers. On the other side of paper work, we created interactive news in digital space. It is archive of the dead workers on the web in the form of digital interactive news and deliver it to more people.   I think we presented one option to the media pursuing orthodox journalism.     ",http://news.khan.co.kr/kh_storytelling/2019/labordeath/,http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201911210600155&code=940702,http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201911210600145&code=940702,http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201911210600115&code=940100,http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201911250600025&code=940702,http://news.khan.co.kr/kh_news/khan_art_view.html?art_id=201911280600055,http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201911280600015&code=990105,,"Kyungsang Hwang, Jihwan Kim, Minjee Choi, Areum Lee, Yoojin Kim"," Our team was organized for this report. Three reporters and two Web developers worked together. I(kyungsang) was in charge of overall planing and coordination. I joined as a reporter and wrote articles through traditional reporting departments, and in recent years, I have been producing interactive news and reporting data journalism. Not only did the writing an article, but also did coding, data refining and analysis. ",15 Jan 2020
United States,Monterey County Weekly,Small,Exclusive data from the Pentagon's language school offers insight into America's shifting foreign priorities.,"Explainer, Database, Infographics, Chart, Terrorism","Microsoft Excel, Google Sheets, CSV"," The Pentagon’s elite language school happens to be located in our town of Monterey, California. Known as the Defense Language Institute, it is an important but opaque part of our local community. I filed a data request for the number of students enrolled in each language at the institute going back to the school’s founding in the 1960s. The data eventually arrived—but in an arcane format. I cleaned, analyzed, and visualized the data, mapping out how the curriculum and enrollment levels changed over time in response to the shifting priorities of U.S. foreign policy.      "," The Defense Language Institute looms large in our town but the community knows little about it. I was completing my story just as the U.S. assassinated top Iranian commander Qassem Soleimani. With the help of the data, and sources like former U.S. Secretary of Defense Leon Panetta showed that alumni of the institute had possibly been involved in the operation.    The story became the talk of the town, and is still referred to by readers even a year later. We are always being asked to apply the same data journalism techniques from that story to new topics. "," I filed a request with the U.S. Army for data on the enrollment levels for each language taught at the Defense Language Institute going back to its founding. After a few months, I received hundreds of pages compiled into one PDF. The pages were computer printouts that had been scanned and digitized.    They showed tables of numbers, but since they were manually scanned, the tables were all slightly crooked or morphed. I tried using various OCR and extraction tools to scrape the tables and converted them into CSV files but nothing worked well enough. Eventually, I tried Abbyy FineReader and I managed to do it.    Once I had a master CSV file, I used Google Spreadsheets and Excel to clean the data. I sorted and filtered and created pivot tables to study the data. I quickly obtained facts that even the language institute didn’t and couldn’t know, such as the top language for each year, the top language overall and even just the list of all languages ever taught at the school.   From there, I used Google Flourish to visualize the data. My visualizations included various interactive charts including a bar chart race. I also created a searchable database with information on each language taught. The story also includes a link allowing the public to download the data and use it as they see fit.  "," The hardest part of this project was working on it alone. I am part of a very small newsroom with no history of data journalism and no one else with data skills. I had to pitch the idea and convince my editor it was worth the effort. Then I had to figure navigate the U.S. Army FOIA process which includes writing to lovely addresses such as ""usarmy.belvoir.hqda-oaa-ahs.mbx.rmda-foia@mail.mil."" I also had to do the data analysis and visualization on my own, figuring out whcih skills and tools I am missing and acquiring them as I went. "," One of the most important takeaways is probably the potential to blur the divide between local stories and national/global ones. In virtually every community, there are institutions or people who play a part in some international saga. Local readers want to know to just about their own communities' affairs but also about how their community fits into the larger world. And because data reporting is still in its infancy in local journalism, there are a million unique data sets like the one I found waiting to be requested.      ",https://www.montereycountyweekly.com/news/cover/exclusive-data-from-the-pentagon-s-language-school-offers-insight-into-america-s-shifting-foreign/article_3e1cf8fa-37de-11ea-8637-f3432fc92073.html,,,,,,,,Asaf Shalev," Originally from Los Angeles, I am a staff writer with Monterey County Weekly. Before joining this small alternative news weekly on the Central Coast of California, I worked in Israel, co-founding an outlet to cover the country's substantial tech industry. I gained my data skills while studying and then working at Columbia Journalism School. I participated in two big data projects at Columbia. One was about the cruise ship industry and was published by Univision. The other was about the greenhouse gas emissions produced by foreign infrastructure built on subsidies from the Obama administration.      ",16 Jan 2020
Spain,Lavanguardia.com,Big,What's going on with space debris?,"Investigation, Explainer, Documentary, News application, Infographics, Video, Satellite images, Environment","Animation, 3D modelling, Json, Adobe Creative Suite, Anime.js",  This piece explains the increasingly worrying situation of the remains of space launchers in the different earth orbits and how they can endanger new missions with the danger of collisions. All this in the context of launching new projects based on microsatellites for worldwide internet coverage.        ,"  The project was published in the science section of the newspaper La Vanguardia on the launch day of March 2020 in Google Amp stories format. Its diffusion was extended to social network channels, through a video version for YouTube, Twitter and Instagram TV, as well as in stories format for the same social network.        ","  The first process was based on the modeling, texturing and rigging of the satellites and rockets involved, to later illuminate and animate it. For this, Maya Autodesk and its Arnold rendering engine were used. Once the animation sequences were rendered in 3D, we went on to video montage and post-production using After Effects and creating the sequence of all the phases of the mission and the detail of the instruments. Once all this was done, the pieces were rendered in mp4 weight optimized to mount them in the Google AMP Stories format in which we assembled the entire sequence. All of them on the wordpress platform.","  The most complex thing was to visualize the saturation situation in the Earth's orbits and to indicate the deployment mode of the new Space X starlink satellites, all unified in a single aesthetic visual format. As well as combining several narratives in vertical scrolling together with the base text of the piece."," I think that in this project, editors may be more aware of the advantage of animation as a narrative and descriptive tool, both in science projects and in many other fields of journalism. Likewise, you can also see how these projects can have output on multiple channels (video, social networks, stories) since they are designed to be multiplatform both in format and composition, to reach the largest number of users and readers. ",https://stories.lavanguardia.com/ciencia/20201202/30070/la-basura-espacial,,,,,,,,"Pablo González Pellicer, Mario Chaparro.","  Born in Valladolid (Spain) in 1976, Graduated in History from the University of Oviedo in 2000, later I graduated in graphic design at the Elisava school in Barcelona in 2004 and since then I have worked in digital agencies carrying out multimedia projects until entering in the newspaper La Vanguardia in 2013, where I work as a designer and infographer, carrying out 3D, multimedia and usability and infographic projects in the multimedia department of the web.",20 Dec 2020
United States,The Pudding,Small,The Physical Traits that Define Men and Women in Literature,"Quiz/game, Illustration, Infographics, Chart, Arts, Culture, Women","Animation, Scraping, D3.js, Canvas, Python"," This project mines the text of 2,000 books to find the adjectives used to describe men and women's body parts.  ", The project confirmed quantitiatvely what women have known qualitatively: that women are often described in streotypically sexist ways.  ," The data collection was done in Python. Then, we worked with an illustrator to weave drawings throughout the narrative. They were imported as SVGs and the animated with Javascript. The project also used D3.js and Scrollama.js for scrollytelling. "," The hardest part was pacing all of the illustrated elements and making sure that even the dataviz, which often is rendered pixel perfect, looked sketchy. "," The more different types of skills you bring into a project, the better the output. ",https://pudding.cool/2020/07/gendered-descriptions/,,,,,,,,"Erin Davis, Liana Sposto, and Matt Daniels"," Erin Davis makes stuff with data. She also likes cats, gardening, and reading lots of books.   Liana Sposto is a freelance illustrator living on a sailboat with her husband in sunny Southern California. Today she saw a sea turtle!   Matt Daniels is a member of The Pudding. He first experienced Internet fame in 2014 and has been chasing that feeling ever since. ",1 Jul 2020
United States,The Pudding,Small,How is flooding affecting your community?,"Explainer, Solutions journalism, Multiple-newsroom collaboration, News application, Chart, Map, Environment","Personalisation, Python"," This project uses data on flooding for every property in America to create shareable, geolocated charts and maps. ", The project was used by Gannett newsrooms across the United States to help tell flooding stories in their local communities.  , The project heavily relied on MapBox for the final persentation. Phython was used to clean and process data. , The hardest part of the project was making sure that each chart and map rendered localities correctly and that no place slipped through as an edge case in the data. , Journalism is often competitive and sometimes it shoudl instead be collaborative for the common good of the public. ,https://pudding.cool/projects/flooding/visuals/,,,,,,,,Matt Daniels, Matt Daniels is a member of The Pudding. He first experienced Internet fame in 2014 and has been chasing that feeling ever since. ,29 Jun 2020
Russia,"Mediazona, OpenDemocracy",Small,Brutalised Minsk: how Belarusian police beat protesters,"Investigation, Explainer, OSINT, Illustration, Infographics, Map, Politics, Crime, Human rights","Scraping, D3.js, Json, Google Sheets, CSV, OpenStreetMap, Python"," This year Belarus has been rocked by mass demonstrations against Aliaksandr Lukashenka, who has ruled the country for more than 26 years. Belarusian police have been dispersing protesters, but every week thousands of residents of Minsk and other cities come out onto the streets.   Mediazona received access to data held by Belarus' Investigative Committee. These data show that the minimum number of people who suffered violence during protests in August and September this year is 1,406. In this article, Mediazona shows how Belarusian law enforcement beats — systematically and with complete impunity — protesters. "," This is the data-investigation, so we think that our visualisation of this data is maybe even more important here than the text of the article.   We have proved the huge level of violence against protesters in Belarus. It was common knowledge that police severely beaten demonstrators, but we managed to reveal the huge number of victims and to show in details, how and where they were wounded and tortured (the real number of victims if definitely higher, but we couldn't prove more cases).   Most independent news media in Belarus and Russia published results of our investigation, both media and political activists still use it when they talk about protests in situation with human rights in the country.    Our media was blocked by the Belarussian government — we think that this political decision was the reaction to this investigation (as well to other our publications). "," We used python, Open Refine and Google Sheets to clen, to organise and to analize our data — we dropped many duplicates, find all locations that were mentioned in this leaked data, we manually analized it and divided it into several types of injuries (how hard this person vas injured, where, what type of weapon used the pilice etc). Then we converted this data to json.   Then we made the visualizasion, we use OpenStreet Map to create the map of places, where demonstrants were beated. We used D3 to create our 'online demo' whre you can find information about any of victims and select them by some types. "," The hardest part was to analize all this leaked data — we had to merge many separate tables, to clean them, to edit this information (it was collected by Belaros Investigative Committe for its own purposes and it wasn't possible to use it without editing), to define types and hardness of injuries etc. The second hard thing was to convert this analized data to our visualization.  "," How to show the levet of political repressions and police brutality in countries like Belarus, where you don't have an opportunity neither get this data officially nor collect it separately. Huge amount of victims was common knowledge before our investigation, but we first have proofed certain number and shown how many people were severely beaten.  ",https://mediazona.by/article/2020/11/03/minsk-beaten-en,,,,,,,,"Dmitry Treshchanin, Maxim Litavrin, Anastasiya Boika, Yegor Skovoroda, David Frenkel, Nikita Shulaev, Maria Tolstova, Anastasia Poryseva, Khatima Mutaeva, Viktoria Rozhitsyna, Mikhail Lebedev"," Dmitry Treshchanin and Yegor Skovoroda are editors in Mediazona, Maxim Litavrin is staff writer for Mediazona, David Frenkel is a programmer (and photographer), Nikita Shulaev is a designer, Maria Tolstova — an illustrator, they all work for Mediazona and Mediazona.Belarus. Anastasiya Boika is staff writer for Mediazona.Belarus. Anastasia Poryseva, Khatima Mutaeva, Viktoria Rozhitsyna and Mikhail Lebedev hepled us to analyze the data.  ",3 Nov 2020
Brazil,"Fogo Cruzado, Pista News, Disque Denúncia, Geni/UFF, Nev/USP",Small,Map of the Armed Groups of Rio de Janeiro,"Investigation, Solutions journalism, Multiple-newsroom collaboration, Database, Open data, News application, Crowdsourcing, Infographics, Map, Crime, Gun violence, Human rights","QGIS, Microsoft Excel, CSV, RStudio, OpenStreetMap, Python", The 2019 Rio de Janeiro Armed Groups Map was an unprecedented pilot project carried out by 5 organisations with the objective of demonstrating the territorial reach of drug factions and militias in the state of Rio de Janeiro and at the same time providing journalists and researchers with a tool to better analyse and understand the living conditions of the Rio's citizens and the impacts of public security choices. The project was inspired by the certainty that not It is a project that believes in the importance of quality and free information to transform the reality of citizens. ," Territorial control is one of the historical and distinctive characteristics of the dynamics of armed groups in Rio de Janeiro since the 1970s, and an unavoidable variable not only for public security, but also important for other urban public policies, from transportation and housing to education and health. Over the years, groups have emerged, fragmented and grown. But despite it's failure, the state's response over the past 40 years has remained the same: confronting these groups through direct confrontation operated by the police, aligned with the international policy called War on Drugs. As a consequence, beyond its natural beauties, Rio has become known for violence and stray bullets. Here, control over space is disputed by guns, both by organized crime and by the state itself. This situation exposes the population at any time to the crossfire, and affects the supply of any and all public and private services, especially in the peripheries where armed groups are mostly concentrated. The most tragic effect of this scenario can be understand by the fact that a 6-year-old boy from a favela in Rio has a better chance of being shot than obtaining a college degree. For this reason, it is surprising that the mapping of armed territorial domain has not been carried out to date or that, when done, it has not become of public knowledge and access. The lack of a reliable historical map of territorial control of these groups - and the disputes between them - not only hinders the elaboration and implementation of public policies in Rio, but also hinders the economic development led by private initiative, since any investment made without this information incurs a high level of blind risk. The project lauched in last october adresses exactly this missing information, and envisions to use the prototype to build. "," The project was carried out by 5 organizations with recognized experience in the area of public security (Fogo Cruzado, Pista News, Disque Denúncia, Geni/UFF and Nev/USP). The Disque Denúncia database was chosen as the primary source to classify the presence and control of armed groups over certain areas, due to its richness of details and territorial and temporal coverage. For the 2019 prototype, 37.883 anonymous denunciations were analyzed, which mentioned militias or drug trafficking, using natural processing language techniques. Of these, 10.206 were considered valid after a process of recognition of the groups mentioned and the type of activity mentioned. The valid denunciations were georeferenced, plotted on the map and processed from polygon maps, following statistical/mathematical criteria, to identify whether or not each polygon was controlled by the groups studied. The maps of polygons were constructed from the expertise of Pista News in mapping, slums and housing estates (informal geographic units, smaller than the official neighborhoods recognized by the city halls). The process resulted in a map that exposes the classification of each polygon identified according to the dominant group (ADA, CV, TCP or Militia) or as an area in dispute, according to data from 2019.   The same analysis was also done to evaluate the distribution of groups by neighborhoods (official classification provided by the Rio de Janeiro Public Prosecutor's Office). In 2019, 92% of the neighborhoods in the city of Rio, where almost 98% of its population lives, were partially or wholly under the control or in dispute for armed groups. The group with the greatest prominence were the militias, paramilitary groups formed by state security agents, who controlled partially or wholly 25.5% of the neighborhoods in the capital of Rio de Janeiro, where 33% of its population lives. "," The construction of the 2019 map of armed groups presented 3 major challenges. The first of them was to read and process thousands of anonymous denunciations (which deal with various types of crime), choosing only those that in fact present convincing indications of territorial control by a certain armed group. To solve this challenge, researchers with PhDs in areas related to public security defined objective criteria for determining territorial control and, based on the reading of a sample of the material, built dictionaries for subsequent automatic classification of text by machine, using natural language processing techniques. At the same time, a team of data and statistical scientists, sought to georeference the complaints of the Whistleblower. This task proved to be a challenge not only because of the amount of records, but also because of the low quality of the address information available due to spelling mistakes and standardization not corrected in the original database, but also because of the high level of urban informality of Brazilian cities, which makes it difficult to locate the addresses accurately. The solutions found were organized in an R package to facilitate new georeferencing rounds. Finally, the last challenge was the construction of maps that reflect the reality of urban informality in Rio. The official neighborhood map, made available by the MPRJ, is interesting for analysis of the coverage of militias (that generally operate in entire neighborhoods), but it is not granular enough to understand the reach of drug factions (usually restricted to favelas or housing estates). The solution was to build 2 complementary analyses. The partnership of Pista News, an organization that maps the control of armed groups in Rio's favelas and housing complexes in real time (but not with historical perspicacity) in a collaborative manner, was essential for the construction. "," The project is a tool for journalism in 2 main directions. Directly, by making the map available for free, becoming a tool for journalistic analysis to reveal trends, from corruption relations between some of these groups and local police units, to relations between politicians, through the direct impact of these groups' disputes on the access of population to services.   As the map's shapefile is open, the journalist can cross-check this information with any other data (health, education, sanitation) that have geolocation and produce diagrams that until then were not produced by the government. Such investigations can reveal the impact of the control of armed groups on the daily lives of the local population in various instances.   The project also encourages journalists to look for non-governmental sources of information to report reality and not to get stuck in the political narrative of the facts. In Rio de Janeiro, most newspapers and TV programs do not name factions and militias so that, according to some journalists, they do not value these groups. But that decision proved innocuous. With the map, we show that the action of these groups has grown and that it is important to give names to everyone, ponting who rules where, what is their progress and representativeness.   In other words, the Map of the Armed Groups of Rio de Janeiro is a journalistic opportunity to question the government in a multiciplinary, didactic and transparent way - and claim for better policies.     ",https://erickgn.github.io/mapafc/,https://brasil.elpais.com/brasil/2020-10-19/milicias-ja-dominam-um-quarto-dos-bairros-do-rio-de-janeiro-com-quase-60-do-territorio-da-cidade.html,https://www.time24.news/2020/10/rio-has-3-7-million-inhabitants-in-areas-dominated-by-organized-crime-militia-controls-57-of-the-city-area-says-study-rio-de-janeiro.html,https://www.clarin.com/mundo/milicias-paramilitares-poderosas-narcotrafico-rio-janeiro_0_G4elMpnvx.html,https://atualprodutora.com/wp-content/uploads/2020/10/apresentacao-16.10.2020.pdf,,,,"Maria Isabel MacDowell, Cecilia Olliveira, Daniel Hirata, Bruno Paes Manso, Diogo Lyra, Erick Gomes Nieto, Renan de Sousa e Silva, Natália Maciel Block, Maria Eduarda Barroso, Walkir Brito"," A network of academics, researchers and journalists that translated into numbers the expansion of the power of gangs and militias in Rio de Janeiro.  ",19 Oct 2020
Germany,"Funke Mediengruppe (Berliner Morgenpost, Hamburger Abendblatt, WAZ, Thüringer Allgemeine, Braunschweiger Zeitung and many more)",Big,Divide Germany (again) and discover the differences,"Explainer, Database, Open data, Mobile App, Infographics, Map, Sports, Elections, Politics, Environment, Arts, Lifestyle, Business, Culture, Women, Agriculture, Immigration, Health, Crime, Economy, Employment","Scraping, D3.js, QGIS, Json, Google Sheets, CSV, R, RStudio, OpenStreetMap, Python, Node.js","With an app that we launched for the 30th anniversary of German reunification, readers could divide a map of Germany into two and compare different characteristics of these parts. When flicking through the thirty characteristics users discover interesting, some serious and some funny distribution patterns (from income to ), and learn about regional and cultural differences in Germany in a playful way. The maps show how different social, political and economic factors still divide Germany into two parts today - along differing geographical lines. Readers can also use the tooltip to find out how their home town is positioned in"," The project was surprisingly well-received by an international audience interested in exploring regional varieties in Germany and learning about the country in its nuances. Some readers used the tool to try and draw connections between certain regions of Germany and regions of other countries they knew. Inside Germany, the application was widely shared, including by the federal statistics office whose data the application was largely but not exclusively based on. Although the time of publication was shortly before the day of German Unity, which is a holiday in Germany, and data visualizations on this topic are no surprise, the special take of being able to divide Germany itself, even into North and South or completely different, was something new that appealed to readers. Despite the pandemic dominating the news, the project met with immense interest, more than 100,000 visits on the first day. "," In the research process we used Python as well as different APIs to gather data on characteristics where there was no official data available. RStudio/R was used for wrangling and then visualizing the different datasets on a map of Germany's 401 administrative districts. These preliminary plots were used to choose datasets with interesting distribution patterns, while the final visualizations are rendered by the web application using D3.js based on csv files we generated with R. "," Some of the data was challenging to convert into administrative districts, as was the case for raster-files of average annual sunshine duration or for data based on postal codes. During the web development process the idea of ""cutting"" the map into two by drawing a line was a difficult function to implement. While many organisations were publishing maps showing different socio-political, geographical etc divides, the project managed to find or create unconventional datasets to add, and developing the functionality that allows to compare the average of two parts and individual districts turned it into a unique, engaging user experience. "," Users from an international audience asked for similar projects to be implemented for other countries. We learned that there is specific added value in a story that is not only focussed on one or a few connected metrics, but lets users explore and compare regions across different dimensions. It communicates a more faceted and complex story and history of different places than a story focussing on one aspect can do.  It was also a good example for finding stories in data, as we set out to find datasets that show interesting patterns and based on that found and told stories of different aspects. Some of these stories were new to us as well as most others and wouldn't have been told if it wasn't for the process- (rather than result-)oriented approach we took with this project.    ",https://interaktiv.morgenpost.de/deutschland-teilen-deutsche-einheit-wiedervereinigung/,,,,,,,,"Marie-Louise Timcke, André Pätzold, Angelo Zehr, Ida Flik, Christopher Möller"," Funke Mediengruppe's Interactive team develops interactive applications and data-driven stories for the Group's various news brands. It acts like a small, interdisciplinary task force of data journalists, designers and programmers within the newsroom, is very visually driven and user-focused, and covers various topics ranging from elections to climate change or social inequalities. ",29 Sep 2020
Brazil,Jornal do Commercio,Big,Confere.ai,"Explainer, Breaking news, Database, Fact-checking, Elections, Politics, Health","AI/Machine learning, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, Python"," Confere.ai is a measure of disinformation characteristics, which identifies patterns of false or misleading content in texts or news links circulating on social networks and the internet. The project involves researching, analyzing, and formatting a database with more than 22,000 news items, in addition to developing web crawlers to search for content and computational intelligence for analyzing and looking for patterns. One of the winning projects for the Google News Initiative Innovation Challenge in Latin America, Confere.ai, also produces educational content to teach the public to identify false or misleading content. "," The first initiative to automate fact-checking in communication vehicles in Northeast Brazil, Confere.ai, was selected by the Google News Initiative Innovation Challenge in Latin America. The tool received about 4 thousand contents to check automatically during the first three months of operation. One of the tool's main impacts is the assertiveness in an uncontrolled environment, which was around 92% for texts and 75% for links. Confere.ai generated more than 70 editorial contents about reach, including articles on disinformation, fact checks, and videos with guidance to the public. The work also generated partnerships with the Universidade Católica de Pernambuco - Unicap. It was mentioned in dozens of local media outlets, and journalism course completion works, promoting knowledge about the automation of fact-checking in communication sciences. The articles produced by Confere.ai had 286 thousand hits over six months, while the tool reached an average of 500 hits per day. Confere.ai also managed to identify the waves of disinformation growing in the local media and debunking some rumors. Finally, the tool became part of the National Network to Combat Disinformation (RNCD). "," The creation and development of Confere.ai were carried out in stages. In the first, a hybrid database was set up: manual collection and storage of news links and texts previously checked by checking agencies; use of the corpus Fake.BR, from USP; and development of web crawlers to extract content from four different sites. The material was distributed in a database - with 21,956 news items, distributed in 9,011 texts and 12,945 links - on google sheets, cleaned with Open Refine. Part of this bank was analyzed manually to identify criteria. Creations were made using filters and a dynamic table for data extraction. From that, 15 criteria were defined for text evaluation and 20 criteria for link analysis. In the final analysis, the behavior of the values ​​obtained in each test performed in the database was observed to determine each criterion's relevance, and patterns were found that separated the real news from the misinformation. Part of the database was used for the development and training of computational intelligence. The following were used: the supervised technique (Random Forest -> uses a set of decision trees to form the fine answer) and input data BoW (Bag of Words) (a way of representing information in a text through the quantity or frequency of the words contained in it + linguistic characteristics of the text - extracted through natural language processing techniques). We built The Confere.ai web platform with python and angular. A dashboard was built from it, which facilitates the visualization of the data patterns entered in the tool. We carried out hundreds of data crossings to validate the previously defined criteria and measure assertiveness - which was 75% for links and 92% for texts. "," The creation and development of Confere.ai faced several challenges. The first was not to have similar reference projects in Portuguese, which could guide which technique is the most assertive to find disinformation patterns. Likewise, academic studies on fact-checking automation were lacking to create the criteria for identifying misinformation on the internet automatically. Due to this limitation, the team needed to seek English studies and try to adapt the results achieved to the Brazilian reality.There are also not many Portuguese corpora of disinformation data ready in Portuguese; there was only one database from the University of São Paulo. This, in turn, was out of date, with a series of links from 2016 and 2017 that had already been taken down, further limiting the analysis to identify patterns of disinformation. Likewise, there is a lack of free natural language processing APIs in Portuguese, which limited the identification of grammatical classes, emotional intentionality of words, grammatical errors, etc. This reduced the possibility of using many of the criteria for identifying misinformation applicable to English-language projects.To overcome these difficulties, the team used two actions. He performed the manual collection of previously classified misinformation, based on the analysis of three years of checks carried out by Brazilian checking agencies. The team also developed web crawlers to extract uninformative texts found on the Boatos.org website and informative texts found on the JC Online, Diario de Pernambuco, and G1 sites. This, however, presented different programming standards, which generated the need to study how each site was created and a dozen tests by the crawlers. In the end, we still face the challenge of convincing people to access a platform to check disinformation independently. "," The Confere.ai project offers a great contribution to the field of automation of fact-checking in Brazil. One of the pioneers in offering a solution that breaks the intermediaries between the uninformative piece and the readers' doubt. It is a project that contributes to thinking about using technology to combat disinformation in Brazil, which has proven to be a powerful engine for the destruction of public debate in the last two years.The originality of the project lies in, precisely, trying to propose a solution that shortens the distance between the content to be checked, the audience that receives it, and the final result of the check. Although it does not indicate whether something is true or false, it proposes to create a 'flea behind the ear' and a critical census in public.The project also innovates by developing a series of technologies capable of searching content on specific pages to form a database with more than 22 thousand contents; something is never before done in Northeast Brazil. This not only makes it possible to improve the tool, but it can also serve as an instrument for consolidating other fact-checking projects in the country's journalism.The differentials of Confere.ai are the creation of a list of criteria, based on the study of patterns of disinformation in text content and news links, which can be made available to other interested journalists and researchers in the communication area; It is also innovative in the application of artificial intelligence techniques in communication vehicles in Northeast Brazil and can serve as a reference for studies on AI in journalism. ",https://confereai.ne10.uol.com.br/#/consultar,https://drive.google.com/file/d/1KN9YjZQXSUjR9edudn0k8AXYsqmDo-HI/view?usp=sharing,https://drive.google.com/file/d/1EHKXSiUYfB1E80eSjVbx5Z9l377VPr0_/view?usp=sharing,https://www.youtube.com/watch?v=NOaEF7FEL1g,https://www.youtube.com/watch?v=BnPtD8Ij9FQ,https://www.instagram.com/projetoconfere.ai/,https://bjr.sbpjor.org.br/bjr/article/view/1178,,"Alice de Souza, Matheus Marinho, Avelino Gomez, Lucas Pitt, Maria Luiza Borges, Dario Brito, Anthony Lins, Debora Oliveira, Lais Arcanjo, Pedro Brasil, Jarbas Agra"," Alice de Souza is a Brazilian journalist, postgraduate in Human Rights. She has a Master's in Creative Industries from the Catholic University of Pernambuco (Unicap) and is currently pursuing a degree in Freedom of Expression, offered by the Sociedad Interamericana de Prensa (SIP) and Universidad Católica Andrés Bello (UCAB). She is the editorial coordinator of the Confere.ai project, selected by the Google News Initiative (GNI) Innovation Challenge in Latin America, in Jornal do Commercio. Before that, she was a reporter for the newspaper Diario de Pernambuco for 10 years. Alice is also a collaborative reporter for the Retruco Independent Journalism Agency. Dedicated to investigating issues of sustainable urban development, health, and human rights. She is 29 years old and has articles recognized in more than 30 local, national, and international awards. She was twice winner of the Cristina Tavares Award and a finalist in the 2nd and 5th editions of the Roche Health Journalism Award, promoted by the Gabo Foundation. She is co-author of the book Panamá - La ciudad entre papeles, of the Gabo Foundation and Concolón Panamá. Alice is a former Cosecha Roja fellow and member of the 3rd generation of the Red de Jóvenes Periodistas de América Latina Distintas Latitudes. She was the most awarded journalist in Northeast Brazil in 2018 and 2019 and the 14th most awarded journalist in Brazil in 2019, according to the ranking by Jornalistas e Cia. ",24 Sep 2020
United States,Parametric Press,Small,Your Personal Carbon History,"Explainer, Quiz/game, Open data, Infographics, Chart","Canvas, CSV, Python, Node.js"," I researched, wrote, and coded an interactive article that centers the geological history of carbon dioxide on Earth around the reader’s life [1]. The article zooms out over millions of years to explain why we’re living in a geologically unprecedented time. This piece grew out of an article in my climate newsletter The Rate of Change [2]. It was published in Parametric Press [3], an experimental online publication for interactive explanatory journalism, with feedback and input from their editors. "," The article was used as a teaching tool and discussed in an environmental science course at Cornell University. I was interviewed about it for the Numlock News newsletter by Walt Hickey, and by The Open Notebook, as part of an interview about my climate science & data newsletter (the Rate of Change). ", I used Idyll and React for interactivity. I used the javascript library p5.js for canvas animations and plotly.js for graphing. ," There are a few aspects that were particularly challenging:   1. My goal was to create an interactive narrative that personalizes the history (and prehistory) of carbon dioxide around the life of the reader. As most interactive frameworks are designed for creating modular interactive elements rather than a customizable narrative, this was a technically complex project that required me to push the envelope in what was possible with these tools.   2. I went through many unsucessful iterations of visualizing the (somewhat abstract and technical) unit of 'parts per million', before arriving at the starfield visualization in the article. This interactive visualization uses motion and depth to convey how minute levels of a greenhouse gas can add up to a large effect.   3. As part of the customizable narrative, I wanted to annotate graphs with events that are specific and relevant to the reader's life. This was a considerable technical challenge, requiring a significant amount of custom code to adapt the plotly library to my needs.   4. Another challenging aspect was presenting this article in a responsive way that scaled and was interpretable on multiple screen sizes, which required a fair amount of custom typography scaling code and visual tweaks. "," Other journalists can learn how to present a large, technical dataset in a manner that is personal, relatable, and customized to the reader's life. They can learn about how to visualize abstract & intangible quantities in tangible ways, and to bring context to deep historical data. The code for this interactive is open-source and available on GitHub [4].     In the spirit of open data & reproducible reporting, I have also published the Python code notebooks that I created to compile the paleoclimate data [5]. ",https://parametric.press/issue-02/carbon-history/,https://rateofchange.substack.com/p/the-rate-of-change-july-15-2019,https://parametric.press/issue-02/,https://github.com/ParametricPress/02-carbon-history,https://github.com/aatishb/climatedata/blob/master/Parse%20CO2%20Data.ipynb,,,,Aatish Bhatia," Aatish Bhatia is an award-winning science writer, educator, and physicist. He creates articles, videos, and interactives that explain complex ideas in simple ways. In 2020, he developed widely-viewed videos and interactives about COVID and the climate. Previously, Aatish taught courses on science, art, engineering, & music at Princeton University. His science writing has been published online in WIRED, Nautilus, Minute Physics, and TED-Ed, and has been highlighted in online venues including NPR, The Guardian, Discover, National Geographic, Scientific American, and The New Yorker.   The Parametric Press is an experiment, a born-digital magazine dedicated to showcasing the expository power that’s possible when the audio, visual, and interactive capabilities of dynamic media are effectively combined. ",19 Oct 2020
Malaysia,Malaysiakini,Big,The Kini News Lab Covid-19 tracker,"Explainer, Database, Open data, Fact-checking, OSINT, Crowdsourcing, Chart, Map, Health","Personalisation, Json, Google Sheets, OpenStreetMap"," The project is a website which includes a dashboard of key statistics regarding the Covid-19 pandemic in Malaysia (at national, state, <a href=""https://web.archive.org/web/20201011000305if_/https://newslab.malaysiakini.com/covid-19/en/state/selangor"">district and subdistrict level ), verified locations affected by Covid-19, cluster information, patient and death information, resources on how to stay safe during the pandemic and rules and regulations of lockdowns and other related information.    It is published in <a href=""https://newslab.malaysiakini.com/covid-19/en"">English , <a href=""https://newslab.malaysiakini.com/covid-19/my"">Bahasa Malaysia  and <a href=""https://newslab.malaysiakini.com/covid-19/zh"">Chinese , with some key parts of the website published in <a href=""https://newslab.malaysiakini.com/covid-19/be"">Bengali , <a href=""https://newslab.malaysiakini.com/covid-19/ne"">Nepali  and <a href=""https://newslab.malaysiakini.com/covid-19/mm"">Burmese  to cater to the more than one million migrant worker population who are not English or Bahasa Malaysia literate. "," The dashboard and information website was the first to be produced in Malaysia providing a bird’s eye view of the pandemic in the country as well as key information about lockdowns and how to stay safe at this time. Since its launch, it has received 30.1 million page views and counting.   Unlike the official releases which are only in Bahasa Malaysia, the tracker publishes in six languages, including three used by blue-collar migrant workers communities who have limited literacy in English and Bahasa Malaysia and are unable to access such information otherwise.   When the pandemic started, much of the data was not provided to the public. To fill this gap, Malaysiakini reporters pressed the authorities for the data on a daily basis, forcing them to finally provide such information to the public every day.   One key information which the government continues to withhold are footprints of those infected. In other countries, like Singapore or Thailand, this is routinely shared as part of contact tracing so those who visited those locations at the same time could monitor themselves for symptoms and seek testing.   To date, the tracker is the only place where Malaysians can find such verified information nationwide. To this day, we receive daily alerts from communities about localised outbreaks in their area, which are not announced by the government.   The format in which the data is collected has also helped academic researchers in this field. We have provided the data to several universities upon request.   State government agencies are also using the tracker as a resource. The Selangor state government, for example, uses the data collated on locations affected by Covid-19 daily in their monitoring of local outbreaks and roll out prevention and containment strategies, including free community testing. "," The daily numbers are recorded on a Google spreadsheet. We then exported it into several JavaScript Object Notation (JSON) files.   We use JavaScript to write several scripts to parse the files to obtain the data we needed to visualise the numbers with different charts and tables on the tracker page.   The charts and tables were created with Highcharts JS API and react-table respectively.   The structure and layout of the game were built with JavaScript, JavaScript library React and React framework Next.js.   We also used a user interface framework Material-UI and CSS framework UIKit for the game’s user interface.   The graphic was done with Adobe Photoshop.    "," Collating the data and navigating the various releases to make sense of it, has been and continues to be the hardest part. Some of the information was not provided initially and had to be requested on a daily basis.   The data releases were also inconsistent, with some data released for a few weeks and then never again, or the information patchy.   Because state health departments can decide how, when and what data to release, the information is to this day, inconsistent across the states. One term would be used to refer to different things according to the state or some information is released by these states and not the other.   Unlike in many countries, the data is not provided in any form of API or even a machine-readable format, but through Facebook albums of <a href=""https://www.facebook.com/kementeriankesihatanmalaysia/posts/10157672322256237?__tn__=%2CO*F"">photos of charts .   This has forced us to spend up to four hours daily to input the information manually. (Various attempts at automating the data were unsuccessful - it took longer prepping the .jpg charts for conversion that it did to manually input the data)   Collating and verifying the information on locations affected too is very challenging and continues to be a labour intensive endeavour.   As we are the only organisation to do this, there is high expectation from the audience for the information to be clean, up to date and timely.   Managing these expectations on a daily basis is one of the greatest challenges, especially as the very small team working on the tracker became smaller as part of the team had to move on to other projects, leaving only one person to work on the tracker full time.   This has been especially challenging this year, when we decided to retire <a href=""https://web.archive.org/web/20201011000305if_/https://newslab.malaysiakini.com/covid-19/en/state/selangor"">part of the tracker , due to a lack of resources. "," While information may be publicly accessible it doesn’t mean it is in fact accessible to many. It may be convoluted, in a language they don’t understand, in platforms they can’t access or just simply difficult to keep track of. Data journalists can play a role to sort, clean and present that information in a way that is easier to consume.   On a project management side of things, a key learning for us is how to manage monsters which we create. By this, we mean when rolling out a project like a tracker dashboard - especially one which somehow becomes an essential resource for many - it is important to plan and try to anticipate the trajectory of the project.   This will help in planning resources to allocate and how to manage expectations by readers on the project. ",https://newslab.malaysiakini.com/covid-19/en,https://newslab.malaysiakini.com/covid-19/zh,https://newslab.malaysiakini.com/covid-19/my,https://newslab.malaysiakini.com/covid-19/ne,https://newslab.malaysiakini.com/covid-19/be,https://newslab.malaysiakini.com/covid-19/mm,,,"Aidila Razak, Lee Long Hui, Wong Kai Hui, Sean Ho, Thiaga Raj Servai, Hazman Hazwan, Syariman Badrulzaman"," The project is presented by <a href=""https://newslab.malaysiakini.com"">Kini News Lab  in <a href=""https://www.malaysiakini.com"">Malaysiakini , one of the most-read news portals in Malaysia.   At Kini News Lab, we experiment with new ways of presenting news by combining visual and interactive storytelling as well as in-depth and data-driven journalism.   We aspire to turn important but complex issues into something that is engaging and enjoyable for the Malaysian public. ",10 Mar 2020
Malaysia,Malaysiakini,Big,Spend Like an MP,"Explainer, Long-form, Quiz/game, Illustration, Elections, Politics","Personalisation, Json, Ink.js"," The “Spend Like an MP” project comprises a special report and a dedicated game.   Built based on interviews in the special report, the game helps readers simulate the role of an MP in Malaysia.   They have to carry out their duties while maintaining their finances and popularity with constituents.   It aims to highlight what MPs do and how they juggle between their constituents, their salary and allocations as well as their personal lives.   The special report discusses the problems with the current lopsided allocation system in Malaysia, what the MPs think about their responsibilities and the way forward.    "," The project was well-received, particularly among younger readers who were previously not interested or aware of this policy problem.   The game was played more than 70 thousand times since launch day and was virally shared on social media platforms.   The project also successfully boosted our subscription revenue between 26 and 41 percent for three consecutive days since its launch day. It shows that the Malaysian public is willing to support good content.    "," We used Ink, a scripting language, to write and test the game script on Inky, the Ink editor. The script was exported as a JavaScript Object Notation (JSON) file.   We then used JavaScript to write a script to parse the JSON file to obtain the data we needed to present the game on web pages.   The structure and layout of the game were built with JavaScript, JavaScript library React and React framework Next.js.   We also used a user interface framework Material-UI and motion library framer-motion for the game’s user interface and animation.   The graphic was done with Adobe Photoshop.    "," In Malaysia, the public expects a lot from MPs beyond their actual job scope - to debate and enact laws in Parliament.   The government and opposition MPs receive an unequal amount of allocation for their constituencies.   The Malaysian public tends to react with outrage when MPs get a salary hike but many are not aware of various issues, including having to spend out-of-pocket for requests by constituents, especially for the opposition MPs who are discriminated against in terms of allocations.   The gamification of a lawmaker’s role is a novel approach to educating readers in Malaysia.   Presenting the issues through a special report with a tailored-made game from the MPs’ perspective helps us to better convey the message in a unique, innovative and engaging way.   The most challenging part of creating the game was to strike a balance between user experience and the key information we want to present.    We wanted to find the best length that can effectively deliver the message without risking fatigue while at the same time is sufficient to help users appreciate the MP’s financial dilemmas and challenges.   We comprise a small team with one graphic designer, one UX designer, one journalist and one journalist/developer. None of us had experienced in a project like this. The final product went through several iterations. We kept testing until we thought it was ready. It was a painstaking process.   Another challenge is data collection. Our MPs are not required to make their spending public. As such, we interviewed many MPs to get insights on how they spend their money.    The process, which was hampered by the Covid-19 pandemic, took up to two months. We were unable to independently verify all the interview information through official sources but gained a good idea by comparing what the various MPs told us. "," We think that gamification of special reports is one of the ways forward for news organisations to cope with changing audiences.    Instead of presenting a wall of text to the readers, news-based games enable journalists to explain complex policies and systems in an interactive and engaging way.   This can be helped by journalists with programming skills and knowledge. They can implement the logic flow of their script and build the site/pages for the game.   In the absence of programming or developer experience, a newsroom can still initiate collaborations. Talents can be found in game design schools in your local colleges or universities where they encourage students to take part in real-work projects.   Another aspect we would like to share is the “freemium” strategy we implemented to boost subscription.   The game can be played for free and provides the core information but if they would like to learn more, they are given an excerpt of the full article. To access the complete content, a subscription is required.   We also have prominent calls-to-action for subscription and donation at the end of the game. We received feedback from some readers that they decided to subscribe to Malaysiakini after playing the game as they wanted to read the article to understand more. ",https://newslab.malaysiakini.com/mp-game/en,https://www.malaysiakini.com/news/552386,,,,,,,"Lee Long Hui, Geraldine Tong, Nigel Aw, Syariman Badrulzaman, Hazman Hazwan, Sean Ho"," The project is presented by Kini News Lab in Malaysiakini, one of the most-read news portals in Malaysia.   At Kini News Lab, we experiment with new ways of presenting news by combining visual and interactive storytelling as well as in-depth and data-driven journalism.   We aspire to turn important but complex issues into something that is engaging and enjoyable for the Malaysian public.    ",25 Nov 2020
South Korea,Hankookilbo,Big,The Watchers Overhead: The Korean Peninsula and a Silent ‘Satellite War',"Solutions journalism, Open data, Mobile App, Infographics, Chart, Map, Satellite images","AR, 3D modelling, D3.js, Three.js, Canvas, JQuery, Json, Microsoft Excel, CSV, Anime.js, Node.js","The Korean Peninsula is one of the world's last remaining divided regions — at conflict for over 70 years with only the demilitarized zone separating the two Koreas. Countless satellites from world powers such as the United States, Russia, China and Japan orbit the airspace above the peninsula, owing to the region's strategic importance. Hankook Ilbo analyzed some 40,000 pieces of space surveillance data to map and simulate in 3D satellites that cannot be seen by the naked eye, revealing the importance of satellites for national defense, the state of Korean satellite development and military tensions in the unseen ‘satellite"," Following our reporting, previously undisclosed information about future plans and the current state of satellite development — such as new launch vehicles, surveillance and GPS satellites — has now been released to the public.   The tools we developed for this project won praise from government agencies, such as the Korea Aerospace Research Institute and the Korea Coast Guard, and experts in the field. We also received numerous inquiries about our tools and development.   We hope that the project and the tools and software used to report the project, which analyzes satellite data in real time and presents them in Korean, will raise interest in the field of artificial satellites and find use in the classroom as well as the newsroom.    "," We sourced our satellite monitoring data from Space-track.org, operated by the United States Strategic Command, and SpaceBook, developed by the firm Analytical Graphics. We analyzed the data using Excel, transforming Cartesian coordinates into latitude, longitude and altitude.  We incorporated the following tools and techniques:  •    3D interactive web-based presentation of satellites and their orbits around the Earth using Three.js  •    Bulk data processing and organization through the D3.js library  •    Chart rendering with chart.js  •    Augmented reality using the reader’s smartphone camera, GPS and gyroscope  •    Real-time tracking of satellite locations with Web APIs  •    3D rendering and real-time search of orbiting satellites with WebGL  •    Transitions and animation using CSS3  •    Real-time satellite location visualization using the open-source Stuff in Space code    ", Optimizing data presentation and storytelling for both desktop and mobile environments proved to be our biggest challenge. We paid special attention to maintaining the flow between technologies such as augmented reality and satellite tracking while effectively presenting tens of thousands of visual data points in real time. ," Our project demonstrates that data journalism can present not only statistical analysis but also raw data in a visually and technologically appealing way, showing journalists a way to provide readers with a new storytelling experience. ",https://interactive.hankookilbo.com/v/satellite/index.html,https://interactive.hankookilbo.com/v/satelliteviewer/index.html,,,,,,,"Ahn Kyungmo, Park Inhai, Kim Jungyoung, Hwang Daehan, Oh Junsik", Hankook Ilbo Media Platform Team ,16 Mar 2020
France,Contexte,Small,"Oldies but goodies? We looked at 40 years of ""anomalies"" in French nuclear plants","Investigation, Explainer, Database, Infographics, Map, Politics, Environment","QGIS, Google Sheets, Python"," For the first time, journalists were granted access to detailed information about 30.000+ so-called “significant security events” that took place in French nuclear power plants since 1977. We explored this huge and very technical dataset to answer a highly sensitive question: are older reactors really more dangerous?   Working closely with independent experts from the Radioprotection and Nuclear Safety Institute, we established that even if the number of incidents is not rising as a plant gets older on average, a growing share of the reported anomalies are caused by / related to the aging of the facilities. "," The results of this months-long investigation fueled a long debate about the future of existing power plants. France relies heavily on civic nuclear power to produce the electricity it needs (70%, a world record), and 20+ of the 58 nuclear reactors built in the country are nearing the age of 40. Decisions are to be made about a possible life extension, that would require costly and time-consuming renovation processes.   “Pro-nukes” and “anti-nukes” spend a lot of energy promoting their views, and the climate crisis has raised the stakes even more: despite all his flaws, nuclear energy is a low-carbon mean to generate electricity, Therefore, some experts consider that keeping the current power plants up and running offers a favorable risk-reward ratio. Others warn that the probability of an accident is getting higher by the day, and that a nuclear-free production mix is a possible goal to reach by 2050, even in France, if the country shifts its focus towards renewable energy.     Yet until the release of our story, the actual publicly available data about what really happen in French power plants was scarce. Occasionally, journalists ran stories about a particular incident, based on press releases from official agencies or environmental NGOs. But they could not put it in a broader context and therefore attempt to draw a conclusion.   Even if Contexte is a B2B independent news outlet relying on subscription, we decided to remove the paywall for such a public-interest story. Our work was praised by the most skilled experts in the field, and heavily commented on social media by stakeholders from all sides. It's by far the most read content of our website in 2020. "," We used Python and specifically the Pandas module to parse, clean and explore the 30 MB of Excel files that were provided by the Radioprotection and Nuclear Safety Institute (RNSI). We produced 60+ summary tables to be imported and studied in Google Sheets. We came up with 25+ charts created with Datawrapper and embedded the most relevant of them in the main story.   We also released an <a href=""https://www.contexte.com/article/energie/entretien-aux-sources-de-notre-enquete-sur-40-ans-danomalies-de-surete-nucleaire_111654.html"">additional interview  with two experts from the RNSI, to give more information about the database itself and how they used it today. Called “Sapide”, it was created in the 1970s by the RNSI. Its first iteration was a collection of paper sheets stored in wooden drawers.   Sapide was digitized and beefed up throughout the years, but the core structure remained the same: each “significant security event”, even minor, must be declared by the plant workers and registered in it, triggering rapide response as well as long term investigations from the regulator. "," While exploring the dataset, we soon came to realize that its content was as difficult to comprehend as a nuclear reactor itself. Even the basic description of the event (a plain text field) does not make sense to the profane reader – at first glance, they all look like the beginning of the Chernobyl TV series, but that's about it.   We therefore had to familiarize with the taxonomy used for each field by going through hundreds of documentation pages. They go from the pieces of equipment impacted by the event to its causes and consequences on the power plant.   Understanding and scoring the severity of a particular event was even more challenging. We has to evaluate the numerous indicators available, understand how they are computed and what they tell us about the safety status of the reactor.   Lastly, to answer our initial question, we had to figure out if the age of the reactor had something to do with the recorded incident – even a brand new reactor can run into a problem, in fact, younger installations experience more issues than mature ones. We decided to base our conclusions on two criteria: was aging listed as one of the causes of the incident in the related field? Is aging mentioned in the description of the incident in some way?     "," Here are some takeaways:     you can work on a dataset even if you don't understand every tiny bit of it.   you probably need help from specialists to understand the bits that matter to you (and they will happyly do so).   there is no subject so complex that you can not make sense of it for you and your reader. It will just take more time and more energy.   Excel is not the best tool to work on Excel files if they are too big.   a journalistic work can be deemed worthy of interest by the most skilled experts – not because it's better, just because we offer a different perspective on things.    ",https://www.contexte.com/article/energie/vieux-et-donc-dangereux-on-a-explore-40-ans-devenements-dans-les-reacteurs-nucleaires-francais_109480.html,https://translate.google.com/translate?sl=auto&tl=en&u=https://www.contexte.com/article/energie/vieux-et-donc-dangereux-on-a-explore-40-ans-devenements-dans-les-reacteurs-nucleaires-francais_109480.html,https://translate.google.com/translate?sl=fr&tl=en&u=https://www.contexte.com/article/energie/entretien-aux-sources-de-notre-enquete-sur-40-ans-danomalies-de-surete-nucleaire_111654.html,https://twitter.com/ContexteEnergie/status/1230747636010864642,https://translate.google.com/translate?sl=fr&tl=en&u=https://www.franceinter.fr/societe/surete-nucleaire-40-ans-de-donnees-dissequees,https://twitter.com/suretenucleaire/status/1230856421367013376,,,"Yann Guégan,Victor Roux-Goeken","   Yann Guégan  is a journalist in charge of editorial innovation inside Contexte’s newsroom in Paris. He designs advanced web scrapers, interactive infographics, data visualizations as well as internal tools for the reporters. He also contributes <a href=""https://dansmonlabo.com"" target=""_blank"">through his blog  to the current conversation about the future of news. He is a proud founding member of the Conseil de déontologie journalistique et de médiation (CDJM), France’s long awaited journalism self-regulatory body. He’s a renowned data driven journalism trainer, host of numerous workshops and conferences in France and abroad.   A former freelance copy editor for various print outlets, he started working online in 2007, and discovered a whole new world, where journalists can enter a conversation with their readers. Since then, he kept on broadening his set of skills, learning data driven journalism techniques, webdesign tricks, user experience processes or useful programming languages.     Victor Roux-Goeken  is editor of the energy and climate section at Contexte, reporting with predilection on nuclear topics. After helding this position for more than 5 years, he’s now preparing the launch of a new environment section. He has been working since 2007 on environmental matters for several specialized media outlets. During this period, he worked as a freelance journalist in Brazil from 2013 to 2014 to report on the impacts of the World Cup on this country. ",21 Feb 2020
United Kingdom,"The Guardian, Forensic Architecture",Big,The Guardian / Mark Duggan police shooting: can forensic tech cast doubt on official report,"Investigation, Long-form, Fact-checking, Infographics, Video, Crime, Gun violence","Animation, 3D modelling, Canvas, Json, Google Sheets, Node.js"," The 2011 police shooting of Mark Duggan triggered the biggest riots in modern English history. The official findings on the circumstances of his death were challenged by Forensic Architecture, a human rights research organisation in 2020. It reviewed hundreds of publicly available documents, including witness statements, diagrams, photographs, videos and expert reports, as well as recreating what went on using 3D technology.    The Guardian visuals team worked in collaboration with Forensic Architecture to report on their investigation, using innovative storytelling techniques to visually communicate and report on the spatial inconsistencies of the official findings. "," Forensic Architecture claimed they had found inconsistencies in both the inquest verdict and the IPCC ruling on Duggan’s death, and this project was able to articulate these in an engaging and original way to a mass audience. In the wake of the publication of the findings the Duggan family called for a reopening of the investigation. The police watchdog responded by saying it would review the findings in line with its reopening policy and new statutory power to reopen investigations if there are compelling reasons to do so.   The case of Mark Duggan is one that has been widely cited by supporters of the Black Lives Matter movement in the UK and was prominently mentioned during protests that took place in the summer of 2020. Bringing the independent investigation to a wide audience at this time, in an immersive 3D format, helped generate renewed public interest in the case.   The interactive received over 100,000 page views in the first 24 hours and had a median attention time of 1m36s. "," We digested the investigation and pulled out the key findings that told the story in a way that could be communicated to a wide audience, and worked on a visual narrative that worked alongside reporting on the findings.   We used Figma to create mockups and prototypes until we settled on a style that referenced forensic investigations. The design and interactive elements of the project were built in parallel. We used Mustache for templating our HTML and Google Docs to generate our JSON (so that editors could work in parallel). We used Javascript and HTML canvas to build the scrollable 3D animations and CSS animation for the header and static image transitions. We also used Adobe Illustrator and After Effects to modify the assets provided by Forensic Architecture to reflect the Guardian’s style guidelines and to make sure they integrate seamlessly with the reading experience. "," The enormity of the project — both in terms of its significance and the amount of information we were given. The findings by Forensic Architecture had a huge range of complexity, multiple animations and were made up of hundreds of pieces of evidence (including photographs, videos, diagrams, expert reports and witness statements) that we needed to present in an accessible, innovative — and most importantly, accurate — way. Working in collaboration with them enabled their unique investigative and reconstructive abilities, which are dedicated to highlighting human rights abuses across the globe, to be brought to a wide audience and hold to account governing bodies in a way that would be impossible to do with textual reporting alone.     "," This project was all about collaboration. Our visuals team worked with the experts at Forensic Architecture, our in-house video editing team and the news team. We believe this demonstrates that collaboration with outside specialists and across a range of disciplines can be enormously beneficial for a newsroom, and can result in outstanding and powerful journalism. In addition, it shows that interactive journalism can be a highly effective tool for holding public bodies to account and giving a voice to local communities. ",https://www.theguardian.com/uk-news/ng-interactive/2020/jun/10/mark-duggan-shooting-can-forensic-tech-cast-doubt-on-official-report,,,,,,,,"Antonio Voce, Frank Hulley-Jones, Lydia Mcmullan, Haroon Siddique, Forensic Architecture"," This project was a collaboration between the Guardian visuals team and Forensic Architecture, a human rights research organisation ",10 Jun 2020
Indonesia,"Earth Journalism Network, Oxpeckers.org, Haluan.co",Small,New Indonesian law on wildlife crime,"Investigation, Long-form, Database, Infographics, Chart, Map, Environment, Crime","Scraping, Google Sheets, CSV","This project was looking into dozen of court verdicts to see how the wildlife criminal suspects punished for their crime. Due to the outdated the conservation Act 1990, I found that the verdict was too lenient and failed to give a deterrent effect. However, the was a case that given a high verdict of up to four years in jail and fined Rp1 billion (US$68,000). The verdict, which became the highest punishment of wildlife crime ever in Indonesia, was given by the Pekanbaru Court given to four members of the tiger cubs syndicate operating between Malaysia and Indonesia. The main"," The impact of the project had various impacts on the stakeholders. The use of the Quarantine Act 2019 by the Pekanbaru Court inspired other prosecutors to sue wildlife criminals with the same act. In July 2020 for example, Tanjung Karang Court sentenced two smugglers of a hundred exotic birds with the Quarantine Act. Along the year, the use of the new act became more common in the fight against wildlife crime.   Moreover, this project also added more pressure to the government and the House of Representatives to consider The Conservation Act 1990 revision. Although had been included in the 2015-2019 National Legislation Programme, the subtitle bill continuously hampered and finally withdrew in May 2019. Some experts said that reform of The Conservation Act 1990 urgently needed to provide a deterrent effect to the wildlife crime in the country. The lenient verdicts, due to the outdated law, which uncovered by this project became a reminder to the stakeholders to prioritize the reform of the Act.    Moreover,  "," The main data resources of this project are the court verdicts which can be accessed by the public. To collect the data, I have to scrape the website of the Supreme Court and transfer them into a spreadsheet. I made a long-list of the verdicts court by and categorized them into proper and clean data.    After making the details of each case, I look into the sentence section and begin to analyze the data. This technique helped me to find out the case of the Irawan Shia syndicate which was given the highest sentence by the panel of the judges. Finally I am able to find the main reason why the four members of syndicate were fined 10 times higher than the other similar cases. In addition, I also transfer the spreadsheet data into a visual graphic with Flourish Studio.    To enrich the story, I also utilize the StoryJS Map to provide a better understanding of Irawan Shia’s syndicate. The scrolling-telling map traced the journey of the syndicate from its origin in Malaysia across to Rupat Island before finally being arrested by the Police in Pekanbaru.      "," The main obstacle of the project was due to the access of the data. Although the data was available at the Supreme Court website, the data was uncategorized and not very user-friendly.  To do this I have to put a keyword and select the case related to the wildlife crime. I found this very exhausting since I have to eliminate thousands of other cases which are not related but kept appearing when I type the keyword.   When I finally found the wildlife case, I had to download the record of the trial in PDF format. The next step was to deduce an important information based on the PDF which contains a dozen pages and put it into a spreadsheet. Finally, I have to repeat the process of collecting data with the other dozens of cases. Actually, there are hundreds of wildlife cases that can be found at the Supreme Court website. However, due to the deadline and limited human resources, I was only able to collect 50 of them.      "," As long as I know, I was the first journalist in the country who scraped into the Supreme Court website, collecting dozens of trial records, and produced stories based on the data. Some journalists might download a single specific case document from the website. However, to do it on this scale of the project and make my own database in the spreadsheet was not very common in the practice of journalistic work in the country.    Actually, this technique has been done by some non-profit organizations in their research. However, the NGO usually only publishes the result of the research which sometimes is not compatible with the journalistic needs.    By doing this on my own, I have raw data that helped me to build the specific angle of my stories. The journalist can apply this technique for different kinds of issues. One of my colleagues, for example, doing a similar approach to uncover the corruption case verdict. I believe, by gathering and building our own database, the journalists will have a better understanding of every issue.     ",https://earthjournalism.net/stories/new-indonesian-law-used-to-crack-down-on-wildlife-smuggling,https://oxpeckers.org/2020/09/new-indonesian-law/,https://haluan.co/article/harapan-baru-pemberantasan-kejahatan-satwa,https://app.flourish.studio/visualisation/3566283/,,,,,"Rezza Aji Pratama (writer and researcher), Wan Ulfa Nur Zuhra (data visualization), Fiona Macleod"," Rezza Aji Pratama has worked as a journalist for eight years, with a focus on business reporting, science, public health, and environmental issues. The grantee of several grant reporting including Earth Journalism Network and Rainforest Journalism Fund by the Pulitzer Center of Crisis Reporting. Due to the pandemic, Rezza just left his position as an Editor at Haluan Media Group and acting as a freelance journalist while in the process of setting up his own media company which specialized in data journalism and graphic stories. The company called Rupadata.id will be launched in March 2021.  ",11 Sep 2020
United States,VirginiaMercury.com,Small,‘The bedrock of wealth inequality': Data shows big racial disparities in mortgage loans and homeownership,"Investigation, Explainer, Solutions journalism, Long-form, Open data, Infographics, Chart, Map, Politics, Business, Economy","Scraping, QGIS, Microsoft Excel, Google Sheets, CSV"," This project showed that African Americans in Virginia are far more likely than Whites to be denied a loan to purchase a home. As a result, Black homeownership rates are far below White homeownership rates. Those disparities help explain the wealth gap between Blacks and Whites because owning a home is key to building wealth. The project used extensive data analysis and expert interviews to document the disparities while also putting a human face to the issue. "," The project triggered a community discussion in Richmond and other areas of Virginia about how to address disparities in home loans and homeownership. The National Association of Real Estate Brokers, Virginia Bankers Association, Richmond Metropolitan Habitat for Humanity and other organizations circulated the story among their members. Virginia REALTORS, an association of 35,000 real estate agents across the state, created a Presidential Advisory Group dedicated to expanding opportunities for diversity and inclusion. In its materials, the group <a href=""https://virginiarealtors.org/wp-content/uploads/dlm_uploads/2020/11/Briefing-Book-draft-November-2-2020.pdf"" target=""_blank"">highlighted the project  as a driving force. The article also was a focus of the <a href=""https://housingforwardva.org/event/2020-virginia-governors-housing-conference/"" target=""_blank"">2020 Virginia Governor’s Housing Conference , which included a session titled, ""Making it Right: How Housers Can Address Racial Inequalities and Close the Homeownership Gap."" "," I downloaded data collected by the U.S. government under the Home Mortgage Disclosure Act. The <a href=""https://ffiec.cfpb.gov/data-browser/data/2019?category=states&items=VA"" target=""_blank"">HMDA data for Virginia for 2019  contained 505,456 records -- one for each loan application handled by each financial institution that year. Using Microsoft Access, I joined the data file with other tables to translate codes and include the names of lending institutions.   I then filtered the data for home-purchase loans that had been approved or denied, giving me a final data set of 127,860 records. I ran a succession of group-by and crosstab queries in Access to calculate how often applicants from each racial or ethnic group were denied home loans, and why. I calculated the statistics statewide, for each metro area and by locality (city and county). I also computed the denial rates by ethnicity-race for applicants with similar incomes ($40,000-$49,999, $50,000-$59,999, $60,000-$69,999 and so forth).   For comparison, I conducted similar analyses of HMDA data for Virginia as far back as 2007 (<a href=""https://www.consumerfinance.gov/data-research/hmda/historic-data/"" target=""_blank"">available from the Consumer Financial Protection Bureau ).   I performed most of the analysis with Microsoft Access and refined the results with Microsoft Excel.   I also downloaded homeownership data from the <a href=""https://data.census.gov/"" target=""_blank"">Census Bureau's data portal , using the <a href=""https://data.census.gov/cedsci/table?q=renters&g=0400000US42&tid=ACSST1Y2018.S2502&hidePreview=false"" target=""_blank"">American Community Survey  estimates. Using Excel, I computed homeownership rates overall and by ethnicity and race for the nation, for Virginia, for each Virginia metro area and for each city and county in the state. I worked initially with the most recent data (2018) and then compared the results with previous years.   I created the online graphics with Datawrapper, the data visualization tool preferred by Virginia Mercury. For the tooltips for the maps, this required extensively manipulating the HTML coding.   Finally, for transparency and trust, I publicly shared <a href=""https://bit.ly/hmda19_va"" target=""_blank"">all of the data  on the project's Google Drive. "," The hardest part of the project was finding people to humanize the data -- to put a human face to statistics. This was complicated by the Covid-19 pandemic, which shut down in-person services at organizations that help prospective homeowners and thus undercut what would have been my strategy for finding people to interview. However, working through social media and with experts I had contacted online and by telephone, I managed to connect with people who represented the story's key data points (i.e., African Americans who had been denied home loans).   The data analysis was critical to forging those personal connections. The national and local experts I had contacted trusted me -- and helped me find ""real people"" sources -- because they knew I had done my homework. I had crunched more than 10 years of HMDA data, sifting through as many as a half-million records for each year. Moreover, I followed a time-tested methodology used by other journalists -- most notably Bill Dedman, who won a <a href=""https://www.pulitzer.org/winners/bill-dedman"" target=""_blank"">Pulitzer in 1989  for his analysis of such data. The data analysis enabled me to approach experts with information they were eager to know.   A crucial component of this project was to look at the big picture -- not just at racial discrimination in home loans but also at American history, from slavery to redlining. I didn't shy away from discussing racist mortgage brokers, but I explained that institutional racism and other factors also contribute to the higher loan denial rates for Black applicants. Moreover, I connected homeownership patterns to the wealth gap and described the vicious circle underpinned by data: Because they are less likely to own a home, African Americans have less wealth; and because they have less wealth, African Americans are less likely to own a home.     "," My project's biggest lesson for other journalists is the value of open data -- especially massive sets of microdata (like HMDA) that are updated regularly by government agencies. I focused on Virginia, of course, because I was writing for a Virginia-focused news outlet. But this story could be replicated in any state.   It was crucial to jump on the issue quickly. The Federal Financial Institutions Examination Council <a href=""https://ffiec.cfpb.gov/"">released the 2019 HMDA data  on June 24, 2020; I published my article in less than a month.   The project involved a lot of numbers, and that can intimidate readers. But I was judicious about which numbers to weave into the text of the story, I offloaded most statistics to data visualizations and tables, and I used narratives, quotes and telling details (""She lives in the three-bedroom, two-bath home — ‘white with red shutters’ — with her special-needs son"") to keep readers engaged.   A final lesson for journalists would be to examine solutions as well as the problems highlighted in the story. I devoted a section of my article to strategies to boost Black homeownership. After the project was published, those strategies helped foster discussion among government officials, business leaders and fair-housing advocates. ",https://www.virginiamercury.com/2020/07/21/the-bedrock-of-wealth-inequality-data-shows-big-racial-disparities-in-mortgage-loans-and-homeownership/,"http://bit.ly/va-hmda-methodology -- ""Nerd box"" explaining where I got the data and how I analyzed it.",https://bit.ly/hmda19_va -- Google Sheet with summary statistics from my analysis of the HMDA data.,"https://bit.ly/hmda19_va_db -- Compressed file containing a Microsoft Access database, which has the tables and key queries from my HMDA analysis.","https://bit.ly/homeownership_va -- Google Sheet with national, state, metro and locality data on homeownership by race.","https://www.datawrapper.de/_/zZ1aU/ -- Bar chart showing mortgage loan denial rates by race for the nation, state and each Virginia metro area.",https://www.datawrapper.de/_/18V0n/ -- Map showing homeownership rates for Blacks and Whites in each Virginia city and county.,,Jeff South," Jeff South was a newspaper reporter and editor for 20 years in Texas, Arizona and Virginia and then taught journalism for 23 years at Virginia Commonwealth University, where he is an associate professor emeritus. He was the first data editor at the Austin American-Statesman and specialized in teaching digital skills at VCU. Over the years, his students won more than 65 national, regional and state awards for news stories produced under his guidance. Jeff himself has won several awards as both a journalist and a teacher, including a Fulbright, and has taught data journalism in China, Ukraine, Vietnam and Azerbaijan.     ",21 Jul 2020
Canada,The Globe and Mail,Big,Bias Behind Bars,"Investigation, Long-form, Database, Infographics, Chart, Politics, Women, Crime, Human rights","Animation, AI/Machine learning, Adobe Creative Suite, Microsoft Excel, CSV, R, RStudio"," In late October, The Globe and Mail published Bias Behind Bars, an investigation into systemic racism in Canadian federal prisons.   Through a database of more than 50,000 inmates obtained via freedom of information request, we examined the structural biases inherent in the tools used to assess prisoners. Risk assessments are meant to be an impartial guide of who can be rehabilitated, and are steeped in decades of research. But, as The Globe discovered, these tools are fundamentally, powerfully biased against Indigenous and Black inmates, placing them in higher security classifications and assigning them worse odds of successfully re-entering society. "," The response to The Globe’s story was immediate. Within 48 hours, a member of the House of Commons’ public safety committee had pledged to conduct a parliamentary study of systemic racism in federal prison risk assessments. “This needs action,” he told The Globe. A day later, the study was officially announced with all-party support. Then, a day later, in a rare political acknowledgment of the challenges faced by Canada’s usually invisible prison population, Prime Minister Justin Trudeau himself said more had to be done by the federal government to fight systemic racism in prisons.   The investigative series' impact has also extended beyond Parliament. Lawyers have used our findings at parole hearings, professors are using our stories as teaching material, and both senators and Canada’s correctional watchdog have referred to the series in recent statements. In early January, a civil rights lawyer filed a class-action lawsuit against the government on behalf of tens of thousands of inmates, arguing the use of these risk scores amounts to a deliberate discriminatory practice.   The strongest words, however, likely came from Jack Harris, the legislator who first proposed the parliamentary study. “It’s something that the government has to respond to,” he said. “The fact this has been done by somebody outside the system as opposed to the system itself is a condemnation of the effort that should’ve been done sooner and more effectively.”   Testimonials from those personally affected by correctional risk scores have driven home the importance of this form of data-driven investigative reporting. “Thank you for publishing a very important issue in The Globe and Mail newspaper,” a former inmate wrote. “I was held for an additional 10 years in high- and medium-security prisons by this biased manipulative system. … Thank you for telling our country the other side of the story; the truth.” "," The genesis of Bias Behind Bars was a freedom of information request, filed to the federal government, asking for years of data on federal inmates. Data-driven freedom of information requests of this sort are unusual in Canada – but after seven months of negotiation, the government eventually delivered a 750,000-row file documenting the lives of 50,000 people over a seven-year period.   We used a statistical programming language called R, combined with a data journalism analysis framework – <a href=""https://github.com/globeandmail/startr"">startr , built and open-sourced by The Globe two years ago – to unpack and analyze the file.   After weeks of analysis, it became clear that simple descriptive statistics such as tallies, percentages and rates of change wouldn't be enough to isolate the impact of race on someone's risk assessments. Variables like age, gender, the severity of an inmate's offence and their criminal history were all interconnected; combined, they led to an inmate's score.   Instead, The Globe turned to statistical modelling. Over several months, and with the assistance of statisticians, criminologists and data scientists, we built a series of multivariate logistic regressions that controlled for all those variables and explained the impact of race on both men and women's most important risk scores.   Finally, the insights gleaned from these models and the overall trends identified in the data were fed back into the reporting process, inflluencing the kinds of documents we sourced, the people we interviewed and the direction of the ultimate reported story. Those findings were also shared with The Globe's design and graphics teams, who distilled the information into visual explanatory content, like the animated dots which guided readers through how inmates' risk scores broke down by race. "," For decades, federal watchdogs and prisoner advocacy groups have suspected race plays a role in determining an inmate’s risk scores. But, in report after report, analyses of risk levels only scratched the surface, looking simply at, for instance, the rate at which Indigenous inmates were classified to maximum security, or how frequently women were deemed to have a low potential for reintegration.   The Globe’s statistical investigation cut through this noise by doing something that had never been done: accounting for the myriad factors – age, offence severity, criminal history and so on – that play into a final score, effectively isolating the impact of race. The process of developing and fine-tuning a methodology for that analysis was by far the most complex part of the investigation.   It was also an analysis that likely wouldn’t have occurred without The Globe. During early conversations ahead of filing a freedom of information request for inmate data, several sources warned us the information we sought would never be released by Correctional Service Canada, the country’s federal prison agency. In fact, the agency itself had never undertaken an analysis of race and risk scores on the scale of what The Globe intended.   When the analysis revealed the biases in these critical prison tools, a second question emerged: How could the prison agency have missed the massive impact of race on inmates' scores?   The answer, we suspected, was that they hadn’t.   In January, 2021, after months of reporting, we confirmed what had been an open secret within the agency for decades: An internal federal government document from 2004, obtained by The Globe, showed unequivocally that senior Correctional Service leadership had been warned of serious flaws in its security risk tool for 16 years – and yet the scale remains in use, unchanged, to this day. "," Above all else, Bias Behind Bars reveals what’s possible when filing ambitious, data-driven freedom of information requests. Moonshots like these don’t always pan out, but when they do, they often lead to incredible stories. We’ve since begun filing more of these – even if they could take years to bear fruit.   As so often happens during large investigations, we pivoted several times early in the reporting process. Our original intent was to look at the diversity of Canadian juries – but that data didn’t exist. Instead, we began looking at sentencing patterns, but our freedom of information requests didn’t net enough data for that, either. Instead, they pointed to our ultimate topic: risk scores.   Experimentation was also essential to the project. This was the first time The Globe had built a statistical model for a story, and while I was already comfortable analyzing data, I’d never modelled a dataset. The process was intense and required a countless conversations with statisticians and academics, but resulted in a skill I’ll rely on in for future stories.   Finally, Bias Behind Bars shows the importance of using compelling characters and narrative journalism to pull readers through a story that would have otherwise been too dry, data- and policy-heavy.   While data-driven findings suffused all our reporting, those were carefully balanced against the story of Nick Nootchtai, an Indigenous man who spent his entire 12-year manslaughter sentence behind bars, the majority of it in maximum security. Instead of simply running readers through our findings and telling them how Mr. Nootchtai’s scores affected his prison life, we showed it, detailing the few prison jobs open to him, how his wife could never join him for conjugal visits and the fact that his high security score even denied him access to pencil crayons for arts and crafts. ",https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/,https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prisons-methodology/,https://www.theglobeandmail.com/canada/article-this-needs-action-ndp-experts-call-for-solutions-to-racial-bias-in/,https://www.theglobeandmail.com/canada/article-committee-mps-support-push-to-study-systemic-prison-racism/,https://www.theglobeandmail.com/canada/article-more-needs-to-be-done-to-fight-systemic-racism-in-federal-prisons/,https://www.theglobeandmail.com/canada/article-for-indigenous-women-systemic-racial-bias-in-prison-leaves-many-worse/,,,Tom Cardoso," Tom Cardoso is a crime and justice reporter and data journalist at The Globe and Mail, a national Candadian newspaper. Based in Toronto, Tom has been with The Globe for seven years, and his work often focuses on obtaining large government datasets through creative use of freedom of information legislation. He has previously reported extensively on gun violence and white collar crime, the latter of which netted him the international Data Journalism Awards' investigation of the year prize in 2018. In October, he published a years-in-the-making investigation on systemic racial bias in Canada’s prisons. ",24 Oct 2020
United States,High Country News in collaboration with grants from The Pulitzer Center for Crisis Reporting and The Fund for Investigative Journalism,Small,Land-Grab Universities: How expropriated Indigenous land became the foundation of the land-grant university system,"Investigation, Explainer, Solutions journalism, Long-form, Database, Open data, Fact-checking, Illustration, Infographics, Map, Satellite images, Politics, Culture, Human rights","Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, OpenStreetMap"," Nearly 11 million acres of Indigenous land, over 160 violence-backed treaties and land seizures, approximately 250 tribes, bands and communities, and fifty-two universities: Our investigation reveals how expropriated Indigenous land financed the land-grant university system, and how many institutions continue to profit. "," Within months of publication several major initiatives took shape in response to our reporting. At Cornell, the largest Morrill Act beneficiary, faculty launched a project to document the university’s financial windfall. Similar internal reviews are ramping up at MIT and the University of Connecticut, which is developing an exhibition based on the data as part of a push for a new Cultural Center for Native & Indigenous Students. At Ohio State researchers have partnered with the First Nations Development Institute to draft a <a href=""https://u.osu.edu/landgranttruth/"">reconciliation plan  that will benefit the tribal communities whose land seeded the school’s founding. Washington State University has taken the lead in actually rewriting its land acknowledgment to incorporate the report’s findings, and pledged to commission a team to determine reconciliation plans. Working groups at Colorado State, Arizona State, the University of Minnesota, and others are likewise laying the groundwork for reforms tying their land-grant legacies to the needs of their Indigenous students.   This year, thousands of undergraduate and graduate students at scores of universities are reading the report in journalism, education, and liberal arts courses. The University of Missouri even made it required reading university-wide through its freshman composition program. At the University of Florida, Yale University, and Cornell University, student governments and advocacy groups demanded investigations, protested for racial justice, and petitioned for increased recruitment and funding for students from tribal nations disadvantaged by the Morrill Act. The response is reminiscent of the reaction to Brown University’s Report on Slavery and Justice (2006), which sparked a reckoning with higher education’s connections to the slave trade. Only it appears to be moving at a more accelerated pace.  ","      This investigation relied on a unique combination of large-scale spatial analysis and historical research. To tell the story, we had to uncover ties between contemporary universities and Indigenous land redistributed by the federal government more than a century ago. To accomplish this, we built a geodatabase of nearly 80,000 land parcels. This database recreates the complete footprint of a major US land law for the first time.   To populate our geodatabase, we constructed parcels in ArcGIS using data digitally extracted and hand-transcribed from fifty different sources. We linked these parcels to Indigenous land cessions from nine publicly available spatial datasets and maps, several of which required original georeferencing. We incorporated data on past payments for Indigenous land, acreage distributed, and principal raised for universities from 28 other sources, primarily court cases for broken treaties and government reports. Our sources ranged from crumbling archival manuscripts from the 1860s to state disclosures posted online in the past few years. To visualize and analyze this data, we processed it with a dozen different software programs. The database enabled our photographer to visit parcels and our cartographer to produce maps and graphics. It generated statistics that punctuate the story. And it revealed long hidden connections between prosperous universities and dispossessed tribal nations that structured our narrative. Because we developed the methodology for this story from scratch, we also published an essay detailing our process and sources. These materials both document our findings and illustrate an approach future investigators can adapt to examine other sites of state and institutional wealth building through the expropriation of Indigenous resources. "," This investigation relied on a unique combination of large-scale spatial analysis and historical research. To tell the story, we had to uncover ties between contemporary universities and Indigenous land redistributed by the federal government more than a century ago. To accomplish this, we built a geodatabase of nearly 80,000 land parcels. This database recreates the complete footprint of a major US land law for the first time.   There were many missing records in any single place, which meant material had to be gathered from multiple sources, cross-referenced, and duplicates eliminated to reconstruct the full record for a given state/school. Some of these records were crumbly and illegible (or almost illegible), literally falling apart. The polygons had to be constructed to match the plss notation. Hundreds had to be drawn by hand. There was no guide to tell us exactly how much material we were looking for. We had to construct that from research, too. It was like putting together a massive puzzle for each state, except the pieces aren’t contiguous, you don’t know how many there are, they’re stored in different boxes, there are duplicates, and there’s no cover image to tell you what you’re looking for.     ","  Because we developed the methodology for this story from scratch, we also published an essay detailing our process and sources. These materials both document our findings and illustrate an approach future investigators can adapt to examine other sites of state and institutional wealth building through the expropriation of Indigenous resources. ",https://www.hcn.org/issues/52.4/indigenous-affairs-education-land-grab-universities,"https://www.landgrabu.org/. This is link to overview that includes methodology, public database, stories and follow-up",https://www.hcn.org/articles/indigenous-affairs-the-land-grant-universities-still-profiting-off-indigenous-homelands,"Cornell University addresses stolen Indigenous land in new project October 23, 2020 https://www.hcn.org/issues/52.11/latest-cornell-university-addresses-stolen-indigenous-land-in-new-project","Students and faculty urge deeper look at land-grant legacy December 22, 2020 https://www.hcn.org/issues/53.1/indigenous-affairs-land-grab-universities-students-and-faculty-urge-deeper-look-at-land-grant-legacy",,,,"Robert Lee, Tristan Ahtone, Margaret Pearce, Kalen Goodluck, Geoff McGhee, Cody Leff, Katherine Lanpher, Taryn Salinas"," Robert Lee is a histoiran and lecturer in American History at the University of Cambridge in the United Kingdom.   Tristan Lee was the head of the Indigenous Affairs Desk at  High Country News  at time of publication; he is currently editor-in-chief in Auistin, Texas, at The Texas Observer. He is a member of the Kiowa Tribe.   Margaret Pearce is Citizen Band Potawatomi and a cartographer whose work has been exhibited nationally and internationally. She is based in Rockland, Maine.   Kalen Goodluck was a contributing editor/photojournalist for  High Country News  at the time of publication. He is Diné (Navajo), Mandan, Hidatsa and Tsimshian tribes and an enrolled. member of the Three Affliated Tribes of the Fort Berthold Indian Reservation in North Dakota. He currently resides in New Mexico.   Geoff McGhee is a Seattle-based multimedia journalist and a veteran of  The New York Times,  Abcnews.com, and  Le Monde.    Cody Leff  is a designer and software engineer based in Los Angeles.     Katherine Lanpher is the interim editor-in-chief of  High Country News  and lives in upstate New York.    Taryn Salinas is a research editor who is currently at National Geographic and who has also worked at The Museum of the Amerian Indian. She is based in Alexandria, Virginia.                         ",30 Mar 2020
Brazil,Nexo Jornal,Small,"A tragedy, calculated","Illustration, Politics, Health","Google Sheets, CSV, R","  The publication opens a series of five special articles that address the impacts of covid-19 in Brazil, at the moment when the pandemic reached the mark of 100 thousand deaths. The publication aimed to show the size of the loss of life that occurred in the country.","  The coronavirus pandemic is the biggest health crisis the world has faced in recent years and its effects in Brazil are still priceless. Five months after the first registration of a covid-19 case in Brazil, the country reached the mark of 100 thousand deaths. At the time of publication, it was the second country in number of deaths from the disease and was among those with the highest relative number, weighted by the population. This material aimed to give a closer view to the numbers represented in the statistics of the Ministry of Health.","      There were some team meetings until we got to know how to tell such a sensitive story, within a set of materials that the entire Nexo newsroom would participate in, until we reached a consensus to combine data on deaths from covid-19 in the country with images with a more direct relationship to the lives lost.    To approximate the numbers of people, we use means of transport with emphasis on the image of a train crowded with dead people every two days from June to August. Both the visual refinement of the graphic and the illustrations were drawn in Adobe Illustrator, thinking mainly on how this set would work on mobile devices, and only then produce the same version of it on the desktop."," The hardest part was to represent with empathy the tragedy. For this reason, we put two visual languages ​​together in this narrative. The visualization of data, where each point symbolizes a life lost in five months of pandemic and the image of the train and other means of getting around that are powerful figures when it comes to imagining how many people we have lost these days. "," To represent events considering the individual value of each person present in data. To integrate data visualizations with other journalism formats, in a special publication.  ",https://www.nexojornal.com.br/especial/2020/08/08/100-mil-mortes-no-Brasil-o-c%C3%A1lculo-de-uma-trag%C3%A9dia,https://www.nexojornal.com.br/especial/2020/08/14/O-c%C3%A1lculo-de-uma-trag%C3%A9dia-sem-ci%C3%AAncia-na-gest%C3%A3o,https://www.nexojornal.com.br/especial/2020/08/21/O-c%C3%A1lculo-de-uma-trag%C3%A9dia-hesita%C3%A7%C3%A3o-econ%C3%B4mica,https://www.nexojornal.com.br/especial/2020/08/28/O-c%C3%A1lculo-de-uma-trag%C3%A9dia-Brasil-sem-m%C3%A1scara,https://www.nexojornal.com.br/especial/2020/09/04/O-c%C3%A1lculo-de-uma-trag%C3%A9dia-quem-se-responsabiliza,,,,"Lucas Gomes, Caroline Souza, Gabriel Maia, Renata Rizzi, Marina Menezes, Guilherme Falcão, Gabriel Zanlorenssi","   Lucas Gomes,  information designer     Caroline Souza,  designer assistant     Gabiel Maia,  data analyst     Renata Rizzi, Marina Menezes, Guilherme Falcão, Gabriel Zanlorenssi,  editors ",8 Aug 2020
Netherlands,"Argos, Aripaev, Investico, Die Zeit, Publico, the Guardian",Big,Money to Burn,"Investigation, Long-form, Cross-border, Multiple-newsroom collaboration, OSINT, Satellite images, Environment","360, Drone, QGIS, Adobe Creative Suite"," Money to Burn is an investigation by a team of journalists and European newsrooms, led by Argos and funded by Investigative Journalism for Europe. For a period of three months, our cross-border team investigated everything related to the biomass trade, from subsidies, to certifications, to the European lobby, after learning from a colleague in Tallinn that Estonia was exporting almost all of its pellets overseas, with increasing impact on the country’s forests. The result: a cross-border story about the effect of European subsidies on Estonia's forest, plus a series of radio, print and online publications in partner media. "," Our team reporters worked together for three months to create a 10,000 word interactive feature on the biomass trade, including interactive timelines and graphics on how pellets are made, a certifications ""game"" and using 360 degree imagery and Google Street View to take the reader to the site of environmental devastation. An extended team of 16 reporters across 11 newsrooms worked on a suite of stories in national press, including the Guardian in the UK, Zeit Online in Germany, Publico in Spain, and De Groene Amsterdammer and Investico in the Netherlands and across national press and television in Estonia.    Our investigation has been cited in the Estonian and Dutch parliament. Since it was published, the new Estonian Government has promised to review cutting rates in state-owned forests. Our Dutch radio broadcasts were picked up across national Dutch media. Our interactive investigation was cited by scientists and campaigners at a European Commission meeting to discuss a revision to the Renewable Energy Directive. The Guardian version was cited by Greta Thunberg as an ""essential read"" on Twitter and cited by the US magazine the New Yorker, which covered the subject a week later. "," To investigate the subsidised European pellet trade and its impact on Baltic forests, we uploaded boundary files for Estonia’s Natura 2000 zones to Global Forest Watch, an online platform for monitoring forests, and found that per-hectare tree cover loss (the removal of the tree canopy rather than outright deforestation) in these areas accelerated after 2015, when Estonian changed the rules around clear-cutting in some of its nature parks. We uploaded the KLM file to Google Earth Engine to identify areas where we could test for changes in forest cover. We used Google Timelaps to test for changes, then singled out Hanja Nature Park using open source GQIS, which allowed us to test one area for tree cover and some related statistics. In order to verify the Global Forest Watch data, which has its critics, we built an overlay in Goggle Earth Engine and checked the patches using Google Earth Pro's time-slider. We used photoshop to create different overlay images for our story, including GIFs and a slider, to allow the reader to explore differences in tree cover over time for themselves. For the 10,000 word feature, we travelled to Estonia and worked with local designers to custom-build graphics, including a certifications ""game"" and drop down features and timelines to add depth and context to our story. We worked with a local photographer to capture drone imagery of the forests and nearby pellet milles. We used Google Street View technology and 360 degree imagery to take the reader to the site of the environmental devastation so they could explore the differences in forest cover over time. Our investigation relied heavily on open-source, encrypted, collaborative tools including Next Cloud for file sharing, a DocuWiki to organise the mammoth amount of research we accrued over three months, Rocket Chat as a Slack alternative "," Conducting a cross-border investigation in a pandemic comes with no small amount of challenges. We had always planned to report the findings of our sateliite work on the ground, but as the summer waned and another round of lockdowns took hold across Europe, we had to make the call about whether or not it was safe to travel to Estonia. Four made it from Germany, the Netherlands and the UK but travel restrictions and the depleted number of flights made it impossible for others to travel.   At this moment, our well-established collaborative tools came into their own. Using RocketChat, Signal and video calls, we were in daily contact with the team working remotely. During trips to clear-cut areas, reporters sent us their location co-ordinates and we were able to corroborate their photos, videos and other reporting with satellite images in real time.   Because we only had a few days on the ground in Estonia, we adapted the Google Sprint model of iterating startups to our story. In this way we went from concept to product – an interactive feature of 10,000 words – within a week, iterating the story as we gathered evidence on the ground.      ","Cross-border investigative work is still a relatively new field. We are indebted to Investigative Journalism for Europe for funding and Arena for Journalism in Europe for supporting our project with technical and professional expertise. We feel there is enormous value in sharing what we learned during our investigation, from the technology and the systems that allowed us t share data and communicate safely, to the OSINT tools that enabled us to identify areas of interest to the investigation and explore changes in tree cover over time. Journalists might learn from the way we organised our investigation: both over time and practically how we kept track of our reporting. We set out to map the trade in biomass across European borders for the first time, in order to shed light on the major actors in the industry and their practises in three phases: a month on the trade in our own countries, a month on the trade across borders and a final month bringing all the pieces together to create our stories. We met once a week and spoke almost constantly on messaging channels in between. We established a DocuWiki, a kind of investigative wikipedia, which allowed us to navigate through our extensive reporting and find files and references. This was invaluable in helping us to identify who we had spoken to and what we already knew in the reporting phase, and as a reference point when we came to write up our work. Journalists might also learn from the way we used the Google Design Sprint to iterate our story over the course of a week on the ground in Estonia. The methodology meant that we were able to respond and adapt our plans based on what we found locally and in collaboration with local developers, videographers, designers and photographers",https://www.vpro.nl/argos/lees/onderwerpen/money-to-burn/en.html,https://www.vpro.nl/argos/lees/onderwerpen/money-to-burn/interactive.html,https://www.theguardian.com/world/2021/jan/14/carbon-neutrality-is-a-fairy-tale-how-the-race-for-renewables-is-burning-europes-forests,https://www.publico.es/sociedad/residuos-forestales-fuente-energia-renovable.html?utm_source=twitter&utm_medium=social&utm_campaign=publico,https://vimeo.com/486021454,https://www.platform-investico.nl/artikel/hout-uit-kaalkap-beschermde-bossen-voor-biomassa-europese-centrales/,https://www.err.ee/1193797/pealtnagija-puidumassi-maaramine-taastuvaks-kutuseks-soodustab-lageraiet,,"Piret Reiljan, Sophie Blok, Ties Gijzel, Hazel Sheffield, Catherine Joie, Silvia Nortes, Paul Toetzke", Piret Reiljan is a freelance journalist in Estonia.   Sophie Blok is a radio producer for Argos in the Netherlands.   Ties Gijzel is partnership developer for Argos in the Netherlands.   Catherine Joie is a freelance journalist in Belgium.   Paul Toetzke is a freelance journalist in Germany.   Hazel Sheffield is a freelance journalist in the UK.   Silvia Nortes is a freelance journalist in Spain. ,11 Dec 2020
United States,The Atlantic,Big,The COVID Tracking Project at The Atlantic,"Investigation, Explainer, Solutions journalism, Database, Open data, Fact-checking, Crowdsourcing, Illustration, Infographics, Chart, Map, Health","Scraping, D3.js, QGIS, JQuery, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, R, RStudio, PostgreSQL, OpenStreetMap, Python, Node.js, Microsoft Power BI"," The COVID Tracking Project at  The Atlantic  is a painstaking effort to compile more than 800 data points on the coronavirus pandemic from all 50 states, the District of Columbia, and U.S. territories on a daily basis. It is housed within  The Atlantic , and we worked with Boston University’s Antiracist Research Center on race data collection. Data scientist Jeff Hammerbacher and  Atlantic  journalists Alexis Madrigal and Robinson Meyer began this tracking separately before merging their efforts, later bringing on Erin Kissane to co-found the official organization in the first week of March. "," We became the definitive, trustworthy source for U.S. coronavirus data, filling the gap left by the federal government. Our data has been used in thousands of news articles and broadcasts across the political spectrum, and in <a href=""https://scholar.google.com/scholar?hl=en&as_sdt=0%2C33&q=%22covid+tracking+project%22&btnG="" style=""text-decoration:none;""><u>hundreds   of academic and medical papers in the most prestigious journals. It's also used by <a href=""https://coronavirus.jhu.edu/map.html"" style=""text-decoration:none;""><u>the largest   <a href=""https://www.ft.com/content/a2901ce8-5eb7-4633-b89c-cbdf5b386938"" style=""text-decoration:none;""><u>aggregation   <a href=""https://github.com/owid/covid-19-data/tree/master/public/data/"" style=""text-decoration:none;""><u>efforts   and has been cited in multiple <a href=""https://scholar.google.com/scholar_case?case=4182837643689079786&q=%22COVID+Tracking+Project%22&hl=en&as_sdt=80000006"" id=""docs-internal-guid-e274cfe1-7fff-1d23-19ce-8ac316f14b17"" style=""text-decoration:none;""><u>pandemic  -<a href=""https://public.courts.alaska.gov/web/media/docs/avc/memorandum1.pdf"" style=""text-decoration:none;""><u>related   <a href=""https://www.mass.gov/doc/ag-complaint/download"" style=""text-decoration:none;""><u>lawsuits  .       The White House used our <a href=""https://web.archive.org/web/20210113090724/https://www.whitehouse.gov/wp-content/uploads/2020/04/Testing-Overview-Final.pdf"" style=""text-decoration:none;""><u>testing data   rather than the CDC's, and the Biden transition team <a href=""https://www.buzzfeednews.com/article/zahrahirji/biden-trump-white-house-coronavirus-data"" style=""text-decoration:none;""><u>relied on CTP data  . The CDC published <a href=""https://www.cdc.gov/mmwr/volumes/69/wr/mm6924e2.htm?s_cid=mm6924e2_w"" style=""text-decoration:none;""><u>a report   stating that our race data might be more accurate than the federal government's. The CDC's vaccine advisory council used our data in presenting evidence of who should be <a href=""https://www.cdc.gov/vaccines/hcp/acip-recs/vacc-specific/covid-19/evidence-table.html"" style=""text-decoration:none;""><u>included in phase 1A of the vaccine roll-out  . Federal lawmakers repeatedly used our data in demanding answers from the executive branch, and cited us in a <a href=""https://www.govtrack.us/congress/bills/116/hr8073/text"" style=""text-decoration:none;""><u>bill   called the Improving COVID–19 Data Transparency Act. We've been cited by numerous federal agencies and praised by <a href=""https://twitter.com/FaceTheNation/status/1353331719550627840?s=20"" style=""text-decoration:none;""><u>Dr. Deborah Birx   as ""superb"" and <a href=""https://twitter.com/DrTomFrieden/status/1347711455882317824"" style=""text-decoration:none;""><u>former CDC director   Tom Frieden as ""invaluable."" The Biden administration used CTP data in its day 1 <a href=""https://www.whitehouse.gov/wp-content/uploads/2021/01/National-Strategy-for-the-COVID-19-Response-and-Pandemic-Preparedness.pdf"" style=""text-decoration:none;""><u>COVID response plan   instead of the CDC's.     We required states to make data available publicly if they wanted it included in our datasets. We wanted states to report tests in units capturing repeat testing; initially, only 19 states did so, and now all but three jurisdictions report repeat testing. When we began tracking race and ethnicity data, fewer than half of the jurisdictions reported those numbers. Now, nearly every single one does. Our work led to more than two dozen changes to data points on state websites, from correcting errors to clarifying definitions to releasing new data points. And we were instrumental in securing the release—<a href=""https://covidtracking.com/data/hospital-facilities"" style=""text-decoration:none;""><u>and demonstrating the utility  —of new data from the federal government. "," The COVID Tracking Project used a unique approach to gather data, relying on a network of hundreds of trained volunteers. The radically transparent project made all of its data and analyses publicly available so that anyone could use it, including an <a href=""https://covidtracking.com/data/api"">API , <a href=""https://covidtracking.com/data/download"">downloadable data , dashboards, and highly detailed <a href=""https://covidtracking.com/analysis-updates"">explanations  of the data. We also developed an entire suite of charts that we made available for public use that have been used and replicated by broadcast, digital, and traditional print outlets.        Since its inception, the CTP has built out new and unprecedented processes in order to collect the most complete and accurate data possible. Many other groups tried to ""scrape"" COVID-19 data automatically, but that method proved unreliable. We deployed human labor in the form of hundreds of trained volunteers to fact-check every single one of the data points, and we built automated systems that run in the background of the data collection process to help those people. We also created a data quality team that engaged in deep research to provide the metadata that was necessary to understand how states were reporting. We relied on a complex workflow combining Slack, Google Sheets, Airtable, and our own databases.       Because the data was so complex, we built out a reporting operation that made hundreds of contacts with state and federal officials to understand their numbers. We were instrumental in shaking more and better data out of states. Working with local reporters, we were also able to apply pressure to states to release more <a href=""https://covidtracking.com/race/dashboard"">race and ethnicity data , improve the quality of their reporting, and provide details on <a href=""https://covidtracking.com/data/long-term-care"">long-term-care facilities , where a significant portion of COVID-19 deaths have occurred.  "," The data that the CTP deals with is extremely messy and heterogeneous. It is produced by 56 jurisdictions, each of which has its own data pipelines and reporting quirks. Our team must stitch together this data to create valuable statistics. This requires in-depth knowledge not only of the data that a state provides, but its dashboards, data definitions, and caveats.        State data is imperfect and sometimes erroneous, so our teams throw themselves at the walls of government opacity day after day, trying to shine a light on each of the metrics we track. States themselves don’t always understand what’s happening and we’re often the very first to point out problems. Officials have told us that the federal government used our data to help the Coronavirus Task Force understand its own data.        As a brand-new endeavor, we had to establish relationships with every state, the federal government, public health officials, and others. We built a star-studded advisory board composed of public health experts, epidemiologists, technologists, and experts on racial inequities. We raised $1.5 million from top foundations (Rockefeller, Robert Woods Johnson, Chan Zuckerberg, Emerson Collective) to support the managers of our volunteers and build out our technical infrastructure. We did it with infrastructural support from  The Atlantic , but raised the funds ourselves.        The biggest obstacle is the sheer amount of work required to do what the CTP does. This is thousands of hours of work a week that require precision, knowledge, and dedication. We had to build a culture that would bring people in and keep them coming back, despite the ghastly nature of our work. Our data entry team leads have made the CTP a welcoming place that supports those who do the work. "," CTP is a testament to the power of collaborative journalism. We were able to spin up the project quickly and do urgent work by building a coalition of not just journalists, but academics, data scientists, epidemiologists, technologists, and public health experts. It took shape in the crucible of the U.S. pandemic, but could be applied to beats that require cross-disciplinary expertise.       We've also shown the ability to work  with  the public to  do  journalism. We recruited and trained hundreds of volunteers from all over the country and all walks of life—some of whom had no previous experience with data. We used simple, distributed tools to do this: a Google form, Zoom training sessions, Slack, and Google Sheets. We proved that it's possible to build an all-remote journalism endeavor from scratch in the midst of a pandemic, creating a culture that kept volunteers engaged.       We've built a series of workflows to find, calculate, check, and double-check the data. We plan to share these processes publicly, so that newsrooms that work with messy datasets can learn from our experience.       We also showed just how important it is to analyze how governments are collecting and defining data. Our data quality team and journalists not only worked to come up with definitions and analyses to help the public understand what data is available, but they also held officials accountable for the data they published. The COVID Tracking Project makes all of its data public, along with <a href=""https://covidtracking.com/about-data/data-definitions"" style=""text-decoration:none;""><u>data definitions  , <a href=""https://covidtracking.com/about-data/faq"" style=""text-decoration:none;""><u>detailed   <a href=""https://covidtracking.com/about-data/total-tests"" style=""text-decoration:none;""><u>usage   <a href=""https://covidtracking.com/analysis-updates/category/testing-data"" style=""text-decoration:none;""><u>guides  , and a help desk that's answered thousands of questions.       Finally, this project is also proof that journalists can do a better job compiling and explaining data than the federal government. ",https://covidtracking.com/,https://www.theatlantic.com/health/archive/2020/03/how-many-americans-have-been-tested-coronavirus/607597/,https://covidtracking.com/document/cdc-paper/,https://covidtracking.com/analysis-updates/what-weve-learned-about-the-hhs-hospitalization-data,https://covidtracking.com/analysis-updates/visualizing-covid-19s-impact-on-hospitals,https://covidtracking.com/analysis-updates/how-we-source-our-data,https://covidtracking.com/analysis-updates/category/weekly-update/,,"Alexis Madrigal, Erin Kissane, Robinson Meyer and Jeff Hammerbacher"," Alexis C. Madrigal is a staff writer at  The Atlantic , a co-founder of The COVID Tracking Project, and the author of  Powering the Dream: The History and Promise of Green Technology .       Robinson Meyer is a staff writer at  The Atlantic . He is the author of the newsletter The Weekly Planet, and a co-founder of The COVID Tracking Project.       Erin Kissane is a co-founder of The COVID Tracking Project and the former director of content for Knight-Mozilla OpenNews.       Jeff Hammerbacher is a data scientist and the founder and general partner at Related Sciences. ",6 Mar 2020
Argentina,LA NACION,Big,How LA NACION built a unique platform to monitor the Covid-19 pandemic in real time,"Explainer, Database, Open data, News application, Fact-checking, Infographics, Chart, Map","Scraping, D3.js, Canvas, JQuery, Json, Adobe Creative Suite, Google Sheets, CSV, R, PostgreSQL, Python, Node.js",   The news application website  is fed with daily information that LA NACION team manually collects from the reports published by the Ministry of Health and from automatic processes of data that the National Government publishes as well on the national open data portal. The dedicated website presents one-a-kind analysis made with health specialists.         ,"    The development of the platform gave citizens and Health specialists a tool to monitor decisions that were made by the Government following the figures of our country. Since LA NACION´s newsroom actively reuse the data that was feeding the special website, inconsistencies in numbers were found and also reported by citizens. The articles published to make visible contradictions and problems in data promoted changes in the definition of coronavirus cases and adjustments and led to the release of a daily dataset containing detailed figures of what was happening in every corner of the country. The publication increased the use of data by activists, scientists, academics and specialists in diverse fields in Argentina. In this sense, the development of the platform led to the emergence of a group of watchdogs that are in permanent contact through a Telegram group and, as LA NACION, file for FOI requests to fix inconsistencies or problems in data.  ","   Because data about coronavirus cases in Argentina is reported in PDF documents and also datasets, the development of the platform involved a combination of different software, tools and technologies to support the uploading of information in diverse formats (manual and automatic procedures).Frontend development was made in JavaScript using vue.js as main framework. To obtain data from international sources, open source repositories of the websites dedicated to the data registration were used. After obtained those data, Python was used as backend language for data parsing according to the needs of LA NACION development. For the local site, an ad hoc administrator in Django.js was developed and this was used by the team of journalists to manually upload the data from Argentina, several times a day.      As even data which is uploaded through automated processes contain errors, the team also deploy technical validations over the system to guarantee the quality of the information that LA NACION opens to the public. In this sense, our team used SNS (Simple Notifications Service), S3, CloudWatch, Lambdas, E2, Tableau, Vue.js and Big Query.  ","   The most difficult part of the project is the constant monitoring that it requires due to problems in data reported by the Government in official documents and datasets. This situation involves permanent shifts of more than 8 people at different times to check that the quality of data that we receive is consistent with the historical information of cases and also within each specific dataset or PDF document. In fact, this is one of the reasons we established human and automatic validations at specific stages of processes. We also had to train ourselves to acquire the necessary skills in order to understand and analyse Health data related to the pandemic. This expertise took time but also requires flexibility to translate the complexity that arises from national figures into relevant stories for our readers.  ","   One of the most important lessons learned is that interdisciplinary work and the expertise of journalists, developers, data scientists and activists is key to activate the demand of data in countries such as Argentina, where the Government open public information but not in a way to facilitate its consumption. In this regard, the coronavirus website shows the relevance of the power of collaboration. Alliances between our newsroom, university students and Health specialists are always stronger than a set of bureaucratic barriers that aim to discourage the visibility of the Coronavirus pandemic in Argentina. Furthermore, LA NACION, could not have developed a dashboard with more than 150 interactive visualizations without key specialists working in different disciplines.            ",https://www.lanacion.com.ar/politica/how-la-nacion-built-unique-platform-to-nid2571851,https://www.lanacion.com.ar/sociedad/en-detalle-infectados-fallecidos-coronavirus-argentina-nid2350330/#/,https://www.lanacion.com.ar/el-mundo/coronavirus-asi-se-propaga-virus-mundo-nid2351138,https://www.lanacion.com.ar/sociedad/vacunas-covid-19-cuales-llegaran-argentina-que-resultados-nid2526910/,https://www.lanacion.com.ar/sociedad/vacunas-nid2568125/,https://www.lanacion.com.ar/sociedad/coronavirus-monitor-contagios-nid2396526/#/,https://www.lanacion.com.ar/sociedad/reuniones-nid2571833/#/,,"G. BOURET, F. FERNANDEZ BLANCO, P. LOSCRI, G. FERRO, M. TRIGO VIERA, C. ARAUJO, G. DE LA LLANA, N. BASES, N. RIVERA, P. ARELLANO, B. PALLARO, D. ARAMBILLET, F. RODRIGUEZ ALTUBE, G. ALONSO, J. COSTA , J. COPELLO, N. LOUZAU, F. COELHO, R. COLMAN"," The authors are members of LA NACION Data and Visualization Teams.         Gabriela BOURET data analysis, Florencia FERNANDEZ BLANCO General Coordination, Pablo LOSCRI General Coordination, Giselle FERRO Design, Mariana TRIGO VIERA Design, Carlos ARAUJO Developer, Gastón DE LA LLANA Developer, Nicolás BASES Developer, Nicolás RIVERA Developer, Pablo ARELLANO Developer, Bianca PALLARO reporter, Delfina ARAMBILLET reporter, Florencia RODRIGUEZ ALTUBE Reporter; Romina COLMAN reporter,  Florencia COELHO reporter, Gabriel ALONSO Developer, José COSTA Reporter, Juana COPELLO Reporter, Natalia LOUZAU Reporter  ",25 Jan 2020
Brazil,"Revista AzMina, Internet Lab, Volt Data Lab",Big,MonitorA,"Investigation, Explainer, Multiple-newsroom collaboration, Database, Politics, Women","Google Sheets, CSV, R, Python"," MonitorA is an observatory of political violence against female candidates on social networks, a project by Revista AzMina and InternetLab. Throughout the election campaign, from September to November 2020, we collected hundreds of thousands of comments directed at candidates from all political spectrum on different social networks (Twitter, Instagram and YouTube). With automated linguistic filters and also with human analysis, we analyze these publications to understand the dynamics of violence during the elections and we show that political violence against women in networks is sexist and misogynistic. "," MonitorA's main objective was to prove with data what we already empirically predicted: that women are provoked by sexist political violence in political environments. With a partnership with InternetLab, an independent research center in the areas of law and technology, we were able to analyze how sexist hate speech takes place within social platforms, and by partnering with five local vehicles from five Brazilian states, we also managed cover local contexts of political violence, including offline.   MonitorA's first survey revealed that 123 monitored candidates in 7 states in municipal events received more than 40 curses a day on Twitter alone. In the second survey, in the second round of updates, we showed that other female political figures who supported women's candidacies were also attacked. The insults were mostly focused on the physical, intellectual and moral characteristics of these women, and not for their political performance, as we have shown that it happened with men.   This survey was released by the candidates themselves, who, caused by the violence in the networks, reported this harassment. Manuela D'Ávila, a former candidate for the country's vice presidency in 2018, investigates MonitorA's monitoring data in a television debate. Other candidates, such as Joice Hasseman, from PSL, candidate for the mayor of São Paulo, mentioned the surveys on social networks. A candidate for councilor in São Paulo, Erika Hilton, decided to sue more than 50 people who harassed her on the networks and talked to our team.   Our data was also reported and republished by more than 50 media outlets, including television channels such as CNN Brasil and TV Cultura, CBN radio, as well as appearing in reports by UOL, Estadão, etc. It was the first time that Political Violence became a debate in the press. "," We created Python scripts that captured publications that cited nearly 200 applications from around the country for two months on Twitter, Youtube and Instagram. On Twitter on Youtube the APIs of the respective social networks were used. Data collection for Instagram was performed using web scraping techniques. In all, 2.3 million publications on social networks were captured for analysis. The data was cleaned up and organized to address some inconsistencies, such as name changes on social networks, standardization of columns and types of data and formatting of dates. All messages were categorized as offensive or non-offensive, based on regular expressions identified by a linguist. She created dictionaries of offensive terms that covered all profiles of monitored candidacies: for white and non-white women, cis and trans, LGBTs and straight, from different political spectrum, etc. The dashboard that allowed the analysis and visualization of these data by our content team was developed in R with shiny and golem, packages used for the creation of dynamic applications. The application's filters and functionalities were improved throughout the project. With the filters and features we were able to make queries and create databases in CSV for smaller and more specific analyzes, using mostly Google Sheets. "," The hardest part was dealing with this large amount of data for journalistic analysis. We captured 2.3 million publications on social media for analysis. Of these, at least 155,000 contained offensive terms, were potentially violent and could be analyzed. Even with the automated linguistic filters created by our linguist, it still took a great deal of human analysis of these tweets: checking if they were cursing directed at the candidates, how the terms were used in different speeches, etc. Each published content required the human checking of at least 1,000 tweets in a few days, as we follow the electoral campaign calendar, which lasts only two months. All this work brought together a very diverse team: developers, data journalists, linguists, anthropologists, specialists in digital law, etc. In this way, we were able to gather different points of view on the data to make powerful analyzes of the dynamics of political violence in the networks. And it must be remembered that this is an especially sensitive content to be worked on by a mostly female team, which had to focus on misogynistic strategies of violence and attack. "," MonitorA is a project that combines technology, linguistics, journalism, law and advocacy to combat political gender violence. It is, therefore, a collaborative project: we brought together AzMina, InternetLab, an independent research center in the areas of law and technology, and five other local media outlets in five states in the country, which together produced content on political violence on the networks. With that, we were able to make cuts of territory, legal and technology cuts, and journalistic cuts on the subject.   In the technology area, it is also a major text mining and sentiment analysis project: we use linguistic filters to determine whether publications were potentially violent. This can be very useful and inspiring for other journalists and media who want to investigate hate speech on the internet: not just the terms used, but how these speeches are spread, what are the strategies used, the actors involved, the difference in speech in attacks to different profiles of people, etc. It is also possible to learn from the flows and processes used by our team to deal with this large amount of data and transform them into not only quantitative, but qualitative analyzes. ",https://azmina.com.br/projetos/monitora/,https://azmina.com.br/reportagens/violencia-politica-genero-eleicoes/,https://azmina.com.br/reportagens/ataques-a-candidatas-se-estendem-a-apoiadoras-no-2-turno-das-eleicoes/,https://azmina.com.br/reportagens/violencia-politica-de-genero-as-diferencas-entre-os-ataques-recebidos-por-mulheres-e-seus-oponentes/,,,,,"Bárbara Libório, Jamile Santana, Carolina Oms, Helena Bertho, Thais Folego, Mariana Valente, Fernanda Sousa, Alessandra Gomes, Blenda Santos, Catharina Pereira, Jade Becari, Renata Hirota, Sérgio Spagnuolo, Yasmin Curzi, Larissa Ribeiro, Carolina Herrera"," Bárbara Libório, data journalist and project manager at AzMina.   Jamile Santana, , data journalist and project coordinator at AzMina.   Carolina Oms, journalist and AzMina director.   Helena Bertho, journalist and AzMina director.   Thais Folego, journalist and AzMina director.   Mariana Valente, InternetLab director.   Fernanda K. Martins Sousa, coordinator of InternetLab's inequalities and identities area.   Alessandra Gomes, Internet Lab's tech fellow.   Blenda Santos, InternetLab's researcher.   Catharina Pereira, InternetLab's researcher.   Jade Becari, InternetLab's researcher.   Renata Hirota, Volt Data Lab's data journalist.   Sérgio Spagnuolo, Volt Data Lab's CEO.   Yasmin Curzi, researcher.   Larissa Ribeiro, designer and AzMina's art director.   Carolina Herrera, design intern at AzMina.   Karoline Gomes, marketing analist. ",5 Nov 2020
China,澎湃新闻 The Paper,Big,What 11699 Cases Tell Us About COVID-19?,"Explainer, Database, Open data, Infographics, Chart, Health","Scraping, D3.js, Three.js, Canvas, JQuery, Json, Adobe Creative Suite, CSV, Python"," Using epidemiological survey data collected over the past year, we review the spread and development of the 2020 coronavirus pandemic in China through a 3D visualization.   The first chapter is a starry sky symbolizing every individual in the prevention and containment of the virus. The second is a forest illustrating the intricate complexities of human-to-human virus transmissions. Readers are free to explore the details of each patient in the virus transmission chain. The third chapter regroups the dataset into a bar chart demonstrating the variety of disease symptoms at the time of diagnosis. "," The data for the 11699 surveyed cases were made public for further research and usage. Readers can apply for the data at the end of the interactive project. After one week of the publication, near one thousand readers apply for the data. They come from various industries, like universities, media, pharmaceutical companies, consulting companies, freelancers, and more. We encourage them to discover more stories, findings, and patterns of the dataset. After all, humans forget easily. Data shall keep the memory of the special year of 2020 for us. "," Python: Clean and analyze the dataset. Convert the data into the format for the visualization. Merge the words for interactive displaying.    Three.js: Creating and rendering visualizations. Building up animations and transformations among the scenes, like a starry sky, forests, bar charts, and more.   Shader: Data binding for background colors and icons of the circle, and group and regroup of the 3D circles. "," Collecting and cleaning data for our first version dataset was very complicated and time-consuming because the data of local health commissions are spreading out over multiple websites and platforms, and they are all written in different formats. Therefore, we need to clean the data both by hand and by python scripts.   It is also very challenging to use three.js for loading a dataset of over ten thousand rows, converting every row into a 3D bubble, transforming all the bubbles into different shapes and scenes, grouping and regrouping all the bubbles smoothly.    Calculating and visualizing the transmission chain is difficult too. The dataset for the network only contains two columns: ""from"" and ""to."" Therefore, we need to code in three.js to find multiple lines that connect to each other. "," Being a data journalist in China is hard because it is rare to run into a well-formatted public dataset, especially during the pandemic when public departments are swamped with virus controls. For this project, we try to overcome this obstacle by cleaning and organizing data ourselves and cooperating with university researchers. Special thanks to Liu Xiaofan, Xu Xiaoke and Wu Ye for sharing the full dataset with us. In contrary to us, university researchers have more time and resources to conduct long-term data cleaning with machine learning and Natural Language Processing. Therefore, the cooperation between media and university researchers can produce interesting news stories while guaranteeing the high quality of large-scale manual collecting data. This project is a great tryout for the cooperation like this. I think in the future, data journalists in China would reach out to the universities for meaningful story ideas and research datasets.  ",http://h5.thepaper.cn/html/zt/2020/12/liudiao/index.html,,,,,,,,"Zou Manyun, Kong Jiaxing, Du Haiyan, Wang Yasai, Lu Yan"," The Paper data news team features data journalism and explanatory reporting, in all forms including infographic, interactive visualization, animation video and 3D animation.   ",23 Dec 2020
Germany,CORRECTIV,Small,Kein Filter für Rechts,"Investigation, Long-form, Database, Politics, Lifestyle, Women","AI/Machine learning, Scraping, Google Sheets, CSV, Node.js"," The investigative data research #KeinFilterFürRechts, enriched with conversations with insiders from the scene and ex-developers, gave the first comprehensive insight into the right-wing parallel world on Instagram, its most important figures, strategies and codes - and showed with impressive examples how little the company does against it.         "," Instagram reacted immediately to the research, deleting a number of examples from the texts from the platform in the days following publication; German politicians took it as an opportunity to debate the topic of platform regulation. "," The team, consisting of several reporters, a data journalist and a researcher, observed the right-wing scene on Instagram for more than eight months. It developed several tools to collect and analyze large amounts of data on the platform (it had its approach peer-reviewed by two researchers).     The used tools reached from „Exponential Discriminative Snowball Sampling“ to one where we could see how strong connections between accounts on instagram are (based on follows) called „instaball“, to classical network analysis and text data mining. The process went through mulitple iterations including security and bias checks (the full process is explained here: https://correctiv.org/top-stories/2020/10/06/kein-filter-fuer-rechts-instagram-rechtsextremismus-daten-so-sind-wir-vorgegangen/#daten-daten-daten-so-sind-wir-vorgegangen)    Through the database, the team was able to map a network of Instagram accounts for the first time (4,500 accounts) and understand how right-wingers to far-right extremists communicate there and draw young people into their ideology. It turned out that Instagram's algorithmic vulnerabilities are being exploited for this purpose. "," One of the key challenges: The corporate philosophy of Facebook subsidiary Instagram prevents third parties from gaining access to the data stored on the platform. Only the online service itself and the users should be allowed to dispose of it.    For us, this meant that Instagram does not offer any way to collect data from accounts. We therefore had to get creative and find a solution ourselves in order to be able to answer our research questions. Which we did, because we believe that the public has a right to know about right-wing strategies on one the most-used social media platform these days. "," Aside from the combinational usage of different tools in order to show a network on instagram, the way we presented to story deserves attention.     So, what was special about #KeinFilterFürRechts was not only the combination of several research paths, but also in particular the narrative format: The research was published in four parts over the course of a week, with the headlines of the individual stories and their publication date being teased on the website in advance. Interested readers were able to subscribe to e-mail reminders - which they also actively used. In addition, the team used #KeinFilterFürRechts (the research title, which sounds like a slogan but describes the research result very precisely), a highly frequented hashtag, under which thousands of people exchanged views on the topic in social media. ",https://correctiv.org/top-stories/2020/10/06/kein-filter-fuer-rechts-instagram-rechtsextremismus-frauen-der-rechten-szene/,,,,,,,,"Till Eckert, Alice Echtermann, Arne Steinberg, Celsa Diaz, Clemens Kommerell", Till Eckert is a reporter on disinformation and the far right. Since 2019 he's part of the CORRECTIV newsroom. He lives in Berlin.   Alice Echtermann is the teamlead of CORRECTIVs fact-checking team. She joined the organisation in 2019 and lives in Bremen.   Arne Steinberg is a freelance reporter focussing on the football (soccer) industry and the far right. He lives in Cologne.    Clemens Kommerell is a data journalist who created a range of tools to be able to show how instagram networks are behaving. He lives in Leipzig.    Celsa Diaz is a data scientist mainly specialized on text data mining and marketing automation. She lives in Berlin. ,7 Oct 2020
Germany,Zeit Online,Big,Why Is the Risk of Coronavirus Transmission so High Indoors?,"Explainer, News application, Illustration, Health",Animation," Let's assume you meet with another household and spend four hours together indoors. Later, it turns out that one person was carrying the new coronavirus. How high is the risk that someone has become infected – and how high is the risk for you personally?    Aerosols can carry Sars-CoV-2 from one person to another. Our interactive calculator allows for readers to design a room and determine how many people will be in, under what conditions. To visualize the risk of infection for various scenarios we have used data from the Max Planck Institute for Chemistry in Germany.     "," The tool was used several million times and is even shared today daily on Social Media. It helped millions of people to be more aware of the high risk of Coronavirus transmission indoors and even served policy makers to illustrate the risk of transmission indoors. While there was a discussion in Germany on the importance of FFP2 masks, our tool showed what huge difference those masks make. ", Our first mockups are based on collaborative Zoom and Figma sessions from everyone’s home office. From there on we mainly used Javascript and React to create the web app. For the animations of thousands of aerosol particles we used the Javascript library Pixi.JS. We used CSS transforms to construct a three-dimensional room with an isometric perspective. , Our main challenge was to visualize complex scientific data in a model and to make it as accessible and understandable as possible for everyone. But at the same time we had to communicate uncertainty and error margins of said model.  ," The close cooperation between scientists and journalists has shown: together we can visualize complex findings in a vivid way. When you want to visualize people, it’s important to show the diversity of our society. We made an enormous effort to create illustrations of people with different backgrounds and characteristics. The appreciation of our readers showed us: the effort was definitely worth it. ",https://www.zeit.de/wissen/gesundheit/2020-11/coronavirus-aerosols-infection-risk-hotspot-interiors,Original German version: https://www.zeit.de/wissen/gesundheit/2020-11/coronavirus-aerosole-ansteckungsgefahr-infektion-hotspot-innenraeume,,,,,,,"Fabian Dinklage, Annick Ehmann, Elena Erdmann, Moritz Klack, Maria Mast, Julian Stahnke, Julius Tröger, Claudia Vallentin, Paul Blickle"," Fabian Dinklage (Web development and Design), Annick Ehmann (Illustrations), Elena Erdmann (Data analysis), Moritz Klack (Web development), Maria Mast (Text), Julian Stahnke (Web development and Design), Julius Tröger (Project lead), Claudia Vallentin (Text), Paul Blickle (Web development and Design) ",26 Nov 2020
United States,BuzzFeed News,Big,Inside China's Vast Infrastructure To Detain Muslims,"Investigation, Long-form, Breaking news, Human rights","Animation, 3D modelling, Google Sheets"," Combining unprecedented satellite and architectural analysis with the voices of dozens of former prisoners, BuzzFeed News exposed China’s vast new infrastructure that the government has built for the mass detention of Muslims.  "," American politicians from both sides of the aisle took notice of the series, including Sens. <a href=""https://twitter.com/marcorubio/status/1299155590065934336?s=21"">Marco Rubio , <a href=""https://www.facebook.com/jeffmerkley/posts/10157477910411546"">Jeff Merkley , and <a href=""https://www.facebook.com/senatortoomey/posts/4623282704350341"">Pat Toomey . The first two articles in the series came at a crucial moment, when Congress was debating a bill to ban imports from Xinjiang made with forced detainee labor. The legislation easily passed the House in September and is awaiting approval from the Senate.   In February, Rajagopalan wrote about the plight of Tursunay Ziyawudun, a Uighur woman who had been detained for 10 months at internment camps. Ziyawudun escaped to Kazakhstan — but was told she would have to return to China to apply for a new visa, Rajagopalan reported. Returning to the country would likely have meant that she would be detained again. In September, Ziyawudun arrived safely in the United States, ending the threat of forcible repatriation. Ziyawudun’s lawyer said that she believed the press coverage helped her case. "," Killing discovered that Baidu Maps, run by China’s state-owned Google equivalent, had blanked out many satellite images that appeared uncensored on regular Google Earth — a clue that China wanted to hide these locations from the outside world.    There were 5 million of these tiles to wade through, however, which was far too many for individual human beings to process. Camps usually need to be near towns and infrastructure, so the team narrowed their search, yielding a still-enormous dataset of 50,000 locations.    Buschek built a custom web tool to sort through the images systematically. Soon the pool of possible detention sites was much more manageable. Still, the team had to go through thousands of images one by one, verifying many of the sites against other available evidence.    Killing is a licensed architect, a skill set she deployed to analyze the sites in detail. She developed sources at satellite imaging firms and obtained high-resolution images of key locations. These images enabled her to estimate the capacity of some of the compounds. Some could hold more than 10,000 people.   Rajagopalan and Killing homed in on the camp in Mongolküre. Pairing the images with survivor accounts, they provided as complete a picture as possible of how a camp functioned from the inside: the barbed wire pens in the courtyard where detainees were occasionally brought to exercise, the passage leading from the guardhouse to the main accommodation building, the colors of the outside walls.    Using specialized software, Killing developed a 3D architectural model of the camp — which she and BuzzFeed News’ art director, Ben King, deployed to tell the story. They created a scrolling interactive that blended the model seamlessly with the written text, allowing readers to see renderings of the cell blocks and classrooms at key points throughout the piece. "," Soon after China began to round up and detain thousands of Muslims in Xinjiang, Megha Rajagopalan was the first reporter to <a href=""https://www.buzzfeednews.com/article/meghara/the-police-state-of-the-future-is-already-here"">document the rise  of <a href=""https://www.buzzfeednews.com/article/meghara/china-uighur-spies-surveillance"">mass surveillance  in the region and the first to visit an internment camp — at a time when China denied that such camps existed.   In response, the government tried to silence her, revoking her visa and ejecting her from the country. It would go on to cut off access to the entire region for most Westerners and stymie journalists.   Undeterred, Rajagopalan kept digging. She started with a simple question: Where were people being held? With the estimated number of detainees running as high as 1 million, the Chinese government couldn’t just lock everyone up in its existing prisons. But in a region bigger than Alaska, it was immensely difficult for anyone to find evidence of new detention camps.   Rajagopalan teamed up with architect Alison Killing and programmer Christo Buschek to analyze satellite images. They analyzed thousands of satellite images censored by the Chinese government, which they compared with images on uncensored mapping software. Months of painstaking work later, they discovered more than 260 sites with the hallmarks of fortified detention compounds. Many also contained factories where detainees are forced to labor. These were built to be permanent, high-security facilities, signaling that the government aimed to imprison people for years.   Rajagopalan then traveled to Kazakhstan and persuaded more than two dozen former detainees to recount beatings and humiliations in harrowing detail, as well as provide more information on specific camps. Their accounts offered essential insights into life inside, from the ubiquitous cameras to the hierarchy of prisoners. They described being taken away from their homes, the horrors of life inside the camps, and the trauma that remains with them even after fleeing China. "," Other journalists can learn from this project the value of persistence in reporting.   Expelling Rajagopalan was only one way that China tried to stop her. The government also banned people in Xinjiang from speaking to reporters.    In-person door knocking was out of the question, but so was any other form of communication, given China’s highly sophisticated and draconian mass surveillance system. The very few ex-detainees who have managed to flee China live in terror of reprisals against family members back home.    So in order to speak to people who had been locked up, Rajagopalan needed to get creative. It soon became clear that her best bet was Kazakhstan, a landing point for most of the escapees. In a country known for its own authoritarian impulses, Rajagopalan not only had to find survivors but also had to earn their trust.    It would have been a journalistic success to interview just three or four ex-detainees. Rajagopalan spoke to dozens and, as a result, gathered essential details that had never been reported.   Throughout her reporting, Rajagopalan had to endure harassment from the Chinese government, which had persisted beyond forcing her to pack up her apartment in Beijing on short notice. Its representatives repeatedly pressured her to write more-positive stories about the country. State security agents asked her to divulge her contacts. A member of the New York consulate even threatened to demand that her editor fire her. And after the first two stories ran, the Chinese government posted her personal information, including a government identification number, on Twitter. ",https://www.buzzfeednews.com/article/meghara/china-new-internment-camps-xinjiang-uighurs-muslims,https://www.buzzfeednews.com/article/alison_killing/china-ex-prisoners-horrors-xinjiang-camps-uighurs,https://www.buzzfeednews.com/article/meghara/inside-xinjiang-detention-camp,https://www.buzzfeednews.com/article/alison_killing/xinjiang-camps-china-factories-forced-labor,https://www.buzzfeednews.com/article/meghara/china-uighur-xinjiang-kazakhstan,https://www.buzzfeednews.com/article/alison_killing/satellite-images-investigation-xinjiang-detention-camps,,,"Megha Rajagopalan, Alison Killing, and Christo Buschek", Megha Rajagopalan is a BuzzFeed News reporter based in London. She previously reported for BuzzFeed News from Beijing.    Alison Killing is an architect and geospatial analyst.   Christo Buschek is a programmer and digital security trainer. He builds tools tailored for data journalists and human rights defenders.  ,27 Aug 2020
United States,FiveThirtyEight,Big,2020 Election Forecast,"Illustration, Chart, Elections, Politics","D3.js, Json, Google Sheets, CSV, Node.js"," The FiveThirtyEight 2020 election forecast combines sophisicated statistical modeling of election results with innovative data visualization techniques. For this year's version we prioritized accessibility for a wide range of audiences, usability of our data visualizations, and strong integration with our other editorial content. The result is a project that makes a complicated and commonly misunderstood subject easy to understand without skimping on detail. "," This project informed a vast audience about the range of possible electoral outcomes in a critical election year. It sparked interest in the poltiical process amongst our readership, informed our reporting and was widely cited by other digital and broadcast media. By any quantitative measure it was the most successful project FiveThirtyEight has ever created.   But perhaps it's greatest success is in what it did not do. Our focus on accessibility this year led to numerous design tweaks that demystified and simplified the forecasting process, heading off confusion and misinpretation. In particular, we used user experience research and reader feedback to ensure the uncertain nature of the forecast was well-understood by a wide variety of readers, including those with limited statistical literacy. We innovated new data visualization forms (the highly-regarded ""ballswarm"") and added a carefully designed mascot (""Fivey Fox"") to guide readers through statistical concepts and on to our other elections coverage.   This project also provided the raw materials for other projects, including <a href=""https://projects.fivethirtyeight.com/trump-biden-election-map/"">the interactive version of the forecast  we published in October. ", The forecast uses FiveThirtyEight's standard technical stack which involves a Node based build system on the backend and a lot of vanilla javascript on the frontend. We used D3 for charting. The forecast model itself is written in STATA. Some automation was written in Ruby. ," The hardest part of this project was finding the right ways to communicate the level of detail our existing readers expect, while not overwhelming or confusing new readers or those who's first exposure to FiveThirtyEight is the forecast. This a delicate balancing act which entailed extensive user research, endless rounds of design mocking, and hours of debate amongst the team. We further invested a great deal of time into careful choice of words, integration of other explanatory reporting, and accessible design components, such as screenreader summaries for data visualizations. "," Every fresh project is an opportunity to improve on what's come before. As a team we are proud of the intentions we brought to this project. We were aware of the public critiques that had been leveled against previous version of the forecast. Rather than react defensively we engaged in a comprehensive effort to build a forecast that not only engaged all readers, but truly informed them. The use of complicated statistical techniques need not create a black box. A first time reader to the forecast can walk away just as well-informed as someone who understands monte carlo simulations and the nitty gritty of details of the electoral college. ",https://projects.fivethirtyeight.com/2020-election-forecast/,https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/,https://projects.fivethirtyeight.com/trump-biden-election-map/,,,,,,"Many, many people. Please see complete list at the bottom of the project."," FiveThirtyEight uses statistical analysis — hard numbers — to tell compelling stories about politics, sports, science, economics and culture. Our Interactives Team applies cutting edge data visualization techniques to bring our reporting and analysis to life. ",12 Aug 2020
United States,"FiveThirtyEight, ABC",Big,Testing Access,"Investigation, Long-form, Multiple-newsroom collaboration, Fact-checking, Podcast/radio, Chart, Video, Map, Health","QGIS, Adobe Creative Suite, Google Sheets"," To our knowledge, this was the first nationwide review of COVID-19 testing site access. The analysis, by FiveThirtyEight, ABC News and ABC-owned television stations, found that testing sites in communities of color in many major cities faced higher demand than sites in whiter or wealthier areas in those same cities. The result of this disparity was clear: Black and Hispanic people were more likely to experience longer wait times and understaffed testing centers.   In addition to a feature-length story, this analysis led to further reporting by ABC-owned television stations, a video explainer and an episode of FiveThirtyEight's coronavirus podcast, PODCAST-19. "," This project — featured on Good Morning America, Nightline, 20/20 and elsewhere — helped bring light to a racial disparity in the COVID-19 pandemic (access to tests) that had been receiving far less media attention than other disparities (morbidity, for example). This reporting and analysis helped show how and why those disparities in outcomes happen. As a result of increased attention on the gap in access to testing, many states and cities launched and/or strengthened programs aimed specifically at adding testing access capacity in Black and Hispanic communities and neighborhoods. "," In addition to the testing site data from Castlight Health, we used the 2014-18 ACS to obtain demographic data on American cities. We then used the statistical software R to develop an algorithm which calculated how many people would be projected to use each testing site, based on the number of people, the sites’ proximity and the number of available sites. We compared block groups that were majority-white, majority-Hispanic and majority-Black within urbanized areas over 1 million people to see how accessible different testing sites were in different cities. In LA, we also used an over-time approach to compare racial disparities in access before and after the city made a concerted effort to expand to lower-income areas. We showed that this expansion helped, but that racial disparities in access remained.   The visual representation of these findings relied heavily on combining aspects of the data analysis with cartography. We used GIS analysis in R to generate maps that allowed us to compare testing site demand by census tract and demographics. We then used Adobe Illustrator to finalize the presentation of these maps to accompany the feature-length story, including the integration of map elements drawn by hand in Illustrator that added a personal identity to the graphics.   We complemented this data-driven approach with traditional reporting in the cities our data analysis highlighted, interviewing people and politicians to understand why this had happened, what was being done to remedy the situation, and what the consequences were both for individuals’ lives and for containing the pandemic at the neighborhood or city level. "," The hardest part of the project was trying to understand how to take population density into account when assessing disparities in access to testing sites. There are obviously going to be more testing sites where there are more people -- but denser places also have a different racial composition than sparsely populated places. We considered calculating something like the number of people per testing site in each county, but county borders are porous, and people can get tested in nearby counties. The algorithm we came up with was based on the assumption that people would go to closer sites, and looked at how many people each site would have to serve based on the number of people and other sites nearby. It let us compare racial disparities in access while accounting for density in a way that made it part of the story, rather than a pesky confounding variable.   The other big challenge was how to visualize these disparities. We felt good about the metric we had developed, but it wasn’t obvious how best to visualize it. Mapping two separate variables simultaneously -- in our case, race and ease of access -- is not easy, but we wanted to show the geographic component of the analysis as well as the raw stats. We did it by dividing each city into the majority-white and majority-Black or Hispanic areas, and showing ease of access for each. "," The power of combining different types of journalistic evidence. This story would have been good with only the data analysis. It would have been good with only the reporting chronicling everyday Americans’ efforts to get tests. It would have been good with only the reporting on what people in power were doing regarding COVID-19 testing access in Black and Hispanic communities. But all that evidence combined, woven together, built a credibility, power and impact that was, to use the cliche, greater than the sum of its parts.     More specifically: Our reporting provided anecdotal evidence that Black and Hispanic communities had less access to COVID-19 testing. Our data-based measure of testing access was smart, but it was also built on a few assumptions -- informed assumptions, but assumptions nonetheless. But the fact that the reporting and data analysis pointed in the same directions was key. The reporting gave us confidence in the analysis, and the analysis gave us a comprehensiveness and level of detail in finding and showing disparities that we never would have been able to do with reporting alone. The result was a reliable, national picture of access to COVID-19 testing by race and income -- a picture that showed a level inequity that demanded action. ",https://fivethirtyeight.com/features/white-neighborhoods-have-more-access-to-covid-19-testing-sites/,https://www.youtube.com/watch?v=cRMpIfA2Ytk,https://fivethirtyeight.com/features/its-harder-to-get-a-covid-19-test-if-youre-black-or-hispanic/,https://abcnews.go.com/Politics/white-neighborhoods-access-covid-19-testing-sites-analysis/story?id=71884719,https://abc7news.com/covid-19-testing-black-latino-bay-area/6330018/,,,,"Soo Rin Kim, Matthew Van, Laura Bronner, Grace Manthey, Ryan Best, Emily Scherer", Soo Rin Kim is a politics and investigations reporter for ABC News   Matthew Vann is a producer and reporter for ABC News.   Laura Bronner is FiveThirtyEight’s quantitative editor.   Grace Manthey is a data journalist for ABC Owned Television Stations in Los Angeles.   Ryan Best is a visual journalist for FiveThirtyEight.   Emily Scherer is FiveThirtyEight’s designer and acting art director. ,22 Jul 2020
Peru,Convoca.pe,Small,Convoca Deep Data: The most complete data analysis platform on extractive industries in Peru,"Investigation, Solutions journalism, Database, Open data, Chart, Map, Environment, Health, Employment, Human rights","Scraping, JQuery, Microsoft Excel, Google Sheets, CSV, R, RStudio, PostgreSQL, Node.js"," Convoca Deep Data is the most ambitious digital platform that Convoca.pe has ever developed. This platform gathers relevant information on the extractive industries of the mining and hydrocarbons industries in Peru.    In order to develop this platform we processed 2.4 million data, and analyzed information that dated back a hundred years and indicators that measure, for instance, the level of non-compliance with environmental and labor regulations in the last 15 years.    Additionally, we processed a 16-year-period of information that contains over 200 open files on environmental matters and a registry of penalties and sanctions imposed between 2014-2019. "," Given the quality, quantity, and access of the information, non-government institutions and organizations of the civil society in Peru have contacted Convoca seeking to establish alliances and partnerships. These dynamics are essential to both secure the sustainability of the project in the long term and apply our methodology to other business sectors that register too serious infractions violating labor and environmental protection norms affecting indigenous people.    In order to develop DeepData, our team had to process highly sensitive data that contains information on the mining industry, one of the most powerful economic activities that has brought up several conflicts and disputes over natural resources in Peru.    For developing this platform our team created a traffic light that identifies large oil and mining companies based on the severity of environmental law violations (highly,frequently, moderately, or little infringing). In order to do so, we established a weighted value using statistical methods to indicate the number of environmental violations and the level of severity.   Researchers, scholars, reporters, and specialized organizations are now able, for the first time, to find in an integrated way information about the laboral-environment behavior at mining sites operating in Peru. In our country, open digital platforms containing this kind of processed data are not available; hence, projects like Convoca Deep Data show that it’s possible to provide citizens with integrated and processed data related to the mining and oil industries. To this date near 350 people have subscribed to the platform.     Thanks to this platform our reporters published 10 exclusive investigations between September and December of last year. This investigation also made it possible to disclose relevant information including the number of mining workers with Covid-19 and the mining companies that benefited from government programs like Reactiva Peru that offered economic incentives to companies from different business sectors.    "," For cleaning, organization and analysis of the data we used different programs: from dynamic tables of Excel, through the statistical processing software R to generate the calculations of the default indicator and SQL for the crossing of the information from the various tables .    The processed information was complemented with the data of the coordinates of the location of the operations of the companies at the national level to georeference them and visualize them in interactive maps generated with the MapBox tool.   In some cases in order to obtain more information we had to scrap government websites, and cleaned  and processed the information using R. For data analysis, we used different techniques including dynamic Excel tables, software used for R programming to calculate key indicators, and SQL for crossing information with different tables.    We also used geographical coordinates to georeference and visualize mining companies’ operations across the country using interactive maps developed with MapBox. The platform was developed using the version 9 of Angular in order to have more control over the information. The backend was developed using Nodejs/Express.  "," Our biggest challenge was the construction of the severity indicator that was built as a traffic light for alerts to measure the degree of non-compliance by companies. We were able to do so based on the number of violations and the level of severity. This traffic light identifies a large company in the extractive industries as highly infringing (in red), frequently infringing (orange), moderately infringing (yellow) and little infringing (green).   To reach that traffic light, as a first step we organized, cleaned and analyzed more than 2,000 files for environmental violations opened from 2004 to January 2020 to mining and oil companies, as well as sanctioning processes for labor rights.   Then we established an indicator taking into account the number of infractions confirmed by government entities in Peru that overview environmental and labor matters, as well as the level of severity for each sanction (mild, serious and very serious).   Subsequently, we established a weighted value using statistical methods that summarize the number of violations and the level of severity. Finally, the list of more than 200 offending companies was grouped into four percentiles (25% each) to establish a ranking by level, which is expressed in the traffic light indicator that Convoca Deep Data shows. "," Journalists can learn that data can not only be used as a source for immediate publications, but also that data can be integrated with other databases to generate open data platforms such as Convoca Deep Data.   At Convoca Deep Data, Convoca.pe has reflected the vast experience of the organization exercising data journalism in the public interest. In order to build different databases our team filed 50 public information requests from government entities, scraped government websites, used tools like OCR to convert PDFs, and analyzed hundreds of documents.    Our work is best characterized for establishing a methodology to develop and codify the traffic light indicator, which accurately describes the degree of behavior of each of the more than 200 extractive companies analyzed as part of the development of the platform.    Establishing a strong section of methodology and understanding data involves interviewing human sources and experts from different fields, and going over hundreds of official documents.    Hence, data-driven projects need to be collaborative and interdisciplinary. To build Convoca Deep Data, a team of 14 people including reporters, analysts and data scientists, technology developers, graphic designers, and audience editors, made possible the development of the platform. ",https://deepdata.convoca.pe/,https://convoca.pe/investigacion/el-estado-todavia-no-ha-remediado-ni-uno-de-los-3448-pasivos-ambientales-por,https://convoca.pe/agenda-propia/covid-19-seis-companias-mineras-acumulan-el-88-de-casos-de-trabajadores-contagiados,https://convoca.pe/investigacion/el-estado-perdono-s-4-millones-700-mil-mineras-en-multas-por-infracciones-laborales,https://convoca.pe/investigacion/impunidad-y-muerte-en-los-campamentos-de-la-gran-mineria,https://fundacionmohme.org/convoca-deep-data-es-un-modelo-de-periodismo-que-investiga-y-abraza-la-tecnologia/,http://cooperaccion.org.pe/cifras-que-no-cuadran-minsa-y-minem-no-registran-el-mismo-numero-de-trabajadores-mineros-contagiados-de-covid-19/,,"Milagros Salazar Herrera, Edwin Montesinos, Luis Enrique Pérez, Asís Loyola, Malena Maguiña, Diego López, Jimmy Salazar, Víctor Anaya, Walter Reyes, Antonio Manco, Francisco Rodríguez, Elvis Rivera, Jackeline Cárdenas Ipenza, Jimmy Pazos, Javier Pereira."," Convoca is a Peruvian-based digital media outlet that focuses on investigative journalism, data analysis, fact-checking and innovative formats to expose the networks of political and corporate power that affect citizens’ lives. ",23 Sep 2020
Singapore,The Straits Times,Big,"Singapore GE2020: Splitting towns, shifting boundaries: A data analysis of every constituency change since 1968.","Investigation, Explainer, Database, Map, Elections, Politics","D3.js, Json, Adobe Creative Suite, Google Sheets, CSV, OpenStreetMap, Python"," Changes in electoral boundaries, which can be controversial, are closely watched in every Singapore election, including the 2020 General Election.    Just how extensive were the changes over the years? The Straits Times’ election data team ran an analysis and found that every area of Singapore has seen at least one change.    To create Singapore’s first digital archive of electoral boundary maps, the team spent months retracing all boundary changes since 1968, when the country’s first election after independence was held. "," We were able to prepare this in advance and publish on the day elections were announced on June 23 which meant we had hit peak interest on the topic. Some quotes from readers:    “Sharing this because the content presentation should be the new standard of how we consume big media; interactive, concise and visually captivating.”    “Not bad, ST coming up with a NYT style piece complete with graphics and animation. Well done.” "," Hand-drawing electoral boundaries for every election since 1968 from archived reports obtained from the local archives using GIS software.    We then placed a hexagonal grid over Singapore’s land area and counted every time an area gets assigned to a new constituency for every election.    For the interactive map, we used Mapbox.    All the rest are SVG and our team's templated modules in Vue.js. "," The first digital archive of all of Singapore's electoral boundaries since 1968 (Independence) and therefore, the first complete contextual analysis of the controversial topic of ""gerrymandering"".    Previously, comments have been made without any visual or data-driven analysis so this way we were able to pinpoint the phenomenon of what people on the ground feel - which is constantly changing boundaries - and make it more concrete with data.    Interestingly enough and because of our approach, we faced no push back in publishing this story. "," Got something political to say? Say it but verify it first! I think this project proves that even the most controversial of topics in a controversial place inside of a controversial paper, can be done with the right approach and usually data is a good way to go. ",https://www.straitstimes.com/multimedia/graphics/2020/06/singapore-general-election-ge2020-constituency-changes/index.html,,,,,,,,"Rebecca Viviana Pazos, Joseph Ricafort, Thong Yong Jun,Tampus Charles Singson, Alyssa Karla Mariano Mungcal, Xaquin G. V."," Rebecca Viviana Pazos, Interactive graphics correspondent;  Joseph Ricafort, Data visualisation designer;  Thong Yong Jun, Web developer & data analyst;  Tampus Charles Singson, Web developer;  Alyssa Karla Mariano Mungcal, Designer;  Xaquin G. V., Data and Visual Editor ",23 Jun 2020
Singapore,The Straits Times,Big,See Singapore and the world evolve through 175 years of The Straits Times' headlines,"Explainer, Long-form, News application, Chart, Lifestyle","Scraping, D3.js, Canvas, Json, Adobe Creative Suite, Google Sheets, CSV"," What were the biggest headlines that captured the attention of readers over the years? And what's the top news on the day you were born?    This was one of the projects that marked ST’s 175th anniversary in 2020.    The Straits Times’ digital graphics team captured and analysed more than 47,000 headlines that appeared on page one of The Straits Times from the very first edition on July 15, 1845 until October 15, last year, as well as those from The Sunday Times. "," The interactive uncovered topics and stories in their respective years through data analysis methods that could have been difficult to discover using manual curation. It also allows readers to explore and recall the past using interesting insights.    For example, under “epidemics”, users will learn that while Covid-19 might be dominating the headlines today, Singapore has survived previous outbreaks of deadly diseases, such as smallpox in the 1950s. In 1959, ST reported that a 49-year-old was the first person to die of smallpox here. "," The team grouped the headlines into five major topics: epidemics, foreign affairs, local towns, vices and crimes, and war and peace. Each category then allows the reader to browse through 5 to 7 topics highlighted by the respective headline images and click to find a description or story behind the trend or event.    The project uses an algorithm to help in the analysis. The algorithm gives weight to how important subjects were in their respective years. The charts help visualise how heavily each topic was mentioned in the headlines. "," Surprisingly, gathering the headlines data. It is an old newspaper and it's digital archives are not accessible to the publication through a machine-readable format. The team had to build a scraper that could pull first page content as far back as possible. Also, the front pages were typically used as classfieds or ads in the beginning, so we had to manually identify at which point The Straits Times changed editorial priorities for the front page. "," For big datasets such as these, pulling out the interesting stories was fun and also important for highlighting them to the readers. Inspiring them to then want to explore the full data at the end has turned out to be a more engaging technique. ",https://www.straitstimes.com/multimedia/graphics/2020/10/175-years-headlines/index.html,,,,,,,,"Andy Lin, Denise Chong, Faith Melody Zaccheus, Joseph Ricafort, Olyvia Lim Shi Ya, Charles Tampus, Thong Yong Jun, Xaquín González Veira, Yu Sheng Sin"," Andy Lin, Data Analyst;  Denise Chong, Journalist;  Faith Melody Zaccheus, Journalist;  Joseph Ricafort, Data Visualization Designer;  Olyvia Lim Shi Ya, Data journalism intern;  Charles Tampus, Developer;  Thong Yong Jun, Data Analyst / Developer;  Xaquín González Veira, Visual and Data Editor;  Yu Sheng Sin, Digital Sub-editor; ",30 Oct 2020
United States,"CalMatters, The Salinas Californian",Big,CalMatters and The Salinas Californian,"Investigation, Explainer, Solutions journalism, Long-form, Multiple-newsroom collaboration, Database, Chart, Map, Politics, Agriculture, Employment","Json, Microsoft Excel, CSV, R, Python"," Combining detailed census data and intimate narrative reporting, CalMatters reporter Jackie Botts and the Salinas Californian’s Kate Cimini, along with CalMatters’ data reporter Lo Bénichou, were among the first reporters nationally to link overcrowded housing to the spread of coronavirus, in their <a href=""https://calmatters.org/projects/overcrowded-housing-california-coronavirus-essential-worker/?"">Close Quarters series  published June 12. The team’s insightful analysis found the communities hit hardest by the virus had three times the rate of overcrowded homes — and twice the poverty rate of better-off communities. "," When coronavirus began spreading in California, public health officials told people who fell ill to isolate themselves in their homes for two weeks. However, Cimini and Botts had been covering the California Divide for nearly a year and were well aware that many California families simply can’t do that. Forced by the high cost of living, parents squeeze into rooms with their children, cousins sleep on couches, grandparents convert living rooms into bedrooms and farmworkers pile into barracks-style bunk beds.   As the shutdown dragged on into the summer, various state and county hotel programs began offering shelter to essential workers as part of the government's response to slow the spread. In July, Gov. Newsom announced Housing for the Harvest, a hotel program to provide quarantine and isolation rooms for farmworkers who live in crowded housing. It’s clear the team’s reporting helped raise awareness about the link between essential workers and the overcrowded places to which they return home. "," In early April, Botts began pulling data from a handful of county dashboards that were starting to publish case counts by ZIP code. She compared these to Census Bureau data. A clear correlation between crowding and COVID-19 emerged. She consulted an NYU researcher, who had found a <a href=""https://calmatters.org/projects/california-coronavirus-overcrowded-housing-data-analysis/?"">similar pattern  in hard-hit New York City. Over the following month, she requested and analyzed the number of coronavirus cases in each ZIP code for 10 California counties with the highest case counts.   The team’s insightful analysis found the communities hit hardest by the virus had three times the rate of overcrowded homes — and twice the poverty rate of better-off communities. In neighborhoods that were most impacted by COVID-19, 82% were people of color. In the <a href=""https://calmatters.org/projects/overcrowded-housing-california-coronavirus-essential-worker/?"">first article , reporters told the stories of Californians living in crowded homes in the Central Coast, Los Angeles, and the Imperial Valley. In the <a href=""https://calmatters.org/projects/california-coronavirus-overcrowded-neighborhoods-homes/?"">second , reporters zoomed into overcrowded hotspots in Alameda, Los Angeles and San Diego.    The effort was a true team effort. CalMatters’ Matt Levin crunched Census data to reveal the deep racial, health and economic inequities plaguing essential workers living in crowded homes. La Opinion’s Jackie Garcia and the Desert Sun’s J. Omar Ornelas contributed interviews and photos. Bénichou’s designs further amplified the team’s analysis through informative charts and maps. The team published a <a href=""https://calmatters.org/projects/california-coronavirus-overcrowded-housing-data-analysis/?"">guide  to help other reporters replicate the analysis. "," The hardest part was the intricate work: They requested positive coronavirus cases in each ZIP code from more than a dozen counties and calculated the number of infections per 1,000 residents. After that, they painstakingly paired neighborhood-level coronavirus data with American Community Survey demographic data to capture overcrowded hot spots.   The results were striking when comparing the highest and lowest neighborhood rates of infection. Benicho’s designs further amplified the team’s analysis on informative charts and maps. "," Neighborhood-level demographic information is public. So is public health data on the pandemic. When weaved together, reporters can uncover the truth about the extent of COVID spread in poor and minority communities, forcing government leaders and policymakers to confront systemic failings from street to street.  ",https://calmatters.org/projects/overcrowded-housing-california-coronavirus-essential-worker/,https://calmatters.org/projects/california-coronavirus-overcrowded-neighborhoods-homes/,https://calmatters.org/projects/california-coronavirus-overcrowded-housing-data-analysis/,,,,,,"Jackie Botts, Kate Cimini, Lo Benichou"," Jackie covers income inequity and economic survival for the <a href=""https://calmatters.org/projects/california-divide/"">California Divide  collaboration. She previously reported for the Data and Enterprise desk for Reuters News, where her team won 3rd Place in the Philip Meyer Data Journalism Award and the Hillman Foundation’s June Sidney Award for ""Shielded,"" a 2020 data-driven investigative series into an obscure legal doctrine called qualified immunity that protects police accused of excessive force.   Kate Cimini is a reporter with the Salinas Californian and CalMatters' California Divide project. She covers economic inequality, agriculture, and housing.   Lo Bénichou is a visual journalist at CalMatters. ",12 Jun 2020
Brazil,"Abraji, Brasil.IO",Small,CruzaGrafos,"Investigation, Multiple-newsroom collaboration, Database, Open data, Fact-checking, OSINT, Elections, Politics, Corruption, Money-laundering, Business","Scraping, Json, Google Sheets, CSV, PostgreSQL, Python"," CruzaGrafos is a free software graphic tool for cross-checking and advanced data investigations, by allowing the visualization of relationships in graphs, which allow the interconnection of various information in a kind of web. In its current phase it already has 70 million data, from all companies in Brazil and Brazilian politicians since 2014. It is a project created by Abraji (Brazilian Association of Investigative Journalism) and Brasil.IO (a Brazilian open data hub). Our intention is to transform these huge and difficult-to-access databases into visualizations of power relationships that can be seen by non-data science experts. "," It is a work of more than a year carried out by the teams of Abraji and the programmer and transparency activist Álvaro Justen, from Brasil.IO, with the support of the Google News Initiative.   The authors of the project understand that knowledge and understanding of large public databases is one of the ways to improve investigative journalism, especially the data have cross-relationships, context and check.   Many of the databases used originate from the work of Claudio Weber Abramo, an activist for transparency and a pioneer of data journalism in Brazil, who died in August 2018 - who founded the non-profit organization Dados.org together with the journalist and ex President of Abraji José Roberto de Toledo. Abramo's family kindly gave up the bases he built for Abraji to continue some of his work.   Thus with this experience of Abramo and Toledo, the teams of Abraji and Brasi.IO created a project to explore large Brazilian databases and the graph solution to facilitate access to information.   With this initiative, last year we conducted a pilot course in data journalism and compliance techniques using CruzaGrafos and other databases of public interest, for 80 journalists from all regions of the country.   This year we created an investigative journalism newsletter derived from the project (Investigadora), which already had more than 700 subscribers at the end of January, and has the proposal to show investigative journalism techniques weekly using CruzaGrafos and show recent cases of journalistic investigation Brazil based on evidence. An online training program was also created in January for newsrooms, freelancers, students and third sector organizations focused on open data and transparency - the intention is to show the potential of CruzaGrafos and the possibilities of data journalism to improve the journalistic work. "," The technical team used programming languages such as Python, CSS, JavaScript, HTML, Shell and PLpgSQL. All the steps described in obtaining the data, checks and the source code of the platform will also be made available on Github this year.   Technology actions were taken, such as:   - Data processing (partners of Brazilian companies, Brazilian CNPJs - unique company identification code in Brazil -, corporate activities by CNPJ, political candidacies, political donations, health contracts, among other main bases to be selected for launch and over the course of 2021)   - Expanded neighboring nodes and Expanded neighboring nodes by up to 2 degrees have been implemented, allowing you to quickly expand the visualization of the graphs of connections between people and companies - shows the degrees of connection nearby   - The ""Save graph"" feature was made, which will be very useful during the tests - not only to make life easier for the person testing, but also to help us debug in case of errors   - We built a solution to calculate the “path between objects”, which calculates the shortest path between two people/companies and shows in the graph   - We added a functionality that was not initially planned, but that will help a lot in usability, after tests that we did internally: browse the history of the objects (people and companies) searched   It was also very important all the previous work of scraping the databases and exploratory data analysis with Python, SQL and Metabase to understand the information, clean and prepare it for use in production. "," The enormous size of some databases, with tens of millions of rows, required creating alternatives to make the tool both fast and interactive. And we also deal with data with names of people and companies and IDs - they are sensitive data that require a lot of checking controls and to avoid homonyms.   Because of these difficulties, innovations in the project code were necessary. The main innovations in code technology at CruzaGrafos were:  - (1) Entity centralizer: it provides the search for names, companies, municipalities, hospitals, contracts, etc., and gives us the unique universal identifier (UUID). Entities can be: companies, people, applications etc. The lack of a UUID brings problems such as the need to filter through several fields at the same time (which change from dataset to dataset), difficulty in searching in more than one dataset, difficulty in generating the offline ID for external queries, among others  - (2) Graph backend: this is the ""heart"" of the system, which connects to the previous system for searching and manages queries in the graph bank, API etc.  - (3) CruzaGrafos: here we have the ""glue"" of everything and it is the most specific part that only matters to Abraji: it is where we have the integration with the authentication of the Abraji associate system, where we have the scripts that feed the two systems above and the interface the user accesses.   The project is open freely to Abraji associates and anyone who signs up as a user.         "," This project wants to take advantage of the potential of open data and transparency. In the Global Open Data Index, Brazil is in ninth place, so there is a lot of relevant information for society, but most of the time these files are not machine readable, or very large, or without technical details. This makes the work of data journalism more difficult or restricted to people already familiar with data science.   This way, all the work with the data done by the team will allow thousands of journalists and researchers to have access to this information ready for use. And with graph visualization, it is possible to research power relations among millions of people, companies and politicians.   CruzaGrafos has all Brazilian companies registered with the IRS - 43.9 million companies. Also information on political candidacies, a total of 1.1 million people, according to the Superior Electoral Court (TSE). Thus considering companies, their partners and electoral candidacies, the project now has 70.7 million data.   This creates many possibilities for investigation, such as: Search for all companies linked to a politician/candidate for public office in which he or she is a partner or administrator; In these companies see who are the other partners; Also check the proximity network of these partners, that is, of which other companies they are partners and the other respective partners, in different degrees of proximity; and many others.   Also, throughout 2021, the project will continue to update the IRS and TSE databases and include others of public interest on the environment, public contracts, electoral campaign donations and health. Always with the proposal of allowing crossings with identification keys that can be seen in graphs. As well as our newsletter and training throughout the year, they also intend to spread the Project, its information and research techniques. ",https://cruzagrafos.abraji.org.br,https://www.abraji.org.br/noticias/abraji-project-allows-advanced-visual-journalistic-investigations,https://www.abraji.org.br/cruzagrafos-user-s-guide,https://onlinejournalismblog.com/2020/11/20/brazilian-journalists-launch-network-analysis-tool-to-investigate-political-relationships/,https://youtu.be/ITbbkZlqNGs (tutorial with English and Spanish subtitles),,,,"Álvaro Justen, Reinaldo Chaves, Daniel Bramatti, Marcelo Träsel, Cristina Zahar, Stefano Wrobleski"," Cristina Zahar, is executive secretary of Abraji, Deputy Sports Editor and Economy Editor at Folha de S.Paulo, and Editorial Director of Abril Collections; Daniel Bramatti, former president of Abraji and editor of Estadão Dados, data journalism nucleus of the newspaper O Estado de S.Paulo; Reinaldo Chaves, project coordinator at Abraji, programmer and author of several data journalism projects in Brazil; Álvaro Justen, programmer, professor of programming and founder of the open data portal Brasil.IO; Stefano Wrobleski, data journalist and editor at InfoAmazonia and coordinator of geojournalism at the Earth Journalism Network at Internews; and Marcelo Träsel is a journalist, doctor in Social Communication (PUCRS), professor at the Faculty of Library and Communication at the Federal University of Rio Grande do Sul and current president of Abraji ",5 Nov 2020
Qatar,Al Jazeera,Big,Could mega-dams kill the mighty River Nile? (an interactive report),"Investigation, Explainer, Long-form, Cross-border, Quiz/game, News application, Fact-checking, Illustration, Infographics, Chart, Video, Map, Satellite images, Environment, Economy","Animation, 3D modelling, R, RStudio"," 280 million people from 11 countries live along the banks of the Nile. For Ethiopia, a new mega-dam known as the Grand Ethiopian Renaissance Dam (GERD) holds the promise of much-needed electricity. For Egypt, this translates into the fear of a devastating water crisis. “Saving the Nile” is a science-led data visualization explainer measuring the impact Africa's largest hydro-dam could have on the precious water and agricultural resources of downstream countries. Working in partnership with NASA scientists this story helps Al Jazeera's global audience – in English and Arabic – understand the normally complex scientific concepts affecting an entire region. "," The Renaissance Dam dispute remains one of the African continent’s biggest stories.   As a result, the project was the most viewed and shared content explainer on the subject in 2020 with more than half a million page views and an average time on page of 5 minutes.   The web-based scientific report and models have been cited by various academic papers and research institutes and the sharable social media graphics continue to be shared across various digital platforms today.      "," The core experience was built using Mapbox Scrollytelling built in React.js   Data models, based on peer reviewed research, were calculated and interpreted using the R programming language. Designs were then rendered as artists' impressions using the Adobe Suite of tools (Photoshop, Illustrator, Animate and Premiere). "," Academic papers are often complex and difficult to understand. ""Saving the Nile"", interprets and visualizes complex earth and space concepts without compromising scientific accuracy.    This was particularly important given the polarizing political positions on the GERD project.   Throughout the interactive experience we present readers with animations, quizzes, data visualizations, maps and infographics. Each visualization was carefully crafted to improve readers’ understanding of the sensitive biodiversity along the world’s oldest and longest river. To reach the largest possible audience the story was produced in English and Arabic across digital platforms including Mobile, Facebook, Twitter, Instagram and YouTube. This, to our knowledge,  makes ""Saving the Nile"" one of the most comprehensive resources on the River Nile by a news organization. "," At the time of this writing, Egypt and Ethiopia have still not reached an agreement on the filling of the GERD reservoir.    While this topic will likely remain heavily debated, the environmental data and models provided (and subsequently published) in this story provides readers and policy makers with a scientific perspective on a real-world issue that could impact over 280 million people. ",https://interactive.aljazeera.com/aje/2020/saving-the-nile/index.html,https://www.facebook.com/aljazeera/posts/10158460897773690,https://twitter.com/AJEnglish/status/1220355738058608641,,,,,,"Dr. Essam Heggy, Mohammed Haddad, Zouhaier Hamdani, Mohamed ElAli, Moawia Al Zubair, Carla Bower, Hala Saadani", Dr Essam Heggy is an earth and space scientist at NASA.   Mohammed Haddad is Al Jazeera's interactive data editor.  ,23 Jan 2020
Bosnia and Herzegovina,"Organized Crime and Corruption Reporting Project, Kloop, RFE/RL Radio Azattyk, and Bellingcat",Big,The Matraimov Kingdom,"Investigation, Explainer, Cross-border, Multiple-newsroom collaboration, Database, OSINT, Illustration, Infographics, Map, Satellite images, Politics, Corruption, Money-laundering, Lifestyle, Crime","Scraping, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, OpenStreetMap"," “The Matraimov Kingdom” exposes the corruption, illicit riches, and political influence of a powerful Kyrgyz clan headed by Raimbek Matraimov, a former top customs official.   During his years in customs, Raimbek enabled and profited from a massive smuggling empire worth over a billion dollars.    But though Matraimov was later fired, his stranglehold over Kyrgyzstan’s customs service has only grown. Our stories showcase his family’s extravagant lifestyle; unveil his connection to an infamous underground criminal boss; and explain how the Matraimovs converted their wealth into political clout. "," On October 5, 2020, just days after publication, a mass protest took place in Bishkek. Citizens were outraged at the corruption the stories revealed — and at the fact that the Matraimovs’ political party, which benefited from their illicit wealth, had done so well in the parliamentary election through vote buying and other illegal tactics.   The protests quickly grew into a popular uprising and a revolution that led to the ouster of the president and the cancellation of the <a href=""https://www.occrp.org/en/the-matraimov-kingdom/pro-government-election-victory-sparks-overnight-revolution-in-kyrgyzstan"">election results .   Kyrgyzstan’s new government has promised to pursue corruption with a new focus. On October 19, the State Committee of National Security (GKNB) <a href=""https://kloop.kg/blog/2020/10/19/gknb-vyyavleno-40-chelovek-iz-okruzheniya-matraimova-kotorye-mogut-byt-prichastny-k-korruptsii-na-tamozhne/"">announced  that it was investigating Raimbek Matraimov’s corrupt customs schemes, which had been revealed in the investigation. They said they had already identified 40 customs employees tied to Matraimov.   On the next day, Matraimov was <a href=""https://rus.azattyk.org/a/30903519.html"">arrested  (but later released under a pledge not to leave Kyrgyzstan). The new government reached an agreement that he would pay about $24 million into the treasury as compensation for the stolen funds. He is now on trial for corruption.   On October 22, Kamchybek Kolbaev, the criminal boss our investigation links to Matraimov, was <a href=""https://kloop.kg/blog/2020/10/22/gknb-v-bishkeke-zaderzhan-kriminalnyj-avtoritet-kamchy-kolbaev/"">arrested  under charges of “establishing organized criminal groups.”   Kyrgyzstan’s new post-revolutionary acting Prime Minister has <a href=""https://kloop.kg/blog/2020/12/07/naladit-uchet-i-uprostit-protsedury-kak-novikov-pytaetsya-navesti-poryadok-na-tamozhne/"">pledged  to eliminate corruption in the customs service and introduce a number of anti-corruption measures.   One of our sources, who was both a witness and a victim of Matraimov’s corrupt schemes, was released from custody after the uprising. He had been imprisoned after speaking with reporters.   On December 9, the United States <a href=""https://kloop.kg/blog/2020/12/09/ssha-vnesli-rajymbeka-matraimova-v-sanktsionnyj-spisok-magnitskogo/"">sanctioned  Raimbek Matraimov and his spouse under the Global Magnitsky Act.   On December 24, the GKNB head told the press that Matraimov had <a href=""https://rus.azattyk.org/a/31020231.html"">confessed  to having established corrupt schemes in the customs service.     "," Using open-source investigative techniques, reporters were able to expose the extravagant lifestyle the Matraimovs enjoyed despite Raimbek’s modest official salary.    As it turns out, Raimbek’s wife was an enthusiastic social media poster. Reporters gained access to her account and found hundreds of images that showed her, her children, and her friends cruising on private jets, enjoying yacht outings, and staying in deluxe hotels. Her posts were used to make an interactive timeline that shows the family travelling the world in luxury even as Raimbek worked in government.   Reporters also used his wife’s social media posts to catalogue her purchases of luxury watches, clothes, shoes, bags, and jewelry. These were listed in a spreadsheet, and web tools such as Yandex Reverse Image Search helped reporters identify the items and establish their value: Over $400,000 in total.   The photos also led reporters to a previously unknown acquisition: A penthouse apartment in Dubai. Google Earth Pro, Street View, and corroborating photos posted on social media were used to identify the exact location of the Matraimovs’ new apartment.    Turning to more traditional investigative methods, reporters used the Kyrgyz land registry to find two dozen properties — from elite apartments to busy places of commerce — owned by the Matraimov family. Analyzing the associated corporate records showed that many were acquired in a dubious way. An interactive map published with the project details these possessions and explains their backstories.    Corporate and tax records enabled reporters to build a database of Matraimov companies and use a tool called Maltego to visualize the links between them. A public list of Kyrgyz voters helped locate more relatives and connections. On-the-ground reporting then uncovered how the family used its network of proxies to obscure the virtual monopoly it had built over major international trade routes.     "," The first problem was security. Covering the Matraimov family means covering criminal groups, from Raimbek’s street-tough “sportsmen” to organized criminal bosses.    Many Kyrgyz reporters have been attacked for reporting on the family. While working on this investigation, an RFE/RL reporter was warned by a former Matraimov associate that the shadowy operator wanted him “dead or alive.” He provided proof showing that the reporter had been followed by a private detective. Moreover, one of the key sources for a related investigation was murdered.   Extreme measures of secrecy and security were undertaken at all steps of the reporting, especially for journalists working on the ground in Kyrgyzstan.   Reaching sources was difficult because the Matraimov family is so powerful and influential that few were willing to discuss them. Lots of time and dedication was required to build trust. One source, contacted because his business had been seized by Matraimov associates, was arrested and jailed.   Reporters faced an ethical dilemma when a source asked for a delay in publication because she feared her husband, who was involved in one of the stories, could come to harm. We made the difficult decision to move forward because the stories were in the public interest and our judgement was that the safety risks were minimal. Later, she understood our position and took no offense.    A regional office of one of the publication partners was surveilled by unknown people every time the Matraimov family was sent requests for comment. There were also constant online attacks by trolls and bots working in support of the Matraimov family.   Another difficulty was obtaining corporate documents. At one point, the Kyrgyz Ministry of Justice refused to provide company information in violation of the freedom of information act. Kloop has sued the ministry and the case is now before the Supreme Court.     "," This project shows the value of approaching an investigation from multiple angles, covering the political, economic, and even social aspects of corruption. Instead of limiting the stories to examinations of individual instances of corruption, the stories set these criminal acts in the context of Kyrgyzstan’s fragile political system and struggling economy.   This holistic element is precisely what was missing from previous reporting about the Matraimov family. Some of their suspicious acquisitions and actions had been reported, but no outlet has ever published a comprehensive look at this family, explaining not only what they had done but how they had done it — and how Kyrgyzstan’s brand of crony politics enabled them to get away with it.   In order to maximize the impact of these investigative stories, the project was intended from the ground-up to be a compelling narrative experience. Capturing the human element was prioritized in reporting, writing, and editing.    Multiple interactive features, as well as charts and explanatory boxes, ensure that readers understand the complex criminal schemes being described and appreciate their impact. The reporting is backed by two explainers that prove, step-by-step, how some of the most important findings were established. In a region known for sensational journalism and widespread rumor, these materials establish the investigation as a trustworthy standard and model for future reporting on crime and mass corruption.   Such a sprawling investigation, with multiple partners and reporters working in multiple languages, would not have been possible without intentional project management and a systematic way of storing and sharing data. Databases, calendars, shared drives, encrypted group communication, and other tools were employed to manage the project — and maintain security in a dangerous environment. ",https://www.occrp.org/en/the-matraimov-kingdom/,https://www.occrp.org/en/the-matraimov-kingdom/the-matraimov-family-properties,https://www.occrp.org/en/daily/13282-kyrgyz-authorities-arrest-matraimov-the-700-million-man,https://www.occrp.org/en/the-matraimov-kingdom/the-beautiful-life-of-a-kyrgyz-customs-official#matraimov-timeline,,,,,"This project is a collaboration between OCCRP, Kloop, RFE/RL's Radio Azattyk, and Bellingcat."," This project was reported and published by the Organized Crime and Corruption Reporting Project (OCCRP); Kloop, a Kyrgyz outlet; Radio Azattyk (RFE/RL’s Kyrgyz service); and Bellingcat. Due to multiple threats received by reporters and editors over the course of the reporting, the names of individual contributors are not disclosed.     ",2 Oct 2020
United Kingdom,Sky News,Big,Sky News: Automation in the newsroom to cover the COVID-19 pandemic story,"Explainer, Breaking news, Database","AI/Machine learning, Personalisation, Scraping, Json, Microsoft Excel, Google Sheets, CSV, R, RStudio, Python, Node.js"," Sky News has developed a system to automate part of the data process to improve the management and communication of the COVID-19 story.   It automatically checked for updates and gathered new data from a dozen sources every 15 minutes. It also cleaned and re-structured it before storing it in a database.   This database became the starting point of multiple analyses, as well as the source of more than 50 automated visualisations.   This system saved the data team many hours, as well as helped our readers to keep track of the numbers, putting them into context and providing local information. "," In a year in which data teams have exceeded their healthy capacity, this system has been crucial for the team at Sky News.   Data sources routinely used for analyses were integrated into this automation process. As soon as a new source became relevant on an ongoing basis, we incorporated it to the in-house database, automating the process of gathering, cleaning and restructuring the data.   Because of this, the most essential and widely used data was “ready to be used” at any time in analyses and visualisations that updated in real-time.  That saved the data journalists hours of repetitive work and made the story-telling more visual, creative and efficient.   Automation has enabled us to respond to the high demand for stories and improve the quality of our journalism. The time saved has been invested in researching, collecting more specific data to complement official sources, and facilitate explainers and investigations. This has expanded our knowledge, allowing us to give our audience more comprehensive information about coronavirus and its impact, covering angles not included in official data.    This system has also been hugely beneficial to the wider newsroom, laying the foundations to explore automation and Artificial Intelligence at Sky News. These vital technologies for the future of journalism make newsrooms more efficient, enrich our coverage and help us to personalize stories relevant for people’s lives.      The impact can also be seen in the response of our audience.  Millions of users have engaged in data-rich stories and explainers created through this automation project.   The range of what we offered has been extensive, from analyses of the evolution of the pandemic to the impact of the virus in several sectors. And we have presented it at all levels (national, regional, local and postcode) to make the information as personal as possible for our audience. "," Data gathered from all the sources required reformatting. But datasets were scattered around and consisted of text published within static pages, CSVs, Excel, Google sheets, PDFs, JSON, XML feeds, and various dashboards that didn’t provide exporting methods.   We built a cloud-based solution using FaaS (Function-as-a-Service) microservice architecture, where each atomic service handled specific tasks to detect changes in the source and extract, format, clean-up and store the data.   Vision and Document artificial intelligence services managed data extraction in places where it couldn’t be exported: public dashboards, images, and unstructured files.   The extracted data was stored in Big Query tables and automatically updated multiple data feeds which were load-balanced across global content delivery network.   Flourish Live API with JavaScript was used across visualisations to consume live data feeds and dynamically edit content to keep the graphs up to date. For some visualisations, postcode API was also required to allow the user to look for their local information.   As some of the information wasn’t published in a machine-readable format, but given in press conferences or government documents, we created spreadsheets in Google Sheet and Microsoft Excel in which journalists – even without technical knowledge - could easily import the information that was linked to our in-house database.   Using R programming language, the data journalists could then easily query the database for stories. As the data kept the same format, we wrote several scripts that have been used and re-adapted multiple times to speed up the process, allowing more time for research and to improve the presentation and the design of our stories.  "," Consistent data was a key challenge since the beginning of this project. Sources have changed the structure of the data and their sharing methods multiple times, as well as backdating the changes without notification. To ensure accuracy, this system has been in constant revision.   Access to some data has proved difficult on multiple occasions. Some institutions did not allow data to be downloaded, but just explored it in their dashboards. We reached an agreement with some of them for a downloadable file, but we also had to develop complex scrapers to get daily numbers for some of the most basic variables.   The lack of a common body or place which united all the relevant information, especially at the beginning of the pandemic, increased the time required to identify the sources and the relevant data in each of them. And, as there isn’t a common standard for how the information is published, the cleaning and formatting process was quite laborious.   All these data challenges could have been better handled with time, but the urgency of the story, together with the relevance of breaking news at Sky News, increased the pressure on the project and the need to complete each stage quickly, without affecting the quality of the product.   The fast-moving and changing nature of the COVID-19 story has also contributed to the pressure.  We have been constantly revising, adapting and incorporating new sources and variables to the process.   It should also be noted that the two people involved in this project could not work on it exclusively but had to combine it with other responsibilities. Prior to the start of the project, there was also no proper data structure and architecture in the newsroom, and no established data team.  This was created months later.   "," Investing time and resources in automating processes and reducing repetitive tasks greatly improves the efficiency, efficacy and quality of the product. The benefits are particularly relevant to small teams, which could not otherwise compete with larger ones.  It also helps them to react to the pressure and needs of the newsroom in a demanding and fast-changing context.   Although big and multidisciplinary teams can build complex systems, small teams can benefit hugely from automation, identifying time-consuming repetitive tasks which would have an important impact on journalists’ efficiency and their stories if done automatically.   The lack of technical resources is usually the main inconvenience, especially in small teams. Working on this project, we have learned that identifying the right people within the company and collaboration are the keys.   Editorial teams normally lack technical professionals, but they are embedded in bigger groups, teams or companies. Building synergies with other departments within the newsroom or within the company can be a solution to that shortage of technical knowledge.   Data journalists are usually a bridge between the editorial and the technical teams. They have more technical skills and, although they are not at the same level as the developers, they have more knowledge and understanding of the requirements than the editorial team.  This contributes to more fluent communication and smooths the delivery of the project.   This has been an innovative project for Sky News, something the organisation has never done before. Developing it within the company instead of externally has also been crucial. It has given Sky employees the opportunity to expand their knowledge and skills and it has created the know-how within the company. Now these skills exist in the team, they are being re-used on other projects and even expanded as we continue to explore automation and AI in the newsroom. ",https://news.sky.com/story/coronavirus-in-the-uk-how-many-have-died-or-tested-positive-where-you-live-and-where-the-new-hotspots-are-12047916,https://news.sky.com/story/coronavirus-how-covid-19-is-spreading-around-the-world-12061281,https://news.sky.com/story/what-are-the-restrictions-in-your-area-find-out-with-our-uk-map-and-postcode-checker-12102497,https://news.sky.com/story/covid-19-wheres-your-nearest-coronavirus-vaccination-centre-12194966,https://news.sky.com/story/covid-19-key-data-behind-the-controversial-tiers-decision-as-mps-threaten-rebellion-12144281,,,,"Carmen Aguilar Garcia, Przemyslaw Pluta"," Przemyslaw Pluta is an award-winning creative technology leader with more than 10 years’ experience in innovation and product development across broadcast and digital platforms. In his current role as the Head of Platform Solutions at Sky he is responsible for introducing and developing new innovative solutions across all platforms along with identifying strategic opportunities to evolve emergent technologies.       Carmen Aguilar is an award-winning journalist with more than 10 years of experience in several media from Spain, Chile and the UK. She is currently a senior data journalist at Sky News working for all platforms. She covers the whole data process, from gathering, cleaning and analyzing data to telling data-led stories and communicating them with visualisations. She also offers support and advice to other reporters regarding data stories. Since the beginning of 2021, she is part of the Data and Forensic team at Sky News. ",1 Mar 2020
United States,"Independent; formerly WBEZ, The Chicago Reporter",Small,Tracking COVID-19 in Illinois,"Explainer, Multiple-newsroom collaboration, Database, Open data, News application, Mobile App, Infographics, Chart, Map, Health, Human rights","Scraping, D3.js, Json, Google Sheets, CSV, PostgreSQL, PostGIS, Python, Node.js"," Far from the noise and spectacle of 2020, massive audiences were hungry for reliable information on their communities and Tracking COVID-19 in Illinois delivered it.   Our series of graphics and tools tracking Illinois’ COVID-19 numbers and policies made the information accessible for journalists, academics and Illinoisans making everyday choices in a pandemic. Our work is available for anyone to embed on their own platforms. Our focus on reporting-driven design met people where they were, served fundamental information needs and offered easy access to the underlying data we collected, some of which is the only public, stable record of its kind. "," Tracking COVID-19 in Illinois reached a mass audience more efficiently and effectively than either government or legacy news outlets. It generated investigations, academic research, and community engagement.    We powered journalism. After we published our ZIP code map, WBEZ reporter Maria Zamudio reached out to obtain the case and testing data. We added a download button for anyone to use. Zamudio’s <a href=""https://www.wbez.org/stories/testing-lags-in-latino-communities-hit-hardest-by-covid-19-in-chicago/50613d3b-8770-45a3-ba6b-76f97bc6e1d0"">investigation of testing rates  revealed systemic disparities and led to expanded testing in Latinx neighborhoods.    We reached the experts. Suburban civil servants used our historical data in their planning and in community meetings. The data has been used in at least four academic research projects that we know about.   ""We were really happy to find the Illinois data set that you made available, and to be able to use it as an example of what could be possible if more local and state health departments would be willing to release their data with that degree of detail,"" Jarvis Chen, a research scientist at the Harvard School of Public Health who used our data for a <a href=""https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1266/2020/04/HCPDS_Volume-19_No_1_20_covid19_RevealingUnequalBurden_HCPDSWorkingPaper_04212020-1.pdf"">national analysis of the unequal racial and socio-economic burden of COVID-19 , told us.   Most of all, the project was a hit — and in a global pandemic, reach matters. Our project has reached roughly 1.2 million users via our embeds on more than sixteen sites and native apps. The embeds have been viewed almost 2 million times. We suspect many more because our work is used in newsletters like Block Club Chicago and shared on social platforms.   The spread of the disease is a numbers game. If 1 in 100 of the people who saw our work decided to wear a mask or avoided an unnecessary trip even once, then our project helped avoid a few unnecessary deaths and many more hospitalizations. "," This project represents the cutting edge of local journalism. The technology is sophisticated, reliable, and represents a great value. In almost a year, we’ve spent well under $200,000 to reach other a million people in multiple languages. Our competition did less with more resources.         Translation    Almost 14 percent of people in Illinois are native Spanish speakers. To cover the pandemic in Illinois, our work needed to be in both English and Spanish. But translation is often expensive, time-consuming, and technically challenging. We approached the challenge by prioritizing information design over traditional newswriting and with sophisticated use of accessible technology like Airtable.         Collaborative coding    Our Observable notebooks lowered the barrier to entry for collaboration and helped make translation possible. Because of our innovative use of this technology, reporters and editors without hard technical skills could make significant contributions.   The project was successful in the Observable ecosystem; our historical data notebook was one of the top ten most <a href=""https://observablehq.com/collection/@observablehq/2020-viral-notebooks"">viral  and <a href=""https://observablehq.com/collection/@observablehq/2020-most-viewed-notebooks"">viewed  notebooks on the platform, and we <a href=""https://observablehq.com/@observablehq/may-2020-virtual-community-meetup-archived"">presented our work to the community  in May 2020.         Embedding    Observable notebooks also meant we could easily share our data graphics with their powerful embedding system. Pasting a simple snippet of code worked for over 16 websites and native apps.          Geolocation    We use geolocation to effectively personalize the experience and meet our audience where they are. The unobtrusive, internet address-based location service makes our graphics more friendly and inviting.         Data management     We employed clever hacks like using a content delivery network to mirror the state’s data and avoid expensive servers early in the project. Now we use a “serverless” GraphQL database that grows and shrinks based on our traffic. Because of our work with these technologies, we’ve been <a href=""https://hasura.io/events/hasura-con-2020/talks/hasuras-role-in-data-journalism/"">advocates for data journalism  in the broader tech world. "," Like any project, we had issues with dirty and incorrect data, like funky ZIP codes. The state kept moving the data and changing the format. We used plenty of shoe leather to confirm and understand the data.  But that’s why local organizations should invest in data reporters. We made the reporting calls and wrote the code.    The organizational backdrop of this project was stark and challenging. Like many others, we had to contend with the steadily deteriorating conditions faced in local news in the past year.   All three of the project’s originators have moved on to new jobs, and neither WBEZ or The Chicago Reporter (effectively closed since Sept. 2020) currently support it. Despite the troubles these venerable organizations are facing, they deserve recognition for launching and sustaining this project during a critical period.    Our continued persistence in spite of these conditions shows that our project is one of the best value propositions in journalism.   About 80% of our 1.2 million web users came from Illinois; we estimate that about 1 in 10 people in Illinois have seen our work. Hundreds of thousands in Spanish. Our donation campaigns did better when we talked about our commitment to meeting our audience where they are and open data. Our newsletters and social posts had better engagement when they included strong data visuals about the pandemic. Our work powered investigations, academic research, and <a href=""https://www.facebook.com/belmontcraginunited/posts/1483824208486790"">community discourse . ","The lessons from this project are systemic and organizational. Design matters In the internet world, journalists can skip writing multiple, disconnected stories about updates delivered at press conferences and instead consistently updated and accurate editorial products.That requires an investment in design, as different policies and numbers often require bespoke approaches. Our editorial products were built to be low-word (easily translatable) and fundamentally visual (easily shareable). Reach people where they are Our competition makes the user do a lot of work; we automatically locate you. In turn, we reached the most affected parts of the state. The highest traffic to our ZIP code map came from the ZIP codes most affected by COVID-19 and ZIP codes with the largest numbers of workers. Be there every day Our rock-solid technology and low operating costs means even now, thousands of people get clear, accurate information about COVID-19 in their community. And we created a real feedback loop with our community, including concerned citizens, health care experts, and local policymakers. Complement national news We offer a viable, local alternative to national coverage. As Amy Cesal said, ""National news and counts are overwhelming. I just want to know what's going on immediately near me. And the local news [sources] I read also use this one."" Practice radical generosity Over 16 publications used our data products. They included the Univision native app, the Chicago Sun-Times website, municipal government websites, rural public radio sites, and suburban mom-blogs. The graphics have been used by Block Club Chicago, The TRiiBE, and other local publications on their social feeds and in their email newsletters. The Illinois ZIP code-level data is perhaps the only historical public record of ZIP code-level COVID case data broken down by age and race demographics, and it goes back nearly to the start of the pandemic.",https://observablehq.com/collection/@chicagoreporter/illinois-coronavirus-data,https://www.wbez.org/stories/coronavirus-in-illinois-updated-cases-and-deaths/749dc1ea-1ca8-40df-915d-d74a5c673085,https://www.chicagoreporter.com/how-is-covid-19-affecting-your-zip-code-in-illinois/,https://www.chicagoreporter.com/whats-open-in-your-region-of-illinois-coronavirus-plan/,https://www.wbez.org/stories/what-is-cps-school-reopening-plan-heres-a-simple-breakdown/ee1e9bff-ea56-4daa-bc76-f2ab0ff7919d,https://observablehq.com/@chicagoreporter/chivaxbot?collection=@chicagoreporter/illinois-coronavirus-data,https://observablehq.com/@chicagoreporter/historical-illinois-covid-19-zip-code-data?collection=@chicagoreporter/illinois-coronavirus-data,,"Paula Friedrich, David Eads, Asraa Mustufa, Pat Sier"," Many people worked on this project from around Illinois. The project is currently independently operated by volunteers and maintained by David Eads as a personal/independent project. Here's the entire team.    Core editorial team:       Paula Friedrich (formerly WBEZ): Editorial product design and development, data graphics, reporting       David Eads (volunteer/formerly The Chicago Reporter): Editor, data engineer, reporting       Asraa Mustufa (formerly The Chicago Reporter): Editor, engagement reporting, reporting, project coordination       Pat Sier (City Bureau): Data engineering, editorial product design and development          Translation:       Gisela Orozco (freelance)          Design and development contributors:       Will English IV (freelance)          Reporting contributors:        Josh McGhee (The Chicago Reporter)       Becky Vevea (WBEZ)       Kate Grossman (WBEZ)       Janelle O’Dea (St. Louis Post-Dispatch)          Editing contributors:       Fernando Diaz (The Chicago Reporter)       Al Keefe (WBEZ)       Angela Rozas O’Toole (WBEZ)    ",20 Mar 2020
Spain,elDiario.es,Big,Why women stopped giving birth on weekends or holidays in Spain,"Investigation, Database, Chart, Women, Health","D3.js, JQuery, Json, Microsoft Excel, CSV, R, RStudio", A data analysis of more than 15 million births that took place in Spain since 1975 show that every year more and more births in public or private hospital are scheduled on labor days and children are born much less on holidays and weekends than 40 years ago. This analysis lead us to prove that doctors and administrative personnel in hospitals scheduled births on week days to avoid working on weekends or holidays or preventing natural birthing.  ," This project was one of the most read of the year in elDiario.es and also one of the one that made most people suscribe and pay for eldiario.es during the year, according to our statistics. For the first tim e exclusively, we revealed the dates of birth of millions of people in Spain.In addition, several organizations related to the promotion of natural childbirth that lobby the goverment shared the research, highlighting the excessive medicalization of childbirth in Spain. During this days a lot of women shared their stories on social media on how they experienced schudeled birth without any explanation of the medical staff and without any complication on their pregnancy process, this stories showed how the data are related to experiences and proved a systematic process from the staff in the hospitals.  "," R and Rstudio for data compiling and data analysis, D3, Datawrapper and Flourish for visualizations, Javascript for scroll narrative. "," The most complicated part of the project was to analyze and clean the database with more than 15 million births in Spain since 1975. In some cases, the data had to be reviewed because there were births that were dated on non-existent days and also to harmonize the variables of 4 different forms with which births have been registered in Spain during the last 40 years. This was the first time that the National Statistics Institute deliver this information to journalists by request.  ", This project can inspire journalists from other countries to request public data on births and analyze the evolution of the medicalization of childbirth in each country. Is there a trend in other countries to be born on a working day and less to be born on a holiday? This can be a crossborder project that tries to colllect birth data from a list of countries. This investigation can prove there is a tendency of obstetric violence that is only related to dates where medical staff works.  ,https://www.eldiario.es/nidos/no-ninos-nacen-toca-dar-luz-semana-21-probable-hacerlo-lunes-viernes_1_6400307.html,,,,,,,,"Raúl Sánchez, Marta Borraz, Paticia Gea"," Raúl Sánchez: Spanish data and investigative journalist covering stories of inequality, gender and corruption and Covid at elDiario.es. He coordinates elDiario.es data team.   Marta Borraz: Spanish journalist specialized in gender committed to human rights and journalism with a gender perspective.   Patricia Gea: Spanish jorunalist specialized in families, parenting and conciliation. Always linked to the social and to the service of human stories. ",22 Nov 2020
Portugal,Público,Big,School success. These four municipalities row against the tide,"Explainer, Solutions journalism, Illustration, Infographics, Chart","Animation, Json, Microsoft Excel, CSV, R, RStudio"," Using the data from the national exams and the percentage of poor students by municipality, I calculated the euclidean distance to a hypothetical place where all students were poor and yet had 20 out 20 on their national exams. That led us to four municipalities that contradicted the trend of a poorer environment meaning worse marks at school and talked with schools and local authorities to try to understand what was being done differently in those places. "," Every time the national exam results get published, it’s always the schools from richer areas that get highlighted by the media. Using this statistical approach - a bit more statistically complex than simply ordering columns in Excel - I was able to give a voice to schools that don't often get highlighted for their good work in very precarious conditions.  That also led to good critics of my piece, especially because every time this data is turned publically, the major criticism made to the media is that they always ignore socio-economic factors. "," The national exams database is a huge .mdb file that the Ministry of Education provides to the journalists under embargo. I’ve used R to read that database and build the excel file for other non-data savvy reporters that were working on other stories about this issue, highlighting possible stories.   Since the data on the database gives us the results for all students in the country, I’ve started to filter out the results for the eight exams with more students and grouping by municipality calculating the mean value for every municipality. Then, using the data about the percentage of students that the government provides some kind of aid because of their parents' income, I’ve created a scatter plot with that data and calculated, for every point, the distance to the point where 100% of the students were poor and had an average of 20 out of 20 on the national exams. That led us to the four municipalities that were the outliers (I’ve done the same for the previous year's data just to check if those municipalities were the outliers only by chance this year).  Then, and while I’ve talked with school directors and the mayors of those cities, I’ve used scrollama.js to build a scrollytelling piece where I could explain visually what led us to those schools and, as we presented the data from those places, the piece makes a social demographic explanation about the place and tells us about what is being done differently there. "," Even though education is a priority for our newspaper, the hardest part of this project was finding time to do it. With covid-19, almost all the newsroom resources seemed to be channeled to cover the pandemic. Being the only data journalist in the newsroom, it happened the same to me. So finding time to write about something that was not directly associated with the pandemic was hard. "," I would say that the biggest lesson here is that sometimes you just need to look to a database a bit differently to find a new story. This database is published every year, so naturally, everyone ends up getting similar stories. Adding a bit of statistical sophistication can lead you to new approaches.   Another lesson on this project is that I broke all rules or left the explanation about how we got to the story to the middle/end of the piece. Because explaining how we got to those four municipalities was essential to understanding why they were important, the infographic that changes through scroll started precisely there - to show why those places were important to have a look at. ",https://www.publico.pt/rankings-escolas-2019/sucesso-escolar-quatro-concelhos-contra-mare,,,,,,,,"Rui Barros, Andreia Sanches"," Rui Barros is a data journalist/ journocoder/ news nerd currently working at PÚBLICO, a daily newspaper in Portugal. Being the only data journalist in the newsroom, he does everything from doing data-driven investigations, news applications or simply helping someone by scraping a website.  He uses R - mostly the tidyverse family of packages - to do everything data-related and uses HTML, CSS, and JavaScript on his interactive works and data visualizations.     ",27 Jul 2020
United States,"ProPublica, The New York Times",Big,Chinese Misinformation,"Investigation, Long-form, Multiple-newsroom collaboration, Database, OSINT","AI/Machine learning, Scraping, CSV, Python"," The series used a combination of computational techniques and investigative sleuthing to uncover in unprecedented detail how the Chinese government spreads propaganda and disinformation online, both on the Chinese internet and on social media around the world.   Additional publication dates:     30-07-2020   19-12-2020  "," The projects had the following impacts:       Twitter released a dataset of accounts linked to Chinese state-backed coronavirus propaganda in June 2020, three months after we published our article, though they declined to respond when we shared accounts with them prior to publication.       After our story about Larry King, Ora no longer allowed King to tape infomercials on the set.       Rep. Jim Banks (R-IN) introduced the Countering Chinese Propaganda Act (H.R. 8286) in September 2020, citing our reporting.    "," For the first two stories, we scraped millions of interactions on Twitter by suspicious accounts to identify bot networks within them and uncover their origins. Data scraped included tweets, media posted, likes, accounts following and accounts followed. We stored this data in a postgresql database running on Amazon Web Services (AWS). We collected nearly 170GB of data for analysis (about 3GB of account activity and about 167 GB of media).   We used the Twitter API and twint, an open-source Twitter scraping API, to find and scrape millions of interactions by potential bots within the network. We analyzed the data that we had scraped using open source machine learning libraries such as scikit-learn, xgboost and the Chinese language natural language processing library jieba. We did our network analysis of bot account networks using the library networkx and the visualization software gephi.   For the third story, we received a leak of more than 90GB of files, including secret government directives and memos, as well as contracts, documents and other media. We used computer scripts to generate a spreadsheet database from the thousands of directories within the file structure. This allowed our team to read and annotate the files collaboratively. "," It was infeasible to examine by hand each of the tens of thousands of accounts we scraped to determine if they are connected to the same scheme. Instead, we used machine learning to identify likely Chinese state-backed fake accounts based on profile information (e.g., account name, twitter handle, account age, profile picture, etc.) and account behavior (e.g., timing, frequency and language of posts, retweeting and liking activity, etc.). We labelled hundreds of known fake and real accounts by hand and used that information to build a machine learning model that could analyze data from an account it had not seen before and assess the likelihood that it was also fake. We then sampled and checked our results by hand. This process allowed us to identify more than 10,000 similar inauthentic accounts with suspected links to the Chinese government.        It is also often difficult to connect a fake network with its ultimate operators. In this case, we combined our data analysis with reporting to link a 2,000 account bot network to a Chinese internet PR company hired by state media. "," Collaborative projects that combine traditional reporting with data can produce unique works with impactful outcomes. These stories combined reporters with diverse skill sets. We integrated our expertise in multiple domains, including investigative reporting and data reporting; data analysis and machine learning; Chinese language, culture and government; knowledge about the tech industry; ability to read and understand code; and cutting edge document analysis tools. The end result was a set of investigations that very few other teams are capable of doing. ",https://www.propublica.org/article/how-china-built-a-twitter-propaganda-machine-then-let-it-loose-on-coronavirus,https://www.propublica.org/article/the-disinfomercial-how-larry-king-got-duped-into-starring-in-chinese-propaganda,https://www.propublica.org/article/leaked-documents-show-how-chinas-army-of-paid-internet-trolls-helped-censor-the-coronavirus,,,,,,"Jeff Kao, Mia Shuang Li (for ProPublica), Renee Dudley, Raymond Zhong (New York Times), Paul Mozur (New York Times), Aaron Krolik (New York Times)"," Jeff Kao is a computational journalist at ProPublica who uses data science to cover technology and artificial intelligence. He used natural language processing techniques to uncover 1.3 million fake comments submitted to the FCC in its proceeding repealing net neutrality. This work was cited in the Washington Post, Fortune Magazine and engadget, among other publications, and by members of the U.S. Senate. He has appeared as a data scientist in the New York Times and on the WNYC program Science Friday.       Renee Dudley is a tech reporter at ProPublica. Before joining ProPublica in 2018, she was a member of the enterprise team at Reuters, where she reported extensively on issues with college-entrance exams. She uncovered a U.S. higher education admissions system corrupted by systematic cheating on standardized tests. Following public outcry over the use of leaked SAT exams and other issues Reuters uncovered, the test's maker vowed to fix the problems. The series was named a 2017 Pulitzer Prize finalist in National Reporting. ",26 Mar 2020
United States,"ProPublica, The Texas Tribune, The South Bend Tribune, Arkansas Nonprofit News Network",Big,The Pandemic Economy,"Investigation, Solutions journalism, Database, News application, Politics, Business, Economy, Employment","Personalisation, Scraping, QGIS, Json, Google Sheets, CSV, PostgreSQL, PostGIS, Python"," ProPublica placed a bet at the beginning of the pandemic: that by creating our own databases of local court cases from cities and states across the country, we could find unique and impactful stories about how the pandemic was affecting people who are on the margins of poverty in the United States. The result was a series of investigations into how debt collectors and landlords used courts to squeeze the working class despite the toll of the pandemic on their health and finances.   Additional publication dates:     18-05-2020   08-06-2020   24-07-2020   26-10-2020   31-08-2020   12-12-2020  "," By shining a quantitative light on the punitive methods used by landlords and debt collectors to extract money from the poor during the pandemic, our reporting triggered local and national changes.   In response to our reporting about evictions filed in violation of the CARES Act, landlords dismissed more than 100 eviction cases that they had illegally filed against tenants. While reporting this story, it became clear that many tenants were confused as to whether their property was covered under the new law, so we built an app to help renters determine if they were covered by the federal ban. The app was <a href=""https://www.banking.senate.gov/hearings/oversight-of-housing-regulators"" style=""text-decoration:none;""><u>cited   by Senator John Tester during the Senate Committee on Banking, Housing, and Urban Affairs on June 9, 2020.   Our joint investigations team with the Texas Tribune used court data in Texas to uncover a lender who filed thousands of lawsuits against low-income borrowers during the pandemic. When the company learned about our investigation, it dismissed all its pending lawsuits across the country and temporarily suspended new suits. The company would not confirm the exact number of cases it dropped, but our data shows it filed about 10,000 lawsuits in Texas alone during the first half of 2020.   Another investigation found that one Indiana school district filed nearly 300 lawsuits against parents during the pandemic, most for unpaid textbook fees. After contacting the district, it announced it would forgive the unpaid fees of one family mentioned prominently in the story and vacated the small claims judgment filed against her.   Citing our reporting on debt collection cases filed or pursued by debt collectors and banks during the pandemic, Senators Elizabeth Warren and Sherrod Brown <a href=""https://www.warren.senate.gov/oversight/letters/senators-warren-and-brown-call-for-capital-one-and-major-debt-collectors-to-immediately-suspend-lawsuits-and-wage-garnishment-practices-during-the-covid-19-pandemic"" style=""text-decoration:none;""><u>sent letters   to Capital One and other companies named in our story, demanding they suspend aggressive collection activities during the pandemic. "," We scraped court cases from more than a dozen of the nation’s largest counties and some statewide court systems to produce our analyses for this series. That entailed writing scrapers tailored to each site in Python and Ruby, and collating the data across jurisdictions into a single sensible format for our reporting. We also geocoded and linked data sets (including court cases, lists of federally-backed properties and parcel map files) together using PostGIS, Mapbox and Vue.js to analyze illegal eviction filings.       Our database resulted in an interactive news app where readers could look up whether they were covered by the CARES Act and state eviction moratoria, as well as to find properties with large numbers of eviction filings during the pandemic. We used Postgres to store the data and ran queries in SQL and Pandas to analyze the data for various stories. Plaintiff names were extensively cleaned using regular expressions and text clustering in OpenRefine. "," It was a significant technical challenge to coordinate scrapers made by multiple people for multiple court websites to run weekly and create an accurate and up-to-date picture of debt and eviction cases filed in counties across the country. We made a suite of in-house tools and processes for doing so in a uniform fashion. Still, each had to be hand-crafted and vetted for completeness and accuracy, and entailed speaking with legal and housing experts on the ground across the country to understand each unique court system and local context.        Additionally, our decision to link data together to analyze evictions at the building and ownership level gave us a unique tool -- one that, to our knowledge, no other organization has -- that allowed us to publish unique insights on eviction practices during the initial waves of the pandemic. "," If a database doesn’t exist that you want, create it yourself. Likely, that means you’ll have new insights that reporters, researchers and government agencies haven’t uncovered before. Additionally, if there is information that isn’t readily available to the public, or is siloed across different agencies, building tools to make it accessible for everyone is not just a public service, but can inform concerned citizens and ultimately affect policies. ",https://www.propublica.org/article/despite-federal-ban-landlords-are-still-moving-to-evict-people-during-the-pandemic,https://projects.propublica.org/covid-evictions/,https://www.propublica.org/article/capital-one-and-other-debt-collectors-are-still-coming-for-millions-of-americans,https://www.propublica.org/article/the-eviction-ban-worked-but-its-almost-over-some-landlords-are-getting-ready,https://www.propublica.org/article/when-falling-behind-on-rent-leads-to-jail-time,https://www.propublica.org/article/the-loan-company-that-sued-thousands-of-low-income-latinos-during-the-pandemic,https://www.propublica.org/article/the-pandemic-hasnt-stopped-this-school-district-from-suing-parents-over-unpaid-textbook-fees,,"Ellis Simani, Ren Larson, Jeff Ernsthausen, Al Shaw, Paul Kiel, Kiah Collier, Maya Miller, Perla Trevizo, Justin Elliott, Kim Kilbride, Benji Hardy"," Jeff Ernsthausen is a data reporter at ProPublica. He previously worked for the Atlanta Journal-Constitution as a data reporter, and before that, worked as an economic analyst at the Federal Reserve.   Ellis Simani is a data reporter at ProPublica. He has worked at the Los Angeles Times as a graphics reporter and also interned with the Seattle Times’ News Apps team.   Ren Larson is a data reporter with the ProPublica-Texas Tribune investigative unit. Ren previously was a data journalist at The Arizona Republic. In her life before journalism, she was an urban planner, data analyst and case manager.   Al Shaw is a senior news applications developer at ProPublica where he uses data and interactive graphics to cover environmental issues, natural disasters and politics.   Paul Kiel covers business and consumer finance for ProPublica.   Kiah Collier is an investigative reporter for the ProPublica-Texas Tribune Investigative Initiative. She previously worked at the Tribune as a reporter and associate editor since 2015, covering energy and environment through the lens of state government and politics.   Perla Trevizo is a reporter for the ProPublica-Texas Tribune Investigative Initiative. Trevizo is a Mexican-American reporter born in Ciudad Juárez and raised across the border in El Paso, Texas, where she began her journalism career.   Maya Miller is an engagement reporter with the Local Reporting Network. She works with journalists across the country on community-centered investigations.   Justin Elliott has been a reporter with ProPublica since 2012, where he has covered money and influence in the Obama and Trump administrations, the American Red Cross, and TurboTax maker Intuit. ",16 Apr 2020
Finland,Helsingin Sanomat HS.fi,Big,"Expensive Helsinki, cheap countryside?","Open data, News application, Economy","Personalisation, Microsoft Excel"," Expensive Helsinki, cheap countryside? Our story let the reader explore how much it would cost for them to live somewhere than where they currently live. It’s true that housing prices are high in the capital, Helsinki, but e.g. taxes and the cost of a car vs public transportation can be much higher in a small countryside municipality. Our story takes these and much more into account and calculates exactly the amount of money they’d have at their disposal if they moved to another town or municipality in Finland.  "," For an individual, the decision on where they live is a crucial one. Cost of living varies hugely around Finland. However, comparing different places is easier said than done. Housing prices, housing taxes, community taxes, transportation fees, fees for visiting doctor and e.g. the need to own a car varies hugely from municipality to municipality. Furthermore, the average salaries differ from place to place. White collar workers earn more in Helsinki than elsewhere but for nurses or factory workers the difference is not so big.     Our ground-breaking story took all these and more into account. The reader was asked to fill basic information about themselves and then our story calculated where in Finland they would have the most and the least money left after compulsory expenses.    Our story helped 175000 of our readers to evaluate where they would make most out of their money.  "," All this was made possible with the unique concept of changing story designed by Helsingin Sanomat. The story changes with the selection that reader makes, in this case by filling in information about their current home town, salary, house size, transportation preferencies etc. The story was coded with the javascript framework Vue.js.    For the story we acquired many different data sets, eg about housing prices, public transportation, doctor visits and taxes with different income levels in different municipalities.  ", The hardest part was to make the big picture and standardise costs: so many things affect on the cost of living. It was also hard was to get enough data for each municipality. We needed to do a lot of adjustments and manual work for small municipalities , It’s often very fruitful to combine the personal and general level in one story. Along to the reader specific data in the story there was general information on society level and the interviews of two persons living in different parts of Finland. Through them we were able to explain on general level how living in different parts of the country can really affect the cost of living.  ,https://dynamic.hs.fi/2020/elinkustannukset/,,,,,,,,"Päivi Ala-Risku, Elisa Bestetti, Anu-Elina Ervasti, Reijo Hietanen, Uolevi Holmberg, Kaisa Rautaheimo"," Päivi Ala-Risku, data gathering and producing    Elisa Bestetti, visualization and code    Anu-Elina Ervasti, reporting, data gathering and writing    Reijo Hietanen, photos    Uolevi Holmberg, illustrations    Kaisa Rautaheimo, photos  ",22 May 2020
Finland,Helsingin Sanomat HS.fi,Big,Where are the 2015 asylum seekers now?,"Investigation, Long-form, Infographics, Immigration","Animation, Canvas, Microsoft Excel", In 2015 Finland saw over 30 000 asylum seekers coming to Finland. It was a number that this small country had not seen before. This story investigates what has happened to these people. This is done by collecting data from Finnish immigration officials and by interviewing the immigrants themselves.   , To our knowledge this was the first time that this topic was investigated in Finland to this extent. The biggest impact was that the first time the data was presented to the public in this concise format and the data was also given a face - which is important.  , In the visualizations every circle represents 10 asylum seekers that came to Finland in year 2015. Altogether there were 32 477 of them.    The animations progress on scroll to visually show different demographic distributions of this group of people and their different destinies.    The animations are made with HTML Canvas controlled by GSAP ScrollTrigger.  ," Two things:    Getting the data from the officials. Even though the Finnish Immigration Service offers some data in their data portal it does not answer the basic question here: of those who came to Finland, how many have been granted an asylum, how many have been deported, how many are still in the process etc.     We finally got the data from the officials but the numbers don’t exactly add up in the end. That is because the number of people we know from 2015 is counted in persons but officials now count decisions – and one person can have multiple decisions made about him/her.     This said, we still managed to show the big picture.  ", Sometimes it is a good idea to check back on things that were big news in the past. This is especially true about data journalism using data from official sources because usually it takes time before the data is available.     Returning to a major news topic after few years can bring new light to the whole topic. This added with interviews of people who are affected by all this brings the data a face.  ,https://dynamic.hs.fi/2020/ne-30-000/,,,,,,,,"Jussi Konttinen, Antti Yrjönen, Ea Vasko, Konstantin Neugodov, Antti J. Hämäläinen, Anssi Miettinen, Juho Salminen, Lauri Malkavaara"," Jussi Konttinen, text    Antti Yrjönen, photographer    Ea Vasko, photo editor    Konstantin Neugodov and Antti J. Hämäläinen, visuals and code    Anssi Miettinen, text editing    Juho Salminen and Lauri Malkavaara, producing  ",5 Dec 2020
Singapore,Reuters,Big,Stopping the spread: Reaching herd immunity through vaccination,"Explainer, Infographics, Health","Animation, D3.js, Canvas, Adobe Creative Suite, Python, Node.js"," Reuters built an epidemic model to simulate various aspects of the Covid-19 pandemic and how the virus might spread within a population. We ran the model thousands of times to compare when “herd immunity” would kick in for various reproductive numbers and under a range of scenarios.   The model enabled us to show readers what level of immunity would likely be required in order to stop the spread.   An interactive feature also allows parameters to be adjusted to show things like balancing vaccine distribution, effect of interventions like masks and distancing, and people “travelling” and possibly causing super-spreader "," The project provides an evergreen resource for those looking to estimate herd immunity projections or to better understand the concept. It has attracted high readership figures and gathered a lot of attention on social media. The piece was also picked up by other news organisations.    The model is still shared and used widely to date, five months after publication. "," The underlying model was hard coded from scratch using javascript before being pulled into an HTML page and styled using CSS. We were also able to use javascript to isolate some set parameters to show visual comparisons side by side, such as the various immunity levels.    We also batch processed 100,000 runs of the simulation locally to give a comprehensive sample of outcomes, allowing us to accurately show where the herd immunity range fell. We were then able to create some separate SVG static graphics from this data to present in the page as part of the explanation.    We also decided to host the full model at the foot of the page which let's readers adjust parameters in real time for a reactive experience. "," There was no easy part to this project. However, two main challenges stood out above the rest.      Accuracy of an epidemic model   There are a number of variables which can be inserted into an equation to allow an outcome. The challenge for us was to carefully select data which fairly represented the virus and strike a balance between a model which was unrealistically simple and overly complicated, potentially introducing a wider opportunity for error. We then had to make sure all of the behavioral elements between the variables were correct, including social interventions like wearing masks and social distancing.    We worked with epidemiologists and mathematical modeling academics for months to ensure our model was as accurate and realistic as possible, while also making clear the simulation is based on set parameters and assumptions. We also added the interactive at the bottom of the page to allow these parameters to be adjusted.      Running in the browser   Another challenge was ensuring smooth delivery in the browser while hundreds of thousands of calculations are being made live.  "," Sometimes if there is little or no hard data available on a subject, it may be an opportunity to make something completely new and provide a service to readers. It can be a daunting task to build something like this from scratch, especially when a team doesn't have in-house expertise in epidemiology. Taking the extra time to work with experts and learn the theory of the subject can provide a solid foundation for an ambitious project. ",https://graphics.reuters.com/HEALTH-CORONAVIRUS/HERD%20IMMUNITY%20(EXPLAINER)/gjnvwayydvw/,,,,,,,,"Manas Sharma, Simon Scarr. Jane Wardell"," The Reuters graphics desk publishes visual stories and data visualisations to accompany the Reuters news coverage. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and produces many of the visual stories published. ",21 Aug 2020
Philippines,Rappler,Small,Cat-and-mouse game: Twinmark fake network still thrives on Facebook,"Investigation, Database, Illustration, Chart, Politics","Scraping, Microsoft Excel, Google Sheets, CSV"," Just a few weeks after Facebook took down the pages of Twinmark Media Enterprises for coordinated inauthentic behavior in January 2019, the questionable network was already back on the platform, and it continues to exist. to date.   We found that Twinmark relied heavily on its partner Facebook pages to amplify content from its sites and increase clicks. Facebook took action and removed some Twinmark-linked pages and accounts, but only after the issue was raised with them. "," The data-driven investigative piece unearthed the loopholes in Facebook’s system and how these are being used by the same groups/companies/websites that the tech company took down for coordinated inauthentic behavior.   The story also brought to public attention the tactics they use in finding their way back to business, gaining fake engagement, and spreading propaganda and mis/disinformation. My story led to better understanding of the issue among Filipinos, something crucial as we approach a presidential election.   Twinmark and its network have been instrumental in spreading hate, propaganda, and disinformation since 2016. In the run up to the 2022 presidential election, this poses even greater threats to our struggling democracy.   Ultimately, because of my story, Facebook took down several accounts and pages mentioned, thereby confirming results of our months long investigation. "," We used our own social media monitoring tool, which monitors publicly available posts on Facebook, to keep track of the accounts' history of activity. I also closely monitored specific users, accounts and websites daily for almost 5 months. On top of this, I created a database using website identifiers to link the new and previously taken down websites.   A verified internal Twinmark database also provided an added layer of verification for the connections I identified.    I then used Flourish to display the connections among websites, and Facebook accounts and pages so the audience can further understand the ties. "," The most difficult part in doing this project was determining the exact links between:       Old Twinmark websites that were banned by Facebook and the new websites that either skirted the takedown or were created after the ban       Facebook accounts and the new Twinmark websites      Another difficult part was determining the real people behind these accounts and websites. They really tried to hide in anonymity, learning from their mistakes before the first Facebook takedown in 2019.    But we were able to identify them using a variety of methods: through our own social media monitoring tool, their website identifiers, manual monitoring of suspected accounts for 5 months, and through interviews with at least two former Twinmark employees and one Facebook page administrator.   The nominated article should be selected for the award because of how it used social media data to seek accountability from Facebook and for continuously shedding the light on the fight against disinformation in the country. "," While social media data are publicly available, analyzing them could be difficult and complicated. Despite this, the story proves that constant tracking of publicly available posts can help track and fight disinformation.   Millions of Filipinos are on Facebook. It’s practically our Internet. Because of this, the platform has become the real estate of choice by organized disinformation campaigns in the Philippines, including that of the state, even before it was used in the West.   It is critical for the media to learn, unearth, and make the public aware of the different disinformation and propaganda strategies, and to consistently do such stories so as not to drop the ball.   In a way, the story held power, including Facebook, to account. After we broke the story, Facebook took down the offspring of the original networks that were shuttered.   Despite this, we continue to ask: What else should be done?   We continue to do similar stories especially as we prepare for our 2022 presidential elections.   Using data obtained for this project, a colleague and I are currently working on a follow up story which investigates the roles of Filipino celebrities and influencers in disinformation. ",https://www.rappler.com/newsbreak/investigative/remnants-twinmark-media-enterprises-fake-network-still-thrives-facebook-after-takedown,,,,,,,,Camille Elemia," Camille Elemia is Rappler's multimedia reporter for media, disinformation, and democracy.   Prior to this, she covered Philippine politics for years.   A fellow of the Konrad Adenauer Stiftung, she graduated with a master’s degree in journalism from the Ateneo de Manila University in 2016.   In 2017, she won the International Labour Organization (ILO) Global Media Competition on Labour Migration for her series on the plight of undocumented migrant workers in Southeast Asia.   In 2019, she was one of two Filipinos awarded the prestigious Fulbright – Hubert H. Humphrey Fellowship. She spent a year in the US, learning about data journalism, digital journalism, and media entrepreneurship, among others. ",23 Dec 2020
Kyrgyzstan,https://kloop.kg/,Small,"""I would have killed her anyway"". Kloop's investigation of femicide in Kyrgyzstan","Investigation, Explainer, Database, Open data, Illustration, Chart, Women, Crime, Human rights","Scraping, Microsoft Excel, Google Sheets, CSV, Python"," Our project investigates   femicide in Kyrgyzstan . Its uniqueness is that before us nobody has ever conducted any studies on women’s murders, what are the patterns of those murders and what are the grounds behind femicide in the country. Using the data we collected from various sources, we proved that women’s killings are the primary consequence of systematic domestic violence. Women are rarely being murdered outside their home, and the majority of murderers are their intimate partners. Our project tells the stories of women, provides an open-source database of femicides available for download and describes the policy response from the state. "," Our project is not just the data-journalism project, but a complete comprehensive research. The impact of it includes:       We, first of all, introduced the term “femicide” into the media space not just in Kyrgyzstan, but in Central Asia. Before our research was presented, the mass-media was predominantly covering the cases of domestic violence, rarely paying attention to femicide itself. Not just mass-media, but also NGO and research institutes, whose focus is gender equality, have systematically studied the problem of femicide. The data on women’s murders was not publicly opened, even the number of such murders was hidden from the public.        The project has resonated both in Kyrgyzstan and other Central Asian countries. It had received solid media coverage, it was presented in a number of round tables on domestic violence and it was highlighted by the UNWomen Kyrgyzstan, Soros Foundation and a number of feminist organizations.        We have disseminated our findings among several members of parliament and governmental bodies, however, as the project was published in late December 2020, we have not been able to receive any reaction yet.        We have published our research findings as a separate study (in Russian and Kyrgyz languages), and as a longread in online media outlet kloop.kg (in three languages: Russian, Kyrgyz and English.   Please note, that the original publication is dated December 17, 2020. Only the English translation was published later on January 28, 2021. For English translation please see ""Project link 2"" ). The article took 47 place by the number of unique viewers, and the average time spent on the article was almost 6 minutes. This is almost six times longer than the average time for other articles. The facebook coverage was 38,000 users, which is quite unique for our media outlet.     ","   In total we spent 6 months on the project.         We first developed the methodology, based on literature review and existing research on femicide in Georgia, Spain, Latin America etc. Generally femicide is the murder of a woman, usually committed by a man on the basis of misogyny, gender discrimination, and/or as a result of gender-based violence in which the state is complicit. So to properly define the term “femicide” we have come up with at least 12 criteria, by which a woman’s murder can be considered as femicide. For example, the “classic” femicide is murder of a woman by her husband because she was disobeying. Other criteria include murders because she refuses to consent, she was murdered with particular cruelty, she was socially-vulnerable etc.        To understand the femicide patterns we analysed news. We have scraped more than 54,000 of news articles from the largest news agency in Kyrgyzstan. These news also contained press-releases of the law enforcement bodies.        To narrow down our analysis we have identified around 80 words and sentences in accordance with the previously developed femicide criteria. This let us identify news and press releases that covered the women’s murders.        We then analyse more than 3,000 of news in details to identify femicide. As a result we were able to surely define at least 300 women’s murders as femicide during the past ten years. We have created a detailed database, containing the description of each case.        We have calculated the coefficient of femicide (number of women's murders per 100,000 of women) to provide international comparisons. We have also studied criminal statistics to find the “hidden” femicide and to understand the dynamics of women's murders.        Techniques: Python, Scraping, Analysis in Python and Google spreadsheets, data visualization in Flourish.studio, storytelling in Infogram.  "," One of the main difficulty was to analyse each murder to comply with the femicide criteria. The database was created from scratch, we had to constantly update it every time a new detail of the murder was found.    The other difficulty was that any official statistics on women’s murders is not publicly available. We were struggling with bureaucracy to send inquiries to the Ministry of the Internal Affairs, The General Prosecutor’s Office etc. and to get replies from them. The Ministry of the Internal Affairs does not want to connect domestic violence to femicide and they have completely denied our interview requests and ignored us.    It was also quite difficult to research the topic in general, because there are no experts in the country on femicide. We had to extensively search through the literature, find international experts to make sure we understand the topic and can explain it to our audience in a simple and intelligible way. The Kyrgyz society is a conservative and has a traditionalist culture, therefore anything that contains the prefix “fem” is immediately rejected.     However, we believe that we had overcome this and did our job very well.  "," We believe that our methodology is well-developed and journalists are welcome to use it for monitoring femicide in their countries. The methodology can be used by any beginner data-journalist.    We also encourage journalists to use our femicide database for writing their own stories. The database we created, contains full description of the location and date of the murder, the age of both victim and murderer, their initials, the type of a femicide, relation of the victim to the murderer, injures, type of punishment etc.    Despite the wide prevalence of femicide in the world, not every country recognizes it as a separate problem. Legislation in Central Asain countries, for example, does not distinguish murders of males and females. We however find it especially important that femicide should be introduced in legislation as a separate term, because the causes of women’s killings are completely different and are based on existing gender stereotypes and discrimination. We hope that our project can help journalists and other stakeholders to start systematically researching femicide and introduce it widely in their narratives.     We believe that telling about the problem is one of the ways to solve it.  ",https://kloop.kg/blog/2020/12/17/ya-by-ee-vse-ravno-ubil-issledovanie-kloopa-o-femitside-v-kyrgyzstane/,https://kloop.kg/blog/2021/01/28/femicide-in-kyrgyzstan/,https://ky.kloop.asia/2020/12/17/kyrgyzstandagy-femitsid/,bit.ly/femicidKG_data,http://bit.ly/femicideKG_fullresearch,,,,"Savia Hasanova, Anna Kapushenko, Alina Pechenkina, Edil Baiyzbekov, Almir Almambetov, Kairat Zamirbekov, Aziza Raimberdieva"," Anna Kapushenko is an editor in chief at the Kloop.kg. Anna has been working in journalism since 2011. Anna started her career as a correspondent at the Institute for Public Policy.  In 2013, she became the editor of the analytical portal at the Institute. Anna started working at Kloop in 2015 as a correspondent. She became editor a year later, and in 2020 she headed the media.   Savia Hasanova is a researcher, data analyst and data journalism trainer. During the past several years, she has been advocating for open data and data literacy in Kyrgyzstan. She worked closely with journalists, conducting training on data analysis and data storytelling in Central Asia and Mongolia. She is an author of a multiple number of DDJ articles in local and international media. ",17 Dec 2020
Finland,Helsingin Sanomat HS.fi,Big,We know who you are dating,"Investigation, Explainer, Multiple-newsroom collaboration, Illustration, Infographics, Lifestyle",Microsoft Excel," The phone’s dating apps know almost everything about their user: where you are, who you’re communicating with, what music you’re listening to, and even what you’re aroused about. We, together with an American news channel NBC News, went through the most popular dating apps, compared which data each of them collects, looked where the data ends up and what that data can cause in the wrong hands. When you are looking for love, the data privacy risks are not what you are thinking when agreeing on the terms of use.  "," It was an eye-opener for many. Dating apps, and all the tips you find online concerning to use of dating apps, tell you that more info you share about yourself, more accurate partner you can find. The story shows how security risks increase, the more data you share, and also how your data is being sold.  The terms of use can be 19 pages long. The story explains, what does those 19 pages actually mean.  "," To be able to see which data each dating app collects, we had to make an account for each dating app. Then we started to collect info from the apps to an excel. We also did a non-scientific “stalking test”: we tried to find out, how easy it is to identify people based on the dating app account. For this test we took the first three matches from each app we used. Then we did what any curious person would do: we googled them. We used Google image search and traditional Google search. 8 out of 9 of our matches we were able to identify with this methodology, even the dating app account often only shows the first name and pictures.  "," Tricky part was to make the dating app profiles, because there are restrictions, which kind of fake id’s you can use as a journalist. Our accounts had to be “human” enough, but not misleading too much. We wanted matches, but not to get involved.    But the really hard part was to read all the data privacy policies and terms of use and to really understand what they mean. And into which apps the dating apps are connected with and how all these apps share information.   ", To collect data can sometimes be really hands-on.  ,https://dynamic.hs.fi/2020/deittisovellukset/,,,,,,,,"Pauliina Siniauer, Timo Myllymäki, Olli Pietiläinen, Anni Kössi, Petri Salmén, Minna-Liisa Linjala, Merituuli Saikkonen, Uolevi Holmberg, Andrew Lehren (NBC), Connor Ferguson (NBC)"," Pauliina Siniauer, text    Timo Myllymäki, code and visualization    Olli Pietiläinen, data gathering    Anni Kössi, photography     Petri Salmén & Minna-Liisa Linjala, infographics    Merituuli Saikkonen, editing    Uolevi Holmberg, video editing    Pauliina Siniauer & Andrew Lehren (NBC) & Connor Ferguson (NBC),  producing  ",24 Jan 2020
United Kingdom,The Times and The Sunday Times,Big,How safe is it for you to cycle to work?,"Solutions journalism, Open data, News application, Chart, Map","Personalisation, Json, R, RStudio, OpenStreetMap","  The Times and The Sunday Times  has championed cycling safety for years, ever since it launched the “cities fit for cycling” campaign in 2012. In our latest contribution we produced an interactive series which allows readers to compare the safety of their own cycle journeys with the national average as well as with possible alternative routes.   Readers can input a starting position and a destination and calculate how many traffic crashes have been recorded by police en route. The tool is informed by data from the Stats19 database, a record of every vehicle crash kept by the Department of Transport. "," The series ran at the top of the website from midday to midnight on August 28. It reached over 70,000 people over the next few days and scored well against internal engagement metrics.        The wealth of data underpinning the tool led to multiple news stories and a joint Times and Sunday Times series investigating cycling safety.   We combined the national accidents database with census data to determine the most popular start and end points for commuting by bike. The research showed that the overwhelming majority of crashes happened at junctions.   It demonstrated the scale of the challenge facing the government as it promises a “cycling and walking revolution”, with campaigners telling the Times that the findings showed ministers needed to spend more to deliver on their pledge to make travelling by bike safer.   We spoke to Victoria Lebrec who was on her way to work in London when she was crushed beneath a lorry that failed to spot her at a junction. She lost a leg in the accident and told the Times she wanted to see fully segregated cycle lanes to protect riders. A year after Ms Lebrec’s accident a 26-year-old woman was killed at the nearby Bank junction by a left-turning lorry.   The analysis also revealed that Oxford and Cambridge, two of the country’s most notoriously cycle-friendly cities, were home to the most high-risk stretches of road for commuters.   We also used the database to analyse more than 36,0000 weekend cycling accidents over the past decade, revealing that weekend cyclists racing against the clock could be pedalling into greater danger than the average commuter. "," The data was initially cleaned, reformatted and analysed using R, a statistical programming language.    We wanted the interactive to be constructive as well as informative so we used the Mapbox API to return three journeys. Each route was ranked based on the number of fatal, severe and slight crashes that had happened the past three years, with the journey time and crashes per 1,000 miles providing additional detail. How the options compare to the 2,000 most popular routes in Britain is displayed on a violin plot to provide a national comparison.   The tool uses data from the Stats19 database, a comprehensive record of every vehicle crash kept by the Department of Transport. This was cleaned, reformatted and analysed using R, a statistical programming language.   This analysis is done on-the-fly in the reader’s browser using a javascript library called Turf.JS. It generates a buffer area around the route and uses point-in-polygon analysis to match geocoded incidents in the data. These are stored in a MySQL API.     The dashboard was built in vanilla javascript to make the time between searching and results displaying as quick as possible. "," Our main technical challenge was keeping loading times down when analysing data in the browser. We were able to create a workable solution by putting the data behind an API, going without D3 to keep visualisations simple, and taking time to think about the user experience of waiting for results to return. "," We wanted to build a consumer tool which would provide useful information a cyclist could use whenever planning a journey. It would need to be something they could return to and reuse. With this in mind, we made sure to return three possible routes so that users could make an informed choice.   Our community does not shy away from the topic of cyclists and their status on the road; it is one they engage with wholeheartedly. Our goal was to encourage their discussion and inform a constructive debate.    By presenting the information as simply as possible, providing useful context, and offering more than one answer to the question we had posed, our tool was extensive enough so that it did not stand in the way of debate by being too limited or offering simplified answers which could jar with our readers experiences. ",https://www.thetimes.co.uk/article/how-safe-is-it-for-you-to-cycle-to-work-these-maps-will-show-you-xnvtwq6wz,https://www.thetimes.co.uk/article/cambridge-and-oxford-top-list-of-britains-most-dangerous-roads-for-cyclists-7mpchxn0h,https://www.thetimes.co.uk/article/city-cyclists-safer-than-weekend-warriors-wtckdg9dt,,,,,,"Ryan Watts, Sam Joiner, Graeme Paton, Tom Calver, Daniel Clark, Anthony Cappaert, Tony Allen-Mills"," Ryan Watts, Daniel Clark and Tom Calver are Data and Interactive Journalists at The Times and The Sunday Times. Sam Joiner is the head of Data and Digital Storytelling and Anthony Cappeart is Digital Newsroom Design Editor.   Grame Paton is the Transport Correspondent at The Times. Tony Alln-Mills is the Senior Writer at The Sunday Times. ",28 Aug 2020
Philippines,Rappler,Small,Fact-checking: A year of infodemic,"Database, Open data, Fact-checking, Chart, Health","Microsoft Excel, Google Sheets, CSV","   Along with the spread of the coronavirus pandemic in 2020 came the spread of disinformation and misinformation on social media platforms in what is dubbed as an ""infodemic."" To combat this problem, hundreds of media and independent organizations banded together to fact-check these false, misleading, and dangerous social media posts over the course of the year.       This story summarized the frequency of the false posts, the themes and favorite topics of such falsehoods, and the number of fact-check articles published worldwide to correct these posts.  ","   The project collated a year’s worth of false information about COVID-19 not only in the Philippines but globally. It visualized how both disinformation and misinformation about the virus grew, and that this so-called “infodemic” is dangerous, too, like the disease itself.  ","   We analyzed the rows in the central database of fact checks via Google Spreadsheets, and displayed the summaries through graphs made in Flourish and Datawrapper.  ","   The hardest part was going through over 9,000 entries of fact checks from hundreds of organizations around the world. Although the International Fact-Checking Network (IFCN) provides a database, Rappler had to clean the raw data from it and process it to more effectively show the gravitas of the infodemic, particularly in the Philippines.  ","   This project shows that cooperation is important, especially in fighting disinformation. This story would not exist without the IFCN, which made it easier for fact-checkers to verify information with each other as falsehoods spread in different areas in the world.  ",https://www.rappler.com/newsbreak/iq/fact-checking-year-infodemic-2020,,,,,,,,Pauline Macaraeg,"   Pauline Macaraeg is part of the Rappler Research Team’s fact-checking unit. Aside from debunking dubious claims, she also enjoys crunching data and writing stories about the economy, environment, and media democracy.  ",23 Dec 2020
Chile,La Tercera,Big,Del 1 al 10 mil: Cómo Santiago se transformó en una de las ciudades con más muertos por covid-19 (From 1 to 10 thousand: How Santiago became one of the cities with most deaths by covid-19),"Explainer, Open data, Infographics, Map, Health","Scraping, Canvas, Json, Google Sheets, CSV, PostGIS, Node.js"," Considering confirmed and suspected deaths, in September the capital of Chile was reaching 10.000 deceases, placing Santiago as one of the cities with the most deaths from the pandemic in the world, exceeding a thousand deaths per million inhabitants.  We decided to address this with a visualization week by week. As the users scroll down, they see a map of Santiago filling with red and blue points, each for every death, explaining also the most important events of that week. In the end, the map is almost no visible for so many points, which is very shocking. "," We had 20,000 unique visitors in the next weeks after publishing our interactive. It was very commented on social media, highlighting “the impact” of seeing illustrated all the people who died and setting a record about passing the 10 thousand deaths (a number not everyone was aware of). “It distresses you a bit, but it's a very good way to graph it”, someone said on Twitter. We published it during the weekend of our national holiday, stressing the fact of how important was that we all took care of ourselves and we hope it helped at least a bit.  Our main goal with this project was to show a really big number, a data hard to visualize for people, in a way they could “see it”.  How to show, in more than a number, all the persons who died in just around seven months. In that sense, the week by week advance of the scrolling worked quite well, seeing every week more and more points in the maps. We also keep “fixed” in the upper side of the web a score of the number of people dying, which was also increasing every week. We believe this system helped readers to really understand the severity of the casualties. "," We used GatsbyJS, P5.js, ReactJS, GraphQL, WordPress (headless).   We first did the canvas part with P5.js drawing the Great Santiago area with segmented custom topojson files with the “comunas” delimiter and their correspondent centroid. That allows us to group little spots around that point programmatically.   Then we took the scraped data ordered by “comuna” (comune), date and other statistical data into a CSV file (gender, COVID-confirmed, or assumed by close contact), and we load it next to our P5.js drawing. Then we took our P5.js static development into a ReactJS component, sorting the passing days data with additional data fetched by headless WordPress custom meta consumed by Gatsby’s graphql.  With a scroll-aware listener, we were able to sync weeks with week-to-week data through a React useState, painting the corresponding real numbers into our P5 canvas, giving the actual scale of the death toll to our user according to the scroll pass by. "," The hardest part of the project was to decide the graphic layout to show the data. We draw and iterate different options for showing the deaths in the maps and the increase during the time (for example, to make circles of points for each commune or to show the point in a more distributed way. We also tried a lot of ways to build a “dialogue” between the text and the graphics. In the end, we choose what we thought caused more impact on the user.  Another hard aspect was the scrolling and make it work as we wanted, with smooth transitions between weeks.  We worked with a freelance data scientist and a designer, and the rest of the work was developed for 3 editors and our front developer. It took us more time than expected because we almost did it in our “spare” time (we regret nothing). I think our bosses didn’t understand and value what we did until they saw it, which confirms our initial assumption, that this was very difficult to realize how harsh the data were until you see it. "," What we most enjoy about working on projects like this is working in an interdisciplinary team in a very horizontal way. We learn so much from the data scientist comments and suggestions, the same from the designer and of course, our front developer who's always looking for better ways to improve the user experience.  This is the kind of project where journalists can expand their knowledge about visual journalism, which is more important every day. Because one thing is the data, and journalists can be very good at gather, process and write about it, but sometimes, especially in these times, that is not enough.  ",https://interactivo.latercera.com/muertos-covid-santiago/muertos-covid-santiago-data/,,,,,,,,"Álex, Acuña. Valentina, Danker. Ilana, Levin. Tania, Opazo. Ignacio, Pérez. Sebastián, Rivas.",   Tania Opazo:  Interactive and Multimedia Editor at La Tercera. Coordinator and editor of the project.    Sebastián Rivas:  Audiences Editor in Chief at La Tercera.Coordinator and editor of the project.    Valentina Danker:  Engagement Editor at La Tercera. Reporter and writer of the project.    Ignacio Pérez Messina:  Freelance Data Scientist.    Ilana Levin:  Freelance Designer.    Álex Acuña Viera:  Frontend Developer and Designer at La Tercera. ,11 Sep 2020
United States,The Markup,Small,Blacklight,"Investigation, News application, Fact-checking, OSINT, Business","Scraping, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV, Node.js"," Blacklight is a work of experiential journalism that allows people to investigate the state of web privacy in real time and on their own terms. Blacklight instantly reveals the potential privacy violations on any website—and names the companies tracking you.    The tool was used for a companion investigative story, which examined the role of free website building tools in inserting trackers on small, unsuspecting websites, including those that serve vulnerable populations. "," Nearly one million people had used Blacklight to scan websites by early January, just a few months after we’d released it. Some have reached out to us to say they used the results to pressure the organizations they work for or websites they frequent to remove tracking technologies. At its height, when it was featured on the front page of Reddit, Blacklight was conducting more than 300 user-initiated scans every minute.    Many website operators contacted for the “The High Privacy Cost of a ‘Free’ Website” investigation removed user-tracking technology from their sites after we brought it to their attention—including several government webpages. Other website operators tweeted about doing it on their own after reading our story and scanning their site.    One smaller search engine even incorporated Blacklight, the code of which we published open-source, into its own product, allowing users to scan a featured site on the search results page before visiting. Some people are using Blacklight to scan the sites they rely on—and calling them out on Twitter for their ad-based tracking.   Reporters from Forbes, The Logic, and Vox used Blacklight to scan their employers’ sites and wrote about the results and how user tracking is employed in advertising-funded news operations. And a computational journalism class at Stanford has incorporated Blacklight into the syllabus.   Congresswoman Anna Eshoo, who represents Silicon Valley, wrote a letter to The Markup saying, ""While companies that profit from surveillance capitalism may be upset by your decision, I stand in full support of this tool.""    "," Blacklight was written in Node.js and relies on AWS’s Lambda, S3, and Cloudfront services. Using this combination of tools allowed us to build a real-time website privacy inspector that was very precise—the Node.js puppeteer module gave us full control over a browser to run our tests—and is also easy to scale to user demand. Blacklight has never gone down, even when it was receiving more than 300 requests a minute.     The privacy tests the tool runs were designed using existing research to study the techniques used by tracking scripts, then programmatically scraping thousands of websites to find instances of these techniques. This approach ensured that we were testing for things seen in the real world, not just in academic papers.     In addition, we tried to account for how Blacklight might be abused by malicious actors. For this reason we cache results for 24-48 hours on S3 so we’re not hitting the same website more than once a day.      The analysis and raw data on which the results are based can be downloaded from the tool, so that it can be used by journalists, researchers and others.    "," We faced several technical challenges. This tool required a lot of development for both the data collection and the analysis.    First, we had to determine how to measure the various potential privacy invasions of a website and also explain them in ways that were both precise and easy for a non-technical audience to understand. Second, we had to do a lot of testing to ensure we were getting accurate results for a large variety of websites and tracking vendors. We ended up collecting data for more than two million websites.    Blacklight carries out sophisticated tests, but it’s open-source code was written to be accessible to both professional and beginner programmers. We wanted to ensure technically literate folks could use the tool for their own purposes.   We also faced the challenge of explaining the privacy violations—and the role various companies played—in ways that were both accurate and understandable to a general audience.    "," Blacklight was a tool made, first and foremost, by journalists for journalists. We developed it to support our investigation “The High Privacy Cost of a ‘Free’ Website.” Since launch, a number of newsrooms have already used it for their own stories, including Forbes and Vox. A computational journalism class at Stanford University is launching a project this semester using Blacklight.   When it comes to online tracking, we talk a lot about the data and not enough about the companies that are deploying invasive practices to collect that data. Our tool shines a light on which companies are trying to get your data from a given website.    Blacklight was also built with accountability in mind; it allows readers to download a copy of the inspection report, along with all the data used to generate it. This data is used to make descriptive claims about the kinds of privacy violations we found on websites. Given the dynamic nature of the internet, it can be hard to make such claims with confidence. The inspection archive makes that possible by saving a snapshot of what was found.    ",https://themarkup.org/blacklight,https://themarkup.org/blacklight/2020/09/22/blacklight-tracking-advertisers-digital-privacy-sensitive-websites,https://themarkup.org/blacklight/2020/09/22/how-we-built-a-real-time-privacy-inspector,https://themarkup.org/blacklight/2020/09/22/what-they-know-now,,,,,"Surya Mattu, Sam Morris, Simon Fondrie-Teitler, Aaron Sankin, Evelyn Larrubia, Jill Jaroff, Yotam Mann, Chris Deaner"," An engineer by training, Surya Mattu builds tools and gathers data to tell stories about how algorithmic systems perpetuate systemic biases and inequalities in society.   Before The Markup, he worked on Gizmodo’s Special Projects Desk and ProPublica, where he was part of the team that was a finalist for a Pulitzer Prize for the series “Machine Bias.”    Aaron Sankin reports on how technology can be used to harm marginalized people. He focuses on platform governance, online extremism, and regulatory policy.   Before The Markup, he covered online extremism for the Center for Investigative Reporting. He has received a Public Radio News Directors Award and a Webby Award, among others. ",22 Sep 2020
Brazil,Agência Pública (apublica.org),Small,"Under the Bolsonaro Administration, Farms were Irregularly Certified in Indigenous Lands of the Amazon Region","Investigation, Database, Open data, Infographics, Map, Satellite images, Politics, Environment, Health, Human rights","QGIS, Adobe Creative Suite, Microsoft Excel, Google Sheets, CSV","   An exclusive investigation shows that 114 properties have been certified inside indigenous territories awaiting demarcation in the Brazilian Amazon, spurred in large part by a recent statute approved by the national agency for indigenous affairs that leaves these unratified lands unprotected from such illegal land grabs. Landowners have already registered claims for more than 2,000 private properties in indigenous areas, including some that are home to isolated peoples. Indigenous groups, civil society organizations, the Federal Public Prosecutor’s Office and state prosecutors have denounced the statute and are challenging it in various courts.  ","   This journalistic investigation shone light to the consequences of a new policy that facilitated the escalating process of invasion of indigenous lands in the Amazon - which has become an even bigger problem with the Covid-19 pandemic, putting the lives of the indigenous populations in the Amazon at greater risk. It has therefore been referred to in several publications, nationally and internationally - from national organizations such as <a href=""https://www.socioambiental.org/pt-br/blog/blog-do-monitoramento/com-manutencao-de-missoes-religiosas-indios-isolados-estao-em-risco"">Instituto Socioambiental  and the NGO <a href=""https://terradedireitos.org.br/noticias/noticias/que-boiada-o-governo-ja-deixou-passar-na-pandemia/23350"">Terra de Direitos  to international outlets such as <a href=""https://www.jornaltornado.pt/queimadas-e-desmatamento-crescem-na-amazonia-brasileira-em-meio-a-pandemia/"">Jornal Tornado  (Portugal), the german magazine <a href=""https://amerika21.de/2020/06/240864/brasilien-gericht-indigene-funai"">Amerika21  and the public german radio Bayern 2, which interviewed one of the reporters for a piece on ""<a href=""https://www.br.de/radio/bayern2/sendungen/zuendfunk/wie-bolsonaros-corona-politik-die-indigenen-brasiliens-dahinrafft-100.html"">How Bolsonaro's corona policy threatens the lives of indigenous people in Brazil "". This investigative piece has been republished by 20 news websites, in portuguese, spanish and english, being featured in important national and international outlets, such as <a href=""https://news.mongabay.com/2020/06/illegal-farms-on-indigenous-lands-get-whitewashed-under-bolsonaro-administration/"">Mongabay  (US) and <a href=""https://interferencia.cl/articulos/con-bolsonaro-las-fincas-se-certificaron-irregularmente-en-las-tierras-indigenas-de-la"">Interferencia  (Chile). Later in 2020 it was awarded the first place of the <a href=""https://alleyesontheamazon.org/aea-announces-winners-of-the-data-journalism-contest-all-eyes-on-the-amazon/"">Data Journalism Contest ""All Eyes on the Amazon"" , promoted by a coalition of organisations which includes Hivos and Greenpeace.  ","   This project examined three public databases: FUNAI's Indigenous Lands Mapping; private properties registered in the national Rural Environmental Registry system [Cadastro Ambiental Rural, CAR]; and private properties certified by Sigef, the national Land Management System. For the purpose of this data-crossing, we took into account only the unratified indigenous lands, seeing that the new statute issued by the national agency for indigenous affairs authorized the certification of private properties inside these lands. From the CAR database, we considered only the rural properties located in municipalities where indigenous territories exist. From the database of private properties registered in the Sigef, we only took into account those properties which were certified and authorized. The results showed where the selected areas overlapped with indigenous territories. The entire survey took into consideration the eight states of the Legal Amazon region, plus part of the northeast state of Maranhão. We used free data editing programs such as Libreoffice, QGIS and Open Refine and design programs for data visualization such as Illustrator and Photoshop.  ","   The CAR database hasn't been consolidated all over Brazil yet, so we needed to collect the databases from each municipality. In addition, the CAR and Sigef databases have a number of subdivisions, which are not very clear - and understanding them became even more complex due to the lack of assistance from the public agencies responsible for the data. The databases also lacked pieces of information, especially about the people in charge of the properties.  ","   This report provides a clear way to pressure and monitor public authorities actions that directly impact the lives of indigenous populations. It also provides data and information for indigenous organizations - which can help to guide their strategies and actions - and for local journalists who wish to explore the subject further based on the national context. This piece sets an example on how to independently report on a topic, without counting on official bodies, which have offered little or no assistance.  ",https://apublica.org/2020/05/com-bolsonaro-fazendas-foram-certificadas-de-maneira-irregular-em-terras-indigenas-na-amazonia/,https://latin-america.hivos.org/blog/in-the-bolsonaro-administration-farms-were-irregularly-certified-in-indigenous-lands-of-the-amazon-region/,,,,,,,"Bruno Fonseca, Rafael Oliveira","   Founded in 2011 by women reporters, Agência Pública is the first non-profit investigative news agency in Brazil. All of our reporting is grounded in rigorous fact-finding investigations and our uncompromising defence of human rights is at the core of everything we do. We investigate the public administration, including all levels of government and congressional houses; the social and environmental impact of corporations and corrupt and anti-transparent practices; the efficacy, transparency, and equity of the justice system; and the violence against vulnerable communities in urban and rural areas.  ",19 May 2020
Spain,elDiario.es,Big,"Tracking the excess deaths in Spain, day by day","Investigation, Explainer, Breaking news, Chart, Health","D3.js, JQuery, Microsoft Excel, CSV, R, RStudio"," In the last 100 years more than 12,000 people died in Spain in a week until the coronavirus crisis. An investigation by elDiario.es with individual data of more than 15 million registered deaths from 1975 showed that Covid-19 led Spain to the worst mortality crisis in its democratic history. A mortality figure that far exceeds the official deaths by coronavirus published by the Government. This publication is being updated every week.  "," This project compiled for the first time the historical mortality figures from civil registries for the last 45 years. At the worst moment of the crisis, while some public figures still spreading the idea that Covid-19 was a simple flu, this story showed that Spain was registering more deaths than ever during the democratic history of Spain. Measuring the excess mortality shown to be one of the most accurate ways to show real effect of the pandemic while there where thousands of citizend dying of Covid-19 without a positive PCR test. In this information, our readers wree able to see for the first time the impact of the Covid-19 crisis on mortality in their region compared to the last 45 years. The information was updated week after week (and continues to be updated) throughout the epidemic.    This project was one of the most read of the year in elDiario.es and also one of the one that made most people suscribe and pay for elDiario.es during the year, according to our statistics.  "," R, Rstudio and Excel for data compiling and data analyisis. Datawrapper, D3.js, Javascript and Flourish for data visualization. "," The hardest part of the project was combining the historical mortality databases of the National Institute of Statistics and the most recent mortality figures published by the civil registries. For example, it was necessary to combine the individual microdata of more than 15 million deaths registered between 1975 and 2019, which were incorporated at each moment with a different structure. Later, these figures were combined with data from MoMo, the early warning system for excess mortality, which includes 93% of the Spanish territory with the digitized system, and both databases had to be harmonized to make them as comparable as possible. In addition, most of the processes had to be automated to allow them to be updated quickly one or several times each week.    "," During the pandemic data journalist faced a complication, even most of the countries were actively publishing of registered deaths of coronavirus, all of this data showed subregister. Goverments were only incluiding deaths by coronavirus as the ones of people that had positive test for coronavirus, these means that people that died without hospital attention were not being registered or if they died without a PCR test.    This publication show a metholodogy on how data journalist can use historical death registers to really calculate how many people in dying during the pandemic. All the process and automatization can be replicable once journalist access to public data on deaths of the countries or regions.      ",https://www.eldiario.es/sociedad/muertes-esperadas-septiembre-evoluciona-peor-crisis-mortalidad-inicio-democracia-exceso-momo-26-enero_1_6946073.html,https://twitter.com/raulsanchezglez/status/1257224034821066752?s=20,,,,,,,Raúl Sánchez," Raúl Sánchez:  Spanish data and investigative journalist covering stories of inequality, gender, corruption and (now) Covid at elDiario.es. He coordinates elDiario.es data team. ",31 Mar 2020
United States,The Markup,Small,Google The Giant,"Investigation, Long-form, Open data, Infographics, Politics, Business","Animation, Personalisation, Scraping, Json, CSV, Python"," For this groundbreaking investigation into Google’s flagship product, Google Search, we used novel computational techniques to expose how the company routinely boosts its own products, pushing down “organic” search results leading to other websites.    "," The findings of this investigation and a follow-up story informed debate in Washington and a congressional antitrust committee’s calls for wide-ranging regulation of the tech giant and its brethren.      The facts we uncovered were cited in all three antitrust lawsuits filed against Google by the Department of Justice and state attorneys general last year. The Department of Justice’s historic antitrust lawsuit alleged that Google ""has pushed the organic links further and further down the results page” and referenced training documents we obtained with specific instructions to Google employees to avoid using phrases like “market share” and “dominant.”      Another suit, filed by 10 states led by the attorney general of Texas, referred to Google's ""walled garden,"" while another lawsuit, filed by 38 states led by the attorneys general from Colorado, Nebraska, Iowa, and Tennessee, emphasized how Google redirects search traffic to itself.      Our findings were also cited as proof that Google had built a “walled garden,” during the questioning of Google CEO Sundar Pichai at a congressional antitrust hearing with the nation’s four tech giants. The final report from the House Judiciary antitrust subcommittee’s year-long investigation into big tech also mentioned our work as evidence of Google’s monopolistic behavior.     "," We scraped Google Trends to source the top search queries for all available categories (business, entertainment, science and technology, sports, and top stories) every six hours for two months. To do this, we had to reverse engineer Google’s client-side API by listening to network requests in the browser’s “Dev Tools,” copying the cURL request, and retrofitting it to operate as a standalone API.     We then used these search queries to perform Google searches on a mobile emulator we created using Selenium. Like the search queries, we maintained this continuous data collection for two months.     We processed the data using BeautifulSoup, Pandas, and Selenium (more on this step in the next question).     We drafted static figures, using Matplotlib, that were ultimately made in JavaScript.      Due to the experimental nature of our project, we had rigorous error checking and validation. We used the drawing library p5.js to automatically annotate screenshots with bounding boxes indicating the space occupied by Google elements and our other categories (more on this categorization scheme in the next question). We used the annotation software Prodigy to record the precision and accuracy of bounding boxes determined by our parsers.    "," Developing parsers for Google Search required intimate domain knowledge and innovative technology.     There is no existing taxonomy of the enormous variety of results delivered in response to a Google search. We created a classification system that was robust and general enough to apply to all mobile Google search results. This process required four months of research, interviews, and sifting through troves of source code.      We then had to encode this knowledge into automated web parsers. It took months longer to build a total of 68 unique parsers spanning more than 1,000 lines of code to identify elements in our five-category classification system: Google answers, Google products, non-Google, Ads, and AMP.     Much of this process overlapped with our time spent developing our classification system, for a total of more than six months for both.     Still we weren’t done. We had a unique need for our story: to measure the placement and prominence of each of these categories of search results. To quantify this unique spatial data, we created a novel web parsing technique—inspired by the biology lab assay—that “stains” the area occupied by elements in each of our five categories.      We achieved this by leveraging the xpath of elements categorized in our parsers and re-rendering the parsed search pages in the Selenium mobile emulator. This yielded spatial metadata (coordinates and dimensions) for categorized web elements, which allowed us to quantify Google’s self-preferential treatment of its own properties.    "," Our project highlights three important lessons for other journalists:       Build your own datasets       Be mindful about what you’re counting       Be honest about your limitations       Accountability journalism depends on evidence. But when you write about private companies, as we do, information is particularly difficult to obtain. For instance, Google has no incentive to quantify how much space it is devoting to its own products on the search results page. And most independent analysis we had found was sparse or anecdotal data from search engine optimization consultants.    We firmly believe that building your own dataset is essential to service journalism. We hope that our multi-step data collection processes will inspire other newsrooms to devote the time, resources, and leadership support to build their own datasets.    However, even a made-to-order dataset is not necessarily story-ready. The most basic unit of what to count might not be clearly stated as a column in the dataset. Instead, we need to do what all journalists do: interviews and research. This applies to not only talking to people, but also talking to data so that you fully understand the elements and the architecture.    We were able to build our classification system through acquiring both intimate domain knowledge and an understanding of the structure of the search results page. This is how we were able to accurately count something that has never before been quantified.   Lastly, we hope that our Limitations section encourages other journalists to prioritize accuracy and honest conversations over big numbers and shocking statistics. We think it is of the utmost importance to disclose the shortcomings of projects and be precise about the claims that can be inferred from our findings. In this project, we achieved this in part by seeking feedback from computer scientists, statisticians, and industry professionals regarding the methods we were using. ",https://themarkup.org/google-the-giant/2020/07/28/google-search-results-prioritize-google-products-over-competitors,https://themarkup.org/google-the-giant/2020/07/28/how-we-analyzed-google-search-results-web-assay-parsing-tool,https://themarkup.org/google-the-giant/2020/07/29/congressman-says-the-markup-investigation-proves-google-has-created-a-walled-garden,https://themarkup.org/google-the-giant/2020/08/07/google-documents-show-taboo-words-antitrust,https://themarkup.org/google-the-giant/2020/10/15/big-tech-antitrust-google-nondiscrimination-enforcement,https://themarkup.org/google-the-giant/2020/10/20/google-antitrust-lawsuit-markup-investigations,,,"Leon Yin, Adrianne Jeffries, Sam Morris, Evelyn Larrubia"," Adrianne Jeffries writes stories examining the power platforms exert and exploring the consequences of automating decisions. She started writing about tech a decade ago as a reporter for what was then called ReadWriteWeb, and has worked at The Verge, the Motherboard, and The Outline.    Leon Yin creates datasets and methodologies to shed light on the interactions between technology and society. Before joining The Markup, he was a research scientist at NYU’s Social Media and Political Participation lab, an affiliate at the Data & Society Research Institute, and a software engineer at NASA. ",28 Jul 2020
United States,NBC News,Big,The deadly secret of China's invisible armada,"Investigation, Long-form, Cross-border, Documentary, Database, Open data, Infographics, Video, Map, Satellite images, Environment, Economy","Animation, QGIS, Json, Adobe Creative Suite, CSV, Python"," For years, the Sea of Japan has held a grisly mystery: the bodies of hundreds of skeletal North Korean squid fishermen have washed ashore in Japan, their boats battered and drifting for months. Japanese police chalked it up to climate change and declining squid populations, but NBC News figured out the true reason, revealing through text and interactive graphics something extremely significant about the balance of power in the waters of the region. "," Using new satellite data, confirmed by his own visit to the region, investigative reporter Ian Urbina discovered that China was sending a previously invisible armada of industrial boats to illegally fish in North Korean waters. The Chinese boats violently displaced the much smaller and more decrepit North Korean boats, and collected so much squid that fishing stocks declined more than 70 percent.   Working with a group of data scientists and academic researchers, Urbina was the first journalist to describe what one expert called “the largest known case of illegal fishing perpetrated by a single industrial fleet operating in another nation’s waters.” The fishing fleet is also in violation of U.N. sanctions that prohibit foreign fishing in North Korean waters.   Urbina’s work documented China’s willingness to use its maritime muscle in the region with impunity, noting that China has the world’s worst score when it comes to illegal, unreported and unregulated fishing. The country’s fishing boats are famously aggressive, often armed and known for ramming competitors or foreign patrol vessels.   But his story was not just about geopolitics; it had an important human side. So many North Koreans have disappeared at sea in recent years because of the Chinese fleets that some North Korean port towns are now called “widows’ villages.” The economies of even South Korean ports have been damaged by the collapse of the squid harvests, thanks to the huge Chinese mechanized boats. "," To make this complex story more accessible to readers, NBC News prepared a series of remarkable graphics to accompany it. One animated map created by national interactive journalist Jiachuan Wu, used satellite technology to show the path of Chinese vessels as they left port, sailed around the Korean peninsula, and moved into North Korean waters. Another focused on two boats in particular, documenting with precise timing when they left China, when they illegally turned off their transponders, and when they moved into the North’s waters, matched with photographic evidence of their presence.   The data analysis was done using a combination of Python and Qgis. We cleaned the coordinates data with Python and looked for the narratives through all signal points after mapping them out in Qgis. The entire 2018 Automatic Identification System(AIS) routes data is a pretty large dataset with daily entries, which is not ideal for a user’s loading experience. To improve this front-end performance, animations in the story were created using Qgis and later produced to timelapse animation. We exported for desktop and mobile users with a reasonable and legible file size. To show the reader that Chinese lighting vessels are significantly brighter than those of North Korea, we looked for a real satellite image and later compared it with a brightness data map with lighting vessels highlighted. The side-by-side real image and data map shows clearly that at same time, brighter Chinese vessels were actively fishing illegally in the NK water zone.   Urbina also collaborated with NBC video journalist Marshall Crook on a video that showed the huge fleets and the massive lights they use to attract squid, in comparison to the much smaller wooden boats used by the rural North Korean fishermen, hundreds of which have washed ashore in Japan bearing corpses. "," Breaking down large datasets to tell a story in a compelling way — especially when based on a subject our readers aren’t familiar with — was a challenge in and of itself. From a visual standpoint, we wanted to convey the complex research result and methodology to the reader that the vessels detected in North Korea’s water are Chinese vessels. We were able to tell that through several datasets carrying signal coordinates and lighting positions from fishing trawlers detected by satellites. The data showed that vessels departed from China’s seaport, entering the NK water zone, fishing during the night, and eventually going back to China’s seaport. To make this clear to the reader throughout the visual, we started by showing an overview of vessels routes in 2018, along with a step-by-step visual presentation overlaying locations from satellite view and data map view for two specific vessels. "," Not every story needs to be fully text based. Clear, comprehensive visualizations can help readers understand a complicated subject and more journalists would benefit from embracing this form of storytelling.  ",https://www.nbcnews.com/specials/china-illegal-fishing-fleet/,https://www.nbcnews.com/video/-ghost-boats-how-a-secret-chinese-fishing-fleet-is-killing-north-korean-fishermen-88130117858,,,,,,,"Ian Urbina, Jiachuan Wu, Marshall Crook, Elise Wrabetz","   Ian Urbina , a former investigative reporter for The New York Times, is the director of The Outlaw Ocean Project, a non-profit journalism organization based in Washington that focuses on reporting about environmental and human rights crimes at sea.     Jiachuan Wu  is currently a national interactive journalist for NBC News Digital's data/graphics team, where she creates compelling digital experiences, including data visualizations, maps, information graphics, dynamic interactive features, and cross-format multimedia packages. Previously, she was a data visualization developer for Reuters.     Marshall Crook  is a video journalist and filmmaker with NBC News Digital's Features Video team.      Elise Wrabetz  is a photo editor for NBC News Digital. ",22 Jul 2020
China,Caixin Media,Big,Garden: A Memorial Dedicated to the Lives Lost in the Outbreak,"Database, Infographics, Arts, Health","Animation, D3.js, Three.js, Canvas, JQuery, Json, Adobe Creative Suite, Microsoft Excel, CSV"," During the outbreak of covid-19 in Wuhan, news strories are focused on doctors, nurses and famous people, however, there are thousands of ordinary people died in this ourbreak whose name and story should also be remembered. We created this digital memorial for those who are not reported in breaking news. The memorial is presented as a quite and peaceful space filled with cherry blossom petals -- a symbol of the city of Wuhan, and was launched on the day of Qingming Festival, a traditional holiday to sweep tomb and commemorate deceased relatives. "," The date we launched the project is Qingming Festival, and the commemoration for the Covid-19 death has reached a national scale. Media, organization and social media lauched a set of commemorative articles, presenting a long list of significant person died in the ourbreak. However, as far as we know, our project is one of the few that highlighted the idea of ""presenting ordinary people names"". The project is not sorting the names with occupation or significance, but in a random order, to show that every life is equal in this tragedy. With this particular perspective, the project has struck a strong chord with the public. Many people retweeted and replied the link of the project on social media and commented ""they are very touched"". The project will also last for a long time and update more names for their beloved one. "," We use data visualization programming library D3 and THREE JS to create the map. Also we use SQL to program database. We use HTML, CSS, JavaScript to create the website.  "," The hardest part of the project is to collect and fact check the names and information of the dead. For security reason we closed the submit form for public shortly after the launch of the project, instead we left an email address for readers to provide information. However, it's a lot less efficient. "," The success of this project shows us that empathy is a prerequisite for good journalism, and before the national grand narrative, we still need to focus on the individual. Several months we launched this project, the New York Times released the thousand names obituary front page, which also resonates strongly with American people. ",http://datanews.caixin.com/interactive/2020/THREEJS/blossom/,https://datanews.caixin.com/interactive/2020/THREEJS/blossom/cherry/garden%20-%20translation.png,https://datanews.caixin.com/interactive/2020/THREEJS/blossom/blossom.mov,,,,,,"Meng Wei, Huaiwen Dong, Mengyuan Dong, Bing Wei, Shulin Zhang, Chen Huang, Mingzhong Geng"," Caixin VisLab is the data visualization team of Caixin. It is formed by data journalist, graphic designer and visualization developer. It focuses mostly on the field of environmental protection, economy and people's livelihood. Its works have won multiple domestic and global data journalism awards. ",4 Apr 2020
"Hong Kong S.A.R., China",Stand News,Small,Data Series on Cases Related to Hong Kong's Anti-extradition Law Movement,"Explainer, Database, Crowdsourcing, Illustration, Infographics, Chart, Politics","Animation, Scraping, D3.js, Json, Adobe Creative Suite, Google Sheets, CSV, R, RStudio, Python"," The battlefield for the anti-extradition law movement that roiled Hong Kong in 2019 has moved from the streets to the courthouses in 2020. In the past year, dozens of cases related to the movement move through the courts daily, yet it is hard to understand the impact and the extent of the legal crackdown that protesters faced. In this series, our reporters recorded details from every case that concluded since the movement began, totaling over 600 charges, to analyse whether Hong Kongers were granted a fair and just legal process against a government that so ruthlessly stomped down the movement. "," There is no public database of cases related to the anti-extradition law movement, thus it is difficult to assess the ways in which the justice department have been handling these sometimes controversial cases. In this series, we tried to tackle three topics with our database: 1. the quality of police testimony; 2. outcomes of charges; 3. tracking rioting cases.   In the first part of the series, we recorded almost 40 cases in which the judge openly criticised police officers for providing faulty and inconsistent testimonies. Legal experts also noted that there have rarely been any consequences for police officers that provided testimonies that were disputed, even though it is illegal to provide false testimony.    Secondly, analyzing our database, we found that less than half of charges related to the movement were ruled as guilty. The police have repeatedly stated that the rate at which protesters had to “face legal consequences” were more than 80 percent, without making clear their statistical methods.    The most serious charge faced by protesters is rioting — a crime that could result in a 10-year sentence. Currently, 695 people have been charged with rioting. So far only 11 people have been sentenced to prison, while hundreds still face a long waiting process as their cases move through the legal system.   In this series, we went through a tedious data collection process and openly shared our database as a public service. We also tried to analyze case data to find patterns, paired with illustrative footage from protests, as well as various data visualizations, so that readers could better engage with and comprehend the current state of the social movement as hundreds of people are caught up in complicated legal processes. "," Stand News has a team of reporters that cover the courts across Hong Kong daily. We collaborated in recording case details in a Google Spreadsheet. We also used Python to scrape public channels in Telegram that crowdsourced daily updates from the courthouses, for cases that our reporters were not able to physically attend.    In visualizing the data, we used d3.js to build charts and graphics, as well as After Effects in creating animations as well as annotating videos from scenes of protests.    "," The protests in Hong Kong that began in 2019 have sparked major changes in society. Local journalists face mounting pressure in covering protest related news, as politicians and fellow reporters have been arrested and charged for speaking out. Our reporters have stood at the front lines of the most intense clashes of the protests and now, under the guise of the national security law, we continue to do our best to give voice to the people as they face harsh sentences. To us, this series is an important record of the continuation of the protest movement, and a way to hold the government and the justice system accountable. "," Collecting case data was a major challenge. Over 10,000 people have been arrested in the protests and over 2,000 have been charged. We decided only to tackle the cases which have concluded to limit the scope of the project. We would like to encourage other journalists to take on the challenge of building your own database when there is no public information available. At the same time, we think that it was a good strategy to try to limit the scale of the project to something that is more achievable.  ",https://bit.ly/3oOZowF,https://bit.ly/36Jh32y,https://bit.ly/3oNizH3,,,,,,"K.K. Rebecca Lai, Leung Man Ki, Helena CYC"," K.K. Rebecca Lai is a graphics and data journalist who specializes in telling visual stories. Leung Man Ki is a reporter who covers breaking news, and also specialises in data collection and analysis. Helena CYC is an artist with a special interest in animation. ",18 Aug 2020
Australia,ABC News,Big,Rough justice: How police are failing survivors of sexual assault,"Investigation, Long-form, Database, Infographics, Chart, Video, Map, Women, Crime, Human rights","Personalisation, D3.js, JQuery, Adobe Creative Suite, Microsoft Excel, CSV, Node.js","To date, this series remains the most detailed national investigation of police mishandling of sexual assaults reported in Australia. Australia's sexual assault data is separately administered across eight states and territories. Each has its own definitions and classification system but all eight are plagued by complaints of police mishandling of sexual assault investigations. Rough Justice is the only project to date to distil records from across all eight jurisdictions and over 450 police commands into a complete national picture. The data-driven analysis stretches back between 10 and 23 years, depending on jurisdiction, and includes, for the first time, every local"," Nationally, the project sparked widespread calls for an independent inquiry into police handling of sexual assault cases.    In New South Wales, Australia’s largest state, it also prompted an push for the NSW Attorney-General to establish a committee to investigate the improper recording of sexual assault complaints withdrawn by the victim.   In the Northern Territory - the only jurisdiction that refused to supply the requested data - the first two stories in the series (published Jan 28 and 29) led to widespread calls and mounting public pressure for NT police to release their sexual assault data. This happened 3 weeks later, with Part 3 (Feb 20) of the series revealing that NT police are less likely to pursue a sexual assault report than police in any other state or territory.   In addition to being the only project to compile a national, data-driven picture of police responses to sexual assault,  R  ough Justice  is the first to:     Build an unprecedented searchable database of police investigation outcomes for 240,973 reported sexual assaults. This database matched a user’s postcode or suburb to their local area or police division, and allowed them to explore detailed statistics   Uncover the true extent of reports deemed “unfounded” by police, not only nationally but also in every state and territory, and every LGA/police division   Uncover the true extent of reports withdrawn by the victim (again, nationally, in every state and territory, and every LGA/police division)   Present detailed sexual assault statistics, including unfounded and withdrawn reports, for the Northern Territory  ","  Rough Justice  combined painstaking data analysis and stunning visualisations with shoe-leather reporting and first-person victim-survivor narrative.   The story opens with 140,000+ dots built in Adobe Illustrator and then animated “on scroll”. We felt the weight of the figure could only be truly illustrated by implementing a 1:1 ratio. The length of the scroll is hefty, exemplifying the enormity of the issue.   The article is dotted with expandable, first-hand accounts from survivors made with HTML, CSS and jQuery. The text glows to imbue each story with a sense of life, mimicking breath or a heartbeat. The negative space visually breaks up the density of the more complex data pieces and research.   The victim-survivors photographed requested different degrees of anonymity. We were able to respect their wishes yet maintain a consistent “look and feel” by using shadows, edited in Adobe Photoshop, to obscure faces/bodies - the extent of which the victim/survivors decided themselves. This technique allowed the survivors to literally ‘come out of the shadows’ and tell their stories - not only in their own words, but also on their own terms, with full agency.   To allow quick comparison between different neighbourhoods and police commands, we presented the data in a searchable map that presents a detailed analysis for the selected area. The maps were custom-built in JavaScript, using both the Leaflet map plugin and D3.js data tools.    The LGA small multiple stacked area charts were also a custom JavaScript build, using the D3.js data library. Again, this presentation was chosen to enable the user to quickly see the bigger picture, while also enabling easy comparison of specific neighbourhoods.   Data mining, cleaning and blending was done using Excel, Tableau Prep and Tableau Desktop, while initial “proof of concept” visualisation and mapping was done in Tableau. "," In terms of data, the hardest part was reconciling eight disparate datasets to produce coherent, accurate and meaningful national dataset of police sexual assault investigation outcomes.   As a first step, this required familiarity with the unique classification system used by each of the eight police databases. Our reporter worked with two independent teams of experts to create a  framework for “mapping” dozens of police investigation outcomes across these crime classification systems.   The next challenge was to obtain the data with the specific granularity, structure and format that would allow the datasets to be combined under this framework.   Our reporter negotiated with police and politicians for more than a year to obtain this exclusive data for every jurisdiction. In some instances, it involved informal collaboration with police commanders and police statisticians. In others, we were only able to obtain the requested data after applying pressure through backchannels.   Having obtained the data needed, we then worked with our independent experts to painstakingly reassign numerous investigation outcomes from the different jurisdictions to a national category.   In terms of reporting, the hardest part was finding victim-survivors whose cases had not progressed beyond the police. As experts explained to us, “failure” at the police stage meant these people were more likely to bury their experience and withdraw from all types of institutional support, including from advocacy groups, grassroots networks and legal services.   So we relied on personal contacts, building trust over more than six months with a network that included several survivors who had never disclosed their assault beyond one or two friends or family. This proved transformative for some, including Lauren*. It took eight months to gain her confidence but when the story was published, she said she felt “very proud” to have done something positive” to help bring about change for others. "," Sexual assault is regarded as one of the most serious and heinous crimes in Australia. Yet police mishandling of crime - and in particular, sexual crimes - has long evaded national scrutiny because of claims that police investigation outcomes cannot be compared across states and territories.   This authoritative series undermines these claims; in doing so, demonstrating the potential to carry out similar investigations for other categories of crime.   Crime reporting has traditionally tended to expose wrongdoing by revealing new information about a single case, crime or perpetrator. This project takes this worthy endeavour further: it uses a data-driven approach to uncover a long-running pattern of injustice across an entire category of crime involving multiple perpetrators, victims and police precincts.    The project also demonstrates that “new” storytelling techniques - such as data-driven investigation, graphics and interactive design - can be seamlessly blended with traditional person-centred storytelling to powerful effect.  ",https://www.abc.net.au/news/2020-01-28/how-police-are-failing-survivors-of-sexual-assault/11871364?nw=0,https://www.abc.net.au/news/2020-01-29/sexual-assault-legal-system-horror-show-for-survivors/11903584?nw=0,https://www.abc.net.au/news/2020-02-20/sexual-assaults-reported-to-nt-police-least-likely-to-be-pursued/11917478?nw=0,,,,,,"Inga Ting, Alex Palmer, Nathanael Scott, Jack Fisher and Lauren Roberts"," Inga Ting (data journalist), Alex Palmer (designer), Nathanael Scott (developer) and Jack Fisher (video journalist) are part of ABC New’s Digital Story Innovations team. Lauren Roberts is an award-winning reporter based in the Northern Territory of Australia. ",28 Jan 2020
United States,Quartz,Big,The coronavirus epidemic is changing emoji usage on Venmo,"Infographics, Chart, Business, Culture, Economy","Scraping, D3.js, Json, Python, Node.js"," As Covid-19 swept across the US and lockdowns ensued, what used to be everyday transactions—say, paying a friend for pizza or a taxi—drastically changed. Daniel Wolfe was able to illustrate these sharp declines and rises in person-to-person payments by comparing the change in usage of emoji in Venmo payment messages. The results reveal not only the broad shock to society—basketball emoji usage was way down because the NCAA tournament was canceled—but also the micro trends that bubbled up, like sending a nurse money for coffee or a friend payment for sewing a mask. "," Despite being written only in English and Venmo only being available in the US, the piece garnered notice around the world. We saw readers sharing the post with messages in English, Japanese, French, and Spanish, which is indicative of how Wolfe's visualization clarity broke through language barriers. The item was discussed on one of the US's most widely listened to radio program, used by at least one US school district, and cited in numerous newsletters as a way to understand how society was coping with the outbreak. "," The data was collected using a python and shell scripts, cleaned and analyzed using python in a jupyter notebook, then visualized with D3. "," Collecting the Venmo data is not easy. We had collected the 2017 data for a story then, and at that time there was an API that could be used to access public transaction data. By 2020, that API had been shut down so we had to periodically download the limited set of transactions that Venmo published to its website during the month of March then treat that data as a sample to do our calculations.   Parsing emoji is also complicated, and we had to come up with ways to extract the right meaning from the characters. For instance, a person emoji can be given various skin tones or genders. We had to determine whether a multi-part emoji was representative of its use or if combining those uses into one representative emoji would be better for the analysis. That meant grouping emoji like “skin-tone-4 haircut woman,”  “skin-tone-2 haircut man,” and “haircut woman” together as “haircut”. "," First and foremost, journalists can learn from this piece that data and visualization can be the entire story rather than just an aspect of it. That words can be formed around the design and layout of an experience rather than always the other way around. It can also be used as an example of how data collected by companies for reasons unrelated to the reporting can be used to tell a compelling narrative. It also shows that stories about business and personal finance are wonderful vessels to understand our society. ",https://qz.com/1831607/the-most-used-emoji-on-venmo-during-the-coronavirus-pandemic/,https://www.marketplace.org/2020/04/21/venmo-emoji-use-is-changing-in-the-coronavirus-economy/,,,,,,,Daniel Wolfe," Daniel Wolfe is a visual editor at CNN. Previously he was a reporter for the Quartz Things team, a cohort of journalists using non-traditional means to originate and visualize their stories. Prior to CNN he worked designing and developing content at Planet Labs, Tesla, and the Urban Institute. ",7 Apr 2020
United States,Boston Globe,Big,"Boston Globe Spotlight Team's series ""Last Words""","Investigation, Explainer, Long-form, Database, Infographics, Chart, Video, Health","Google Sheets, CSV, R, RStudio, Python"," Months before the pandemic, the Boston Globe Spotlight Team chose to tackle a subject few want to think about: death, dying, and the limited options for most people to die in peace, especially those from marginalized groups.    They found that in Massachusetts, a state that boasts some of the world’s greatest hospitals, income and race are factors in not only where and how people die, but in how long they live. The difference can be 15 years. They found that the region’s nursing homes were ill-prepared to handle the pandemic, and uncovered factors responsible for staggering COVID-19 fatality rates there. "," In a year saturated with bleak news, the stories in “Last Words” were among the most well-read in the Globe, lauded for their mixture of context and compassion, as well as their data-driven objectivity. They provoked heated discussion on a topic so many people prefer to avoid. Over five different days, the editorial page published numerous letters to the editor in response to the stories. The Globe’s comment board drew robust responses.    One reader thanked Globe reporters, saying, “It makes a big crack in the concrete wall of ignorance and denial in our society of the disparities and inequalities that our 'culture' has long tolerated.”   Government officials also responded. The series spurred state officials to release more data on the number of COVID cases and deaths at nursing homes, as they faced repeated pressure from the Globe to exhibit more transparency. Officials also publicly vowed to do better in overseeing safety at nursing homes, where Massachusetts had one of the highest death rates in the nation. One in seven long-term care residents died in the pandemic.    The state attorney general also began a probe into Medicaid nursing home discrimination, acting on information from the Globe’s undercover operation of bias against the poor at these facilities.   Overall, the powerful data analysis of the series and its deeply human stories served as a catalyst for discussions about death, dying and disparities, subjects so relevant in this pandemic year and beyond.  "," We used a variety of tools and techniques, including:   ● Linear and multiple regression to identify factors correlated with higher mortality rates at nursing homes. (First, we had to painstakingly gather data from a variety of state and federal databases and merge them together.)   ● Geocoding. We examined 21 years of detailed death certificate data to look for patterns in deaths (such as where people die by race and income) and identify deaths potentially related to COVID-19. But because the data did not include people's income, we geocoded the data based on the residential address of the decedent, looked up the median income for that Census tract, and then classified the person into an income bucket we devised partly based on Pew Research Center definitions.   ● Survey research. We commissioned Suffolk University to conduct a poll of Massachusetts residents and their views on deaths. We also surveyed the families of 450 people who died in 2016 by mail (using contact information from the state’s death data). The latter survey also included open-ended questions that gave reporters ideas for stories and names of potential interview subjects.   ● Audits. To identify Medicaid discrimination at nursing homes, we mimicked ""pair testing"" approaches often used by investigators to identify potential discrimination. We sent two emails to hundreds of nursing homes expressing interest in potentially placing an elderly parent in the facility. One set of emails indicated the person would have to rely entirely on Medicaid. The other set of emails indicated the person was affluent enough to afford to pay the bill themselves..   ● Content analysis. We conducted a content analysis of transcripts for the Massachusetts governor’s press conferences in the first month of the pandemic, finding that the governor and his staff rarely mentioned nursing homes at the briefings compared to hospitals. "," We faced several challenges in putting together this series. First, we had to sue the Massachusetts Department of Public Health twice to obtain the death data that was the backbone of the series, spending more than $16,000 in legal fees. Second, the data from the state and federal government was often incomplete and missing unique identifiers to easily merge the databases. (We wound up painstakingly joining databases by the names of nursing homes and manually fixing errors where the names of nursing homes were listed differently in different databases). In some cases, the state also withheld the pricese number of deaths at nursing homes, forcing us to contact the nursing homes themselves to find out the actual numbers and fill in that missing information in our database.    In order to successfully survey 450 families who had someone die in the same year, we needed to send out more than 5,000 letters – since most people don’t respond to mail surveys. (The state death data doesn’t contain phone numbers or email addresses for family members - just the name and address of the person who provided information to the state about the deceased.)    Of course, one of the biggest challenges was the pandemic. We were getting close to publishing a series about disparities in death and discrimination in nursing homes when the coronavirus pandemic erupted in the state – forcing us to pause publication of the series and then re-report many part of it.  "," The series powerfully demonstrates the importance and utility of death data - and it's worth it to even sue to get it. We obtained detailed data for 1.2 million deaths in Massachusetts, giving us incredibly detailed information for everyone who died in the state over more than two decades. That data could be used in any number of stories.    We also demonstrated how journalists can borrow some techniques from academia -- including multiple regression, paired testing, and content analysis -- to do their own studies. Multiple regression is a great technique to help figure out what factors are significantly related to deaths or something else reporters are studying. Paired testing is a useful way to to identify potential discrimination. Content analysis is a great way to measure what words or editorial choices people are making in their speeches, underscoring their priorities. We also showed two different ways of using scientific surveys - conducting your own or hiring a professional pollster. But the bottom line is journalists can take advantage of all these techniques themselves.    We think the series also showed how journalists can investigate a topic, then be forced to pivot when another major story breaks — like the pandemic. When that thappened to us, we didn't panic — we actually saw an opportunity to incorporate new pandemc death data and see if our earlier findings about inequities still held up.  They did and the result was a highly challenging and illuminating series to produce.  ",http://apps.bostonglobe.com/metro/investigations/spotlight/2020/09/last-words/,https://apps.bostonglobe.com/metro/investigations/spotlight/2020/09/last-words/part1-dying-poor/index.html,https://apps.bostonglobe.com/metro/investigations/spotlight/2020/09/last-words/part2-forgotten-elderly/index.html,https://apps.bostonglobe.com/metro/investigations/spotlight/2020/09/last-words/part3-nursing-homes/,https://www.bostonglobe.com/2020/09/28/metro/spotlight-team-probe-potential-medicaid-discrimination-massachusetts-nursing-homes/,https://apps.bostonglobe.com/metro/investigations/spotlight/2020/09/last-words/death-interactive-details/,https://www.bostonglobe.com/2020/09/26/metro/massachusetts-poll-race-education-gender-may-influence-some-divergent-views-about-death/,,"Reporters Mark Arsenault, Liz Kowalczyk, Todd Wallack, Rebecca Ostriker, Robert Weisman, Saurabh Datar and Spotlight editor Patricia Wen","   Mark Arsenault    Mark Arsenault covers casino development and gambling issues for The Boston Globe. He joined the newspaper in 2010 in the Washington, D.C., bureau, where he covered national politics and the U.S. Congress.     Liz Kowalczyk    Liz Kowalczyk reports on hospitals, doctors, and the patient perspective for the Globe, focusing on quality of care and medical errors, medical culture, and treatment trends, including for mental health issues.     Rebecca Ostriker    Rebecca Ostriker is a news reporter for the Globe. She was the Globe’s Arts Editor for nine years, during which she won a New York Times Co. Punch Award and two arts critics won Pulitzer prizes.     Todd Wallack    Todd Wallack is an investigative reporter and data journalist on the Globe’s Spotlight team. He won national awards for his work on public records in 2016 and has been a finalist for the Pulitzer Prize three times.     Robert Weisman    Robert Weisman reports on Baby Boomers -- their work, health, money, and lifestyle -- and life after 50. He is particularly interested in retirement and reinvention, aging, and second acts.     Saurabh Datar    Saurabh Datar used data and graphics to tell stories at the Globe. He also worked with reporters to produce interactive digital presentations, and builds tools and applications for the newsroom. Prior to the Globe, Saurabh worked at the Atlanta Journal-Constitution.     Patricia Wen    Patricia Wen is the editor of the Spotlight Team, the Globe’s investigative unit that includes six reporters. She took over in 2017 after having previously worked as a reporter on the team more than two decades ago. ",26 Sep 2020
United States,"BuzzFeed News, the International Consortium of Investigative Journalists, and more than 100 media partners around the world",Big,FinCEN Files,"Investigation, Explainer, Long-form, Cross-border, Multiple-newsroom collaboration, Database, Fact-checking, Podcast/radio, Infographics, Chart, Map, Corruption, Money-laundering, Economy, Terrorism","AI/Machine learning, Scraping, Json, Microsoft Excel, Google Sheets, CSV, PostgreSQL, PostGIS, Python"," The FinCEN Files began with a trove of secret government documents and grew into a partnership of more than 400 journalists in 88 countries — the largest reporting project in history.   Our 16-month investigation proved that five global banks continued to profit from suspect transactions even after they paid hefty fines and promised to clean up their acts.   It revealed how laws meant to stop financial crime have instead allowed it to proliferate.   And it showed, transaction by transaction, how some of the world’s most notorious criminals used the international banking system to legitimize the profits from their deadly dealings. "," Even amid a global pandemic, the FinCEN Files registered significant impact.    The reckoning began even before we published. After we approached the Treasury Department with our findings, it announced a sweeping plan to “address the evolving threats of illicit finance.”   Upon publication, the financial sector shuddered as investors <a href=""https://apnews.com/article/paul-manafort-money-laundering-archive-7a2579c1eac8a1300de258f25504e8e9"" style=""text-decoration:none;""><u>drove down   the share price of major banks cited in the series. Lawmakers in <a href=""https://www.buzzfeednews.com/article/jasonleopold/elizabeth-warren-bernie-sanders-fincen-files"" style=""text-decoration:none;""><u>the U.S.   and <a href=""https://www.icij.org/investigations/fincen-files/european-lawmakers-call-for-coordinated-banking-industry-reforms-in-wake-of-fincen-files/"" style=""text-decoration:none;""><u>Europe   demanded a response from financial institutions and regulators, and <a href=""https://www.bangkokpost.com/thailand/general/1990027/amlo-vows-probe-into-icijs-claims"" style=""text-decoration:none;""><u>Thailand   launched an inquiry into banks cited in our reporting.   Then, in December, the U.S. Congress <a href=""https://www.icij.org/investigations/paradise-papers/advocates-celebrate-major-us-anti-money-laundering-victory/"" style=""text-decoration:none;""><u>passed   the Corporate Transparency Act, the most consequential revision of U.S. anti-money laundering laws since 9/11. Key lawmakers credited the FinCEN Files with getting it over the line. Sen. Sherrod Brown, a co-sponsor, said the series “underscored that we need to strengthen, reform, and update our nation’s anti-money laundering laws.”   The legislation directly addressed our key findings. It forces owners of shell companies to identify themselves, shining a light on the shadowy entities powering the dark economy. It requires the Justice Department to file reports justifying its use of deferred prosecution agreements — sweetheart deals that let banks off easy. And the law adds protections and incentives for whistleblowers.   Closing another major loophole, the law empowers Treasury and Justice to subpoena records from foreign banks with corresponding accounts in U.S. institutions. It also requires the Treasury Department to pursue new technologies for uncovering criminal money flows.   In Europe, the impact included demands in the European Parliament for an overhaul of banking oversight; an examination by U.K. lawmakers into the effectiveness of anti-money laundering systems there; and a proposal from Belgian banks for a system to share information about suspicious transactions.   This <a href=""https://www.icij.org/investigations/fincen-files/heres-what-is-changing-after-the-fincen-files-shook-the-world-of-banking/"" style=""text-decoration:none;""><u>story   summarized much of the impact across the globe. "," The FinCEN Files include more than 2,100 suspicious activity reports, which banks must file with the U.S. government to flag transactions that appear to be linked to illicit activity. Making sense of these reports, which covered tens of thousands of pages, required innovation and exhaustive effort.    Every SAR contains two parts: a set of data tables and a narrative. The tables were unconventionally formatted and could span dozens of pages each, so we wrote Python scripts to extract the information and put it into a single database enumerating thousands of SAR subjects, key dates, suspicious transaction totals, and other crucial details.   The narrative sections contained key information about money flows and relationships between banks. At first, ICIJ partner SVT used machine learning to screen the records and obtained a first set of transactional data that could be used for research.   As variations in language and the complexity of the reports prevented the capture of some essential details, we launched a large-scale data-extraction effort. For more than a year, 85 journalists in 30 countries extracted transaction information in the narratives by hand, sharing them securely with partners on ICIJ’s Global iHub platform.   ICIJ then reviewed each extraction three times, a process that took seven months. To facilitate this effort, ICIJ used the Django web framework to build a fact-checking tool that allowed colleagues to flag errors and track edits throughout the process.   Reporters also built and analyzed databases of public information (such as corporate records) and cross-referenced them with SAR data.   Together, this information offered an unprecedented map of torrents of money circling the globe and funding mayhem — all within view of banks and regulators.   The specific tools included: Python (Django, Jupyter, Pandas, PDFPlumber, NetworkX), Excel, Google Sheets, PostgreSQL, Linkurious, Neo4j, VisiData, Docker, Ansible, ICIJ’s research platform Datashare. "," Data quality issues posed major challenges for the team.   The FinCEN Files investigation involved data on more than $2 trillion in transactions dated from 1999-2017 that had been flagged in the suspicious activity reports by nearly 90 financial institutions.    It was spread across tens of thousands of pages, most of it unstructured. Even just organizing it all, let alone reporting on it, required innovation and exhaustive labor to unpack. This led to a large-scale data-extraction effort and an exhaustive fact-checking effort.   The government forms are poorly designed; bank compliance staffers often fill out SARs with incomplete, confusing, or inaccurate information; and U.S. authorities haven’t done much to police the quality of the information that banks submit.   For example: More than a fifth of the SARs in the FinCEN Files list no street number, city, or even country for one or more of the flagged subjects. In some cases, the bank that submitted the SAR failed to include addresses for customers in its own corporate network. And when an address was included, more than half of the FinCEN Files SARs listed the wrong country, as designated by a two-character code, ICIJ found.   BuzzFeed News and ICIJ went through multiple rounds of validation to confirm the countries and territories used for the analysis.   The records also included hundreds of spreadsheets with data on more than 100,000 transactions filled with party names, bank names, figures, and dates — using a variety of different formats and field names — but were unattached to the narrative that would have explained their inclusion.   ICIJ undertook an effort to standardize the field names and address formats to make them more useful to our partners. We also used shoe-leather reporting with sources and external documents to help make sense of this information. "," We believe this project demonstrates the value of large-scale, cross-border collaboration. The document and data in the FinCEN Files touch nearly every corner of the planet. Our partnership with more than 100 other newsrooms allowed us to produce major stories in dozens of countries, and break news in dozens more.    Getting to that point, however, required a strong shared foundation of trust, training, and collaboration. During the project’s first phase, BuzzFeed News and ICIJ convened a confidential meeting with more than 100 partners in Hamburg, Germany, for two days of meetings to brainstorm, teach, and develop reporting plans. Building off the success of those meetings, reporters continued to collaborate via ICIJ’s secure Global iHub platform, where they could discuss new leads, share insights, and ask questions.    In some ways, this massively collaborative approach added complexity to an already-complex project. Given the sensitivity and complexity of the material, for instance, all partners went through trainings on security and the use of ICIJ’s technologies for research tailored to the project. Partners spent non-trivial amounts of time reading each other's messages, catching up on the latest findings. And coordinating the final publication schedule was an exercise in diplomacy and intricate logistics.   But the benefits greatly outweighed those costs. As reporters scoured the FinCEN Files, they found evidence that assisted colleagues halfway around the world. Together, they pieced together a clearer picture than ever of dirty money flowing through the mainstream financial system. Independently, too, they dove deep into their countries and topics of expertise — now placed in the context of a globe-spanning investigation. The sheer size of the collaboration also unlocked possibilities that would have otherwise been unthinkably onerous, such as the grueling-yet-essential project to manually collect all transaction information described in the SAR narratives, as discussed above. ",https://www.buzzfeednews.com/article/jasonleopold/fincen-files-financial-scandal-criminal-networks,https://www.buzzfeednews.com/article/jsvine/fincen-files-explainer-data-money-transactions,https://www.icij.org/investigations/fincen-files/mining-sars-data/,https://www.icij.org/investigations/fincen-files/explore-the-fincen-files-data/,https://www.icij.org/investigations/fincen-files/confidential-clients/,https://www.buzzfeednews.com/article/anthonycormier/hsbc-money-laundering-drug-cartels,https://www.icij.org/investigations/fincen-files/with-deutsche-banks-help-an-oligarchs-buying-spree-trails-ruin-across-the-us-heartland/,,"BuzzFeed News, the International Consortium of Investigative Journalists and more than 100 media partners around the world"," BuzzFeed News is a global news organization providing original online reporting across the internet’s biggest platforms, and is a two-time finalist for the Pulitzer Prize.   The International Consortium of Investigative Journalists is a global network of reporters and media organizations who work together to investigate the most important stories in the world. Over the years, ICIJ has released dozens of investigations – including the Pulitzer Prize-winning Panama Papers – and has won many awards for its work.   A full list of <a href=""https://docs.google.com/document/d/1tPdXpOjIAbqvBibraURAHSMiK7Czksuyql3blJzl8UU/edit"">media partners can be seen here . ",20 Sep 2020
United States,"International Consortium of Investigative Journalists, FRONTLINE, Expresso, The New York Times and 33 other media partners",Big,Luanda Leaks,"Investigation, Explainer, Long-form, Cross-border, Multiple-newsroom collaboration, Database, Fact-checking, Map, Corruption, Business, Economy, Human rights","AI/Machine learning, Microsoft Excel, Google Sheets, CSV, PostgreSQL, Python"," Luanda Leaks exposes the inner workings of a global business empire fueled by hundreds of millions of dollars in public money siphoned from one of the poorest countries in the world.   Drawing from more than 715,000 leaked records, public documents and hundreds of interviews, it casts unprecedented light on the mechanics of corruption at the grandest scale, and reveals how Western professionals play a vital, little-examined part in the blighting of countless lives.   The investigation centered on Isabel dos Santos, Africa’s wealthiest woman, involved more than 120 journalists in 20 countries. "," Reaction began before we even published. In December 2019, after ICIJ questioned Angola’s government, a court froze dos Santos’ major assets, including banks, a telecom company and a brewery. The government said it was trying to recover $1.1 billion in lost assets.   When the stories went live, the floodgates opened. Angola’s attorney general charged dos Santos with embezzlement and money laundering. A provincial court labeled a deal to acquire the jewelry company de Grisogono that was a focus of our reporting as “fraudulent.”   In Portugal, authorities froze dos Santos bank accounts and seized all of her assets, including luxury properties and stakes worth hundreds of millions of dollars in major companies. The country’s top market regulator launched investigations of nine auditing companies that worked with dos Santos, threatening criminal charges if it could be determined they broke anti-money laundering laws in aiding dos Santos.   Dutch prosecutors began a criminal investigation, examining how a company owned by dos Santos’s husband managed to acquire a lucrative shareholding in the Portuguese oil and gas firm Galp. German police raided a state-owned bank as part of a criminal probe. Cape Verde’s finance minister said the Luanda Leaks revelations helped seal a new law that closes down offshore banks.   The head of PwC’s tax team for Angola and Portugal stepped down. The firm’s chairman said he was “shocked and disappointed” and launched an internal investigation. From a reputational perspective, the dos Santos story was the worst thing to have happened to PwC on his watch, he said.    PwC declined to respond to specific follow-up questions in December, but said that in the wake of the investigation a number of senior employees have left “or been subject to other remedial measures.” ","     The leak itself was of more than 715,000 records. Documents were in different formats (emails, PDFs, spreadsheets) and about half of the files were in Portuguese. To make this large trove of files searchable and share them with the team in a secure way, ICIJ used its research tool Datashare. As most of the team didn’t speak Portuguese, ICIJ used an open source software to translate the records, and have both the original and a translated searchable English version of the file in Datashare.   The high volume of files and unstructured data required additional efforts to analyze them. To help with the mining and reporting, ICIJ and media partners used machine learning to cluster similar types of files that could be of interest for the investigation and added filters with the results of those clusters to Datashare. ICIJ also used graph databases and entity extraction to facilitate establishing connections between the files.   ICIJ manually created massive databases from scratch to propel research and help analyze the material. We combined the records we obtained with corporate registries from more than a dozen jurisdictions, corporate documents and other datasets to compile and map an extensive—and exclusive—record of more than 400 companies owned or otherwise tied to Isabel dos Santos, her husband, or both, including 94 registered in secrecy jurisdictions. Each data entry was verified and fact-checked.   This database offered a unique view of the dos Santos companies and was central for the investigation. ICIJ made the final data available to the public: https://www.icij.org/investigations/luanda-leaks/explore-how-to-build-a-business-empire/ The specific tools used included: Datashare (ICIJ’s developed research tool to make the files searchable - open source), Google Sheets, Apertium, Neo4J, Linkurious, Talend, SQL Server, Universal Sentence Encoder, ElasticSearch, Annoy.     ","The records obtained by ICIJ included emails, government decrees, spreadsheets, ledgers, audits, incorporation papers, organizational charts, meeting minutes and videos, loan agreements, deeds, tax advice and tax returns. While the records, many in Portuguese, laid groundwork for stories, they were incomplete. They referenced other documents that were not included and events not further described. The reporting was like sifting through a giant box of pieces from countless different puzzles — and then finding that even when assembled, each was missing crucial parts. Much time was devoted to reconstructing transactions, tracing the flow of money from an Angolan or dos Santos business through shell companies and then into the legitimate financial system. Simply reading the financial documents required thousands of hours. ICIJ turned to forensic auditors and other experts to help better understand what we were seeing. Team members spent weeks on the ground in Angola, a country with no tradition of a free press and a difficult reporting environment. We chased leads around the globe, from Portugal and Brazil to Malta and Delaware. We interviewed hundreds of people, including government ministers, bank officers, company incorporation specialists and everyday Angolans. The persistence paid off: additional sources led to additional documents, helped fill in gaps in understanding, and pointed to new stories, including embezzlement of millions of dollars from the state oil company dos Santos ran to using her father's backing to plunder state infrastructure funds to a mass eviction of an entire seaside community, driven out by bulldozers and police to make way for a dos Santos real estate development. Another considerable challenge comes with our model: fashioning a single team out of disparate and far-flung news organizations. We shared tips and bits of reporting on our bespoke internal communications platform, held hundreds of virtual meetings and met up on the"," Faced with a trove of records in different formats? Consider a technological solution. ICIJ’s Datashare can power up collaborations and make the task of reading through documents across different desks and even newsrooms an achievable goal. Datashare is open source, meaning that any journalist can download and use it for free. (https://datashare.icij.org/)    For Luanda Leaks, we faced an additional hurdle: It wasn’t a few hundred documents that we wanted to collectively sort through—it was a vast trove of more than 750,000. ICIJ teamed up with Quartz AI studio to use “machine learning” to automate the sorting process. This subset of artificial intelligence could learn to identify similar groups of files more likely to contain stories—pulling out, for instance, balance sheets to queue up for review by our reporters. The results were later integrated into Datashare. Artificial intelligence experts who might be willing to lend a hand dwell in many sectors, including academia. Don’t be afraid to ask.   A set of original documents is the starting place for reporting, not the end. To build the company database, for examples, we started by pulling names and other relevant information from the files and then used public information harvested from company registries, databases and corporate documents to validate our findings and expand on what we were learning. To make sense of it, and to track our work, ICIJ manually built a database that compiled data on companies in which dos Santos or her husband hold, or have held, a stake as shareholder, directly or indirectly. ICIJ also relied on information gathered through previous investigations, including Offshore Leaks. ",https://www.icij.org/investigations/luanda-leaks/how-africas-richest-woman-exploited-family-ties-shell-companies-and-inside-deals-to-build-an-empire/,https://www.icij.org/investigations/luanda-leaks/how-we-mined-more-than-715000-luanda-leaks-records/,https://www.nytimes.com/2020/01/19/world/africa/isabel-dos-santos-angola.html,https://www.icij.org/investigations/luanda-leaks/explore-how-to-build-a-business-empire/,https://www.icij.org/investigations/luanda-leaks/luanda-leaks-reveals-thousands-forced-at-gunpoint-from-fishing-community-at-site-of-dos-santos-project/,https://www.icij.org/investigations/luanda-leaks/after-luanda-leaks-a-billionaires-empire-falls-but-her-enablers-carry-on/,https://www.icij.org/investigations/luanda-leaks/from-colonization-to-kleptocracy-a-history-of-angola/,,"International Consortium of Investigative Journalists, FRONTLINE, Expresso, The New York Times and 33 other media partners"," The International Consortium of Investigative Journalists is a global network of reporters and media organizations who work together to investigate the most important stories in the world. Over the years, ICIJ has released dozens of investigations—including the Pulitzer Prize-winning Panama Papers—and has won many awards for its work.   A <a href=""https://www.icij.org/investigations/luanda-leaks/about-the-luanda-leaks-investigation/"">full list of media partners can be seen here . ",19 Jan 2020
Germany,"Brands of the Funke Mediengruppe (Berliner Morgenpost, Hamburger Abendblatt, WAZ, Thüringer Allgemeine, Braunschweiger Zeitung and many more)",Big,Coronavirus-Monitor,"Explainer, Database, News application, Infographics, Chart, Map, Health","D3.js, Json, Google Sheets, CSV, OpenStreetMap, Node.js"," The Coronavirus-Monitor gives its users a comprehensive overview of the worldwide pandemic - with the latest figures fed into an easy to use tool. With the changing information needs features were changed, removed, and added. The home screen lets users explore the total number of infections, recoveries, deaths - for different countries, German federal states, by total and relative numbers, historical and current, in maps, graphs and tables. Additional sections visualize the development for Germany and lead to subprojects: to the ICU-Monitor showing the current utilization of German ICUs, to the analysis of excess mortality or the Vaccination-Monitor. "," As the Coronavirus-Monitor was the first major German dashboard on coronavirus (first German case February 27, launch March 04), and offered real added value to the Johns-Hopkins-University dashboard through its additional data on Germany's states as well as unique features, it landed a visitor success right on the first day. The enormous rush was kept stable by constantly integrating new features, reacting quickly to new information or information needs, and keeping the Monitor one step ahead of the competition in many of its offerings. With well over 300 million visits (400 million including the subprojects) in 2020, it is probably one of the most popular coronavirus resources in Germany and by far the most successful article in the history of the Funke Mediengruppe. One of the Group's outlets, the Berliner Morgenpost, increased its number of unique users for March by 680 percent compared to the previous year, making the Monitor a huge success in raising brand awareness. In addition, it was instrumental in the success of the Coronavirus newsletter and newsblog, as well as for new subscriptions through its referrals to other resources of the Group, as internal analysis has shown. As it was the only platform at all to regularly show the latest figures for the German states, which were initially only published via individual press releases by authorities, it became the main source for JHU for Germany and thus the main source for many media brands. Thus, we were always one step ahead by having the numbers for Germany first, as the other German media brands received them through JHU with a reporting delay. Although they quickly set up their own great offerings and other data sources got available, the Coronavirus-Monitor remained the first stop for many users, still leading to record daily hits today. "," The Coronavirus-Monitor is built modularly of multiple React.js widgets. The individual sections can be easily moved in their order, features can be adjusted and widgets can be included and updated on other project pages at the same time. Since the site had such immense success, the Maptiles from the third-party provider Maptiler quickly became too expensive, so the team set up their own Maptile Server. Data is scraped from multiple sites and sources or flows through APIs to our data server, where it is cleaned, renamed, and checked. Only then is the data passed to the project as JSON or CSV. Data for Germany is loaded from various official sources into a Google spreadsheet, where journalists from the team perform a daily manual check and then enter the data into the individual sheets for each federal state, from where it is in turn scraped and transferred to the data server. A Slack bot notifies the team of strange discrepancies, such as large downward corrections / decreasing total case counts. The maps are based on Leaflet.js, most of the graphics are built with d3.js.     "," Similar to how all journalists probably felt about this topic, the dynamic situation and the constant changes to the official data sources were a great challenge. Especially in the beginning, when authorities or even the JHU regularly changed their data formats, publication times, or definitions without warning, this meant a lot of manual work, sometimes on a Sunday at midnight. Due to the immensely high number of hits and the fact that our Monitor was the main source for the German figures of the JHU and thus also of many media offerings for months, we also had a great responsibility to double-check the data and to make definition changes clear. In addition, a single statement by a politician could trigger a whole new information need, e.g. a whole new indicator with a new threshold for lockdown policies, to which we had to respond quickly with a new feature. And this year, of all years, we had no designer for months (May to August), which limited the team to two full-time employees and two freelancers (working a maximum of three days a week) for a few months. In addition, we found ourselves in the home office as early as April, and never before had to search so intensively for new backend solutions, as the many accesses and the ever-growing database threatened to bring our normally perfectly adequate technology to its knees. Because normally we work on projects that we plan once, publish, and then update with more current data a year later at most. All in all, every aspect of this project, the space and time circumstances, the technical solutions, was an absolute challenge and turned our entire workflow upside down. The team really outdid themselves in every aspect, and still managed to release other projects and subprojects. "," This project showed what a small, dedicated and well organized team can do, even when circumstances are unfavorable (no designer for months, only two full-time employees, spontaneous home office) and variables are constantly changing (new backend solutions needed, data sources and information needs constantly changing). It also showed us what an impact it can have to quickly publish a well designed, minimal valuable product with the main focus to be simple and clear, a ""no bullshit dashboard"" as we called it internally, and then develop it further and further in the days to come, building it in a modular and agile way so you can react quickly to changes. Also important to the immense success were small features that set the Monitor apart from the competition: For example, it was the first dashboard that also showed recoveries, the first dashboard that offered a time slider, and through the manual work of the team in collecting the individual press releases, it was also the first dashboard with the current numbers for each German state. Such small features made the difference, catapulting a regional brand like Berliner Morgenpost to the top of unique-user growth in Germany and allowing this local newspaper to pass national brands like Süddeutsche Zeitung (known for example for the Panama Papers) or Zeit Online with its page views for a few weeks. Last but not least, this project can also show how money can be made with data journalism. Although it was not behind the paywall but remained freely accessible, considerable advertising revenue was generated and, by successfully redirecting the user flow to other resources such as the newsblog, newsletter sign-ups or other interactive subprojects with advertising integration, the reach of the entire offering of the Group was increased and even subscriptions were generated. ",https://interaktiv.morgenpost.de/corona-virus-karte-infektionen-deutschland-weltweit/,https://interaktiv.morgenpost.de/corona-impfungen-deutschland-bundeslaender-weltweit/,https://interaktiv.morgenpost.de/corona-deutschland-intensiv-betten-monitor-krankenhaus-auslastung/,https://interaktiv.morgenpost.de/corona-uebersterblichkeit-sterberate-deutschland/,,,,,"Marie-Louise Timcke, André Pätzold, David Wendler, Angelo Zehr, Sebastian Vollnhals, Webkid (Christopher Möller, Moritz Klack)"," Funke Mediengruppe's Interactive team develops interactive applications and data-driven stories for the Group's various news brands. It acts like an interdisciplinary working group of data journalists, designers and programmers within the newsroom, is very visually driven and user-focused, and covers various topics ranging from elections to climate change or social inequalities.  ",4 Mar 2020
Brazil,Fiquem Sabendo,Small,120 years of darkness: shedding light on government pensions in Brazil,"Investigation, Breaking news, Multiple-newsroom collaboration, Database, Open data, News application, Chart, Politics, Economy","Microsoft Excel, Google Sheets, CSV, R, PostgreSQL, Python"," For over a century Brazilian tax payers have given a blank check for the government to use on pensions for public servants with no oversight. For three years Fiquem Sabendo battled in the Supreme Audit Institution of Brazil,  pressuring for access to historical records. In 2020 we won, freeing 26 years worth of data about payments. We cleaned, analysed and uploaded the data online, identified issues and corrected the official database by collaborating with the  Ministry of Economy and then organized a pool of journalists to produce stories. Finally, we gave free access to our tool for everyone.  "," This was possibly the largest database ever obtained based on Freedom of Information in Brazil. The data on individual monthly pension payments to civilians from 1994 to 2020 consists of 100 million lines spread across 27 different files. It is also - possibly -  the most important in terms of public money disclosure. We are talking about U$ 88 billion in pensions for over 400.000 public servants' family members, through 95 million individual payments made in 26 years. In the dataset we found lifetime pensions established in 1900 and payments that reach up to R$ 30,000 monthly. Before that, it was impossible to hold the government nor beneficiaries accountable because their names and pensions were unknown to the public.    To take these historical numbers to the broader public, Fiquem Sabendo coordinated a collaborative task force with four journalists to produce stories from the dataset. Lucio Vaz (Gazeta do Povo), Eduardo Barretto (Época), Bruno Fonseca (Agência Pública), and Taís Seibt (Fiquem Sabendo/Yahoo) worked for over a month with our team to find scoops. Agência Pública, for example, found that hundreds of officers accused of torture during the dictatorship have been receiving lifetime pensions from the government.    Over a year since the data was released by our small independent team, it is still being used by many major news outlets (Google ""Fiquem Sabendo"" ""pensionistas""). Even president Bolsonaro tried to weigh in the situation by falsely claiming on social media he was the one to order the release of the dataset. The House of Representatives leader condemned the pensions.   The government was positively impacted by our investigation. We will explain later how our work changed the official records by almost a billion dollars in net value by identifying structural mistakes on the dataset and helping the Ministry of Economy to fix it.  "," We managed to transform the 60 GB dataset from an inaccessible archive of separated files into a dynamic and - most importantly - useful online dataset for reporters, researchers, and citizens.    On Shinyapps, the application structured by Fernando using R, anyone is able to explore filters, timelines and download smaller pieces for further exploration on Excel/Sheets. A very important aspect here is the monetary corrections applied to update the numbers. Through this tool, anyone can see both the original values from the official dataset and the new values considering two decades of inflation and a change of currency. The platform is still online and open to the public.   On Metabase, structured by Álvaro Justen using Python, it was possible to create dashboards of interactive graphs, make calculations, pivot tables, and follow a single beneficiary's complete path over the years. Because this is a tool that costs based on the usage volume, we gave exclusive access for journalists to deeper explore the data.    ","Overcoming public data censorship Fiquem Sabendo started this fight back in 2017 and only after three years in the Supreme Audit Institution of Brazil, we won. The decision, unanimous and favorable to the agency, guaranteed the publicity of payments made to pensioners of the federal government for the first time in history. In January of 2020 we got two months worth of data and - finally - in July we released 26 years of payments for accountability. This was a result of a long collaboration between journalism and law. Despite the huge achievement, we are still in a battle for the full release of the information! Over a year after the tribunal's decision, the government has not yet published the data on payments made to relatives of military personnel and secret agents, nor to relatives of Central Bank employees. Fiquem Sabendo has already filed two petitions for this information to be made public and as soon as it is we will repeat the same process. Identifying and correcting mistakes in the official dataset As we analysed the dataset we noticed strange entries. Because we had built two different tools, in different languages ​​(R and Python), by two different people, we were sure that the problems detected on our platforms were on the official data published by the Ministry of Economy. We then organized the probable errors and inconsistencies found in three spreadsheets: single payments over R$ 1 million, R$ 100,000 and empty cells. For over a month we went back and forth with the Human Resources Management Secretariat to show and correct the problems. How impactful were those mistakes in the public budget? Almost a billion dollars in net value. Because we collaborated with the technical department responsible for the information, our reporting was able to correct the original governmental","   Collaboration with other areas, not just other newsrooms    Having a lawyer as a cofounder working to support journalists showed us how important it is to have a team from diverse areas. Our reporting would not have existed if it wasn't for the whole legal battle that preceded it and the court's decision would not have invoked such public appeal if it had not been appropriately presented to society.      Open for everyone    Leaks and special sources are part of journalism, but we also need to fight to make things actually public. It is a structural part of our job to make public data available not just to do a scoop, but to create real accountability by allowing anyone to check the full data - available online. By using FOIA the reporting is not just about that one piece anymore, it is a pathway for others to push transparency even further. Documents obtained through the legal due process become a precedent for other documents, agencies and for state and city level decisions. The importance of this project does not end on itself, it can now be used to open pension payments all over the country, it can base requests for other parts of the government, etc.      Citizens are potential journalists waiting to act    They have the will. If we share the  tools, they can help us. It was through Twitter DMs of family members that didn't find their own relatives on the dataset that showed us the information first released by the government was not complete. ""Normal people'' helped us identify that the first dataset published by the Ministry of Economy did not include beneficiaries related to military personal, secret agents and the Central Bank. And that's what fueled our second petition to the Supreme Audit Institution of Brazil. ",https://fiquemsabendo.substack.com/p/aps-denncia-da-fiquem-sabendo-governo-17d,https://fabdev.shinyapps.io/graphs_on_demand/,https://fiquemsabendo.com.br/transparencia/pensionistas-e-aposentados-servidores-inativos/,https://br.noticias.yahoo.com/governo-federal-pagou-r-384-bilhoes-a-pensionistas-desde-1994-070059010.html,https://apublica.org/2020/01/as-pensoes-vitalicias-dos-acusados-de-crimes-na-ditadura/,https://piaui.folha.uol.com.br/pensao-a-brasileira/,"https://politica.estadao.com.br/noticias/geral,governo-paga-pensao-a-52-mil-filhas-solteiras-de-ex-servidores-do-executivo,70003185132",,"Maria Vitória Ramos, Bruno Morassutti, Luiz Fernando Toledo, Léo Arcoverde, Fernando Barbalho, Álvaro Justen"," Maria Vitória Ramos is the CEO and cofounder of Fiquem Sabendo.    Bruno Schimitt Morassutti is a cofounder and Supervisory Board Member at Fiquem Sabendo Association and Open Knowledge Brazil.    Luiz Fernando Toledo is a Fiquem Sabendo's cofounder, editor at OCCRP and the directors of Abraji.    Léo Arcoverde is cofounder and president of Fiquem Sabendo, and news producer at GloboNews.   Fernando Barbalho is a data scientist that researches and implements products for transparency in the Brazilian public sector.   Álvaro Justen is the founder of Brasil.IO, a Brazilian programmer, teacher, and free/libre software and open data activist. ",13 Jan 2020
Kenya,Nation Media Group,Big,Unmasked by Nation Media Group,"Investigation, Long-form, Open data, Illustration, Politics, Corruption, Economy","CSV, Python, Node.js","  Unmasked  is a flexible platform for investigative journalists and anti-corruption researchers in Kenya, that enables journalists and newsrooms to capture and publish invaluable open data from daily and investigative reporting.    The Nation Media Group created the project because in politically and technically constrained environment like Kenya, data for journalism is hard to come by and key information is made public by journalists and civil society.    Open data generated from stories increases value for all reporting, by providing insight to past and future stories while unmasking the complicated relationships behind grand corruption of people in power and associated people and organisations.  "," The project pulled  together multiple stories on issues such as <a href=""https://unmasked.nation.africa/issues/covid19millionaires"">#COVID19Millionaires  exposing the different actors, government agencies and companies involved.  The insights gained from curating the procurement stories in Kenya enriched data and investigative reporting at the Nation Media Group resulting in compelling stories including: <a href=""https://nation.africa/kenya/news/how-sh9bn-kemsa-plot-was-nipped-in-the-bud-2452750"">How Sh9bn Kemsa plot was nipped in the bud    Besides enhancing the reporting of procurement stories, other major impacts of the investigative Covid-19 stories done by NMG as part of the project include:     Audit ordered by President Uhuru Kenyatta   EACC finalising files for DPP to charge culprits in court   Kemsa suspended CEO and two directors   Health ministry ordered to publish list of all suppliers   Two parliamentary probes on going   EACC freezes payment of suppliers until verification is done, says expose helped it stop further losses of Sh9 billion     Unmasked,  increases the value of reporting for a newsroom, by connecting and building upon past stories, where each reported story contributes to providing a more complete understanding of the topic.    The platform enables journalists to quickly provide deeper context and insights when reporting, by exposing connections to past stories that might have been overlooked. The use of simple forms based on open standards enables capturing of complicated data and relationships such as beneficial ownership, without requiring expert knowledge.    The South-South cross-regional collaboration in development of this platform between Kenyan and Malaysian data journalists, shows that innovative techniques in using open data standards for reporting and investigations are possible and applicable in similar data and politically constrained environments for data journalism.   Publication of open standards compliant open data via a public API enables not just the newsroom, but other public users to innovate such as the <a href=""https://relations.nation.africa"">interactive data visualisation of relationships .     "," The project was built on top of the <a href=""https://plone.org/"">Plone open source content management system , using open data standards of <a href=""https://www.popoloproject.com/"">Popolo-spec  (People & Organizations), <a href=""https://standard.open-contracting.org/"">Open Contracting Data Standard  (OCDS) and the <a href=""http://standard.openownership.org/en/0.2.0/"">Beneficial Ownership Data Standard  (BODS).    This choice allowed for rapid development and implementation of the standards, while providing a user friendly interface for entering and managing data from stories.    It provided a way to easily join up or extend overlapping data fields from different standards, such as extending fields for a Person from Popolo-spec with additional fields for Politically Exposed Persons (PEPs) from BODS. Similarly, more detailed information for people and positions are made available for OCDS contract information, by using the same content for Persons that was extended before.   Custom views on the data, also allowed the platform to be used for different use cases for media in <a href=""https://unmasked.nation.africa"">Kenya  and for anti-corruption in <a href=""https://politikus.sinarproject.org/"">Malaysia .   It provides different workflow and access states, for which ongoing investigations and new data for unpublished stories, can be kept private for internal use until publication. Full text search throughout the platform allows journalists to not only search data, but also text in supporting documents including images.   Publication of open standards compliant open data, via a public ReST API that enables data journalists internally and externally to create visualizations or use tools to use data for data or investigative journalism.    A script was also developed to <a href=""https://github.com/Sinar/popit_relationship"">export the data, into neo4J  to enable visualization and exploration using network graphs.   Finally the <a href=""https://github.com/NMG-Digital/nmg.unmasked.views"">platform is open source  allowing others to also use it and where updates and additional features are shared among implementers.     "," Unmasked, an open data  public procurement investigation project,that was established by Nation Media Group with support from Hivos East Africa faced many challenges.   The project had to find a way to showcase how open data, especially procurement data can support journalism in uncovering corruption in procurement, in an environment where there is limited open data. It also had to do it with very limited human resources.   Due to Covid19 it was delayed and resource constrained, the entire platform was mainly developed between just two developers on a part-time basis, with additional support from NMG in-house journalists and where all interactions including workshops were done remotely between Malaysia and Kenya. The pandemic  also made it difficut for the journalists to go to investigate leads generated from insights derived from the platform for followup investigative and data driven stories.   Despite the challenges the process of enriching  the platform with more content by journalists and developers is ongoing. ","  Unmasked  is a testament to the fact that in environments with limited open data provided by the government, open data can be generated by the media through day to day and investigative reporting.    Open data standards can provide the structure needed as guidance for investigations, but also for collaboration in capturing and re-using data within the newsroom between different teams. A sports reporter could be reporting on a takeover bid of a football team, and be entering data on beneficial ownership without being a domain expert, while at the same time, possibly getting additional information on the persons and companies involved for an unrelated investigation for a story done by another team.   Finally, in an era of short news cycles and revenue, that project also shows that media can generate and publish open data through reporting, which in turn increases value by extending the relevancy of a story with each new connection made through data for new stories.    That data captured through reporting, contributes to open data that increases value for the entire newsrooom and organisation.       ",https://unmasked.nation.africa,https://relations.nation.africa,https://nation.africa/kenya/news/how-sh9bn-kemsa-plot-was-nipped-in-the-bud-2452750,https://unmasked.nation.africa/issues/tender-fraud-at-communications-authority-of-kenya,https://docs.google.com/presentation/d/1IhtZNORf1yCyJ1lYACZl6MiXouMedzDDUumFoesgPLg/edit#slide=id.g89d9d5ffe4_0_77,,,,"Developer Khairil Yusof, Developer Samuel Ochola, Head of development and learning Churchill Otieno, investigative and financial journalist Paul Wafula, Data Editor Dorothy Otieno"," Chruchill Otieno is the Head of Development and Learning at Nation Media Group. He drives digital transformation and oversees new story telling at the Nation Media Group.   Khairil Yusof is the  Coordinator at SInai Project and investigative journalist working on applying innovative methods of open data and standards, for tranasparency and anti-corruption.   Samuel Ochola is a full stack developer at Nation Media Group. He developed the front-end interface and is responsible for periodic updates and maintenance of the platform.   Paul Wafula is an investigative and financial journalist who broke the  Covid Millionaires  story. He is also the Business Editor at the Daily Nation.   Dorothy Otieno is a  Data Editor at the Nation Media Group who gave guidance on the data aspects for the project.             ",10 Sep 2020
Germany,silo.institute,Small,Privacy Preserving Proximity Tracing,"Explainer, Fact-checking, Mobile App, Infographics, Chart, Health","Animation, Canvas"," Contact tracing is a crucial but labour-intensive strategy to reduce SARS-CoV-2 transmissions and the spread of COVID-19. In early 2020 several strategies to automate this process utilising smartphones and bluetooth technology have been put forward. This visual explainer based on the DP-3T proposal which was adopted by the Google/Apple Exposure Notification (GAEN) system demonstrates how privacy preserving proximity tracing works, why we need it and the risks it entails. "," In line with the privacy preserving focus of our topic we refrained from monitoring visitor numbers which makes quantfying our outreach difficult. However, the project was well received on Twitter. It was <a href=""https://www.berliner-zeitung.de/zukunft-technologie/die-funktion-der-warn-app-modern-erklaert-li.87342"">covered by a local Berlin newspaper (Berliner Zeitung)  and – among others – <a href=""https://twitter.com/JuerMueller/status/1272086508409929729"">shared by Juergen Mueller , CTO of SAP, the company responsible for implementing the Corona-Warn-App in Germany. We also have anecdotal evidence from friends and family that our visual article fostered understanding of how the app works and that it motivated people to install the app when it became available. "," Our project heavily relies on the scrollytelling pattern and features elements of explorable explanations and animated diagrams. Scrollytelling is used throughout the article to keep visual elements in the viewport while reading and to alter the appearance of said elements depending on the scroll position.   The first visualisation demonstrates how the disease spreads assuming different reproduction numbers (R) and allows the reader to explore how infections unfold by interacting with sliders to manipulate levels of immunity and isolation. It is based on a custom (but really simple) model that runs once when the page is loaded. The visual representation is implemented with p5.js.   The main part of the article consists of animated diagrams that all use the same custom-build visualisation engine capable of displaying nodes, links, and annotations in a polar coordinate system. This allowed us to easily position and animate elements by providing the distance and angle from a common reference points rather than x/y coordiantes.   The website is implemented with Vue.js and <a href=""https://github.com/fidelthomet/privacy-preserving-proximity-tracing/"">published on Github  under an open license. "," A major difficulty was that we had to develop our scrollytelling in sync with the actual development of the app.During our conception, there were ongoing discussions at the German and European level about whether to go for a centralised or decentralised approach, which was central to our project. On top of that, the publication of the app became several times.   Another challenge was the academic context in which the discussion about the specific functionality of the app took place.We had to fight our way through highly complex and constantly changing technical documents and papers. All this with the task of reproducing them as simplified as possible, but still correctly. "," First and foremost, they can learn that the use of interactive and visual formats are much more reader-friendly and allow for a higher degree of editorial creativity. We thus managed to convey the complex technical concepts and interrelationships in short and easily understandable text segments.   Transparency also plays a major role. Especially in this day and age, it is elementary to reference plausible sources that any reader can easily verify. We also believe it is helpful to publish the source code as well. ",https://tracing.ft0.ch/,https://pudding.cool/process/pudding-cup-2020/,https://www.berliner-zeitung.de/zukunft-technologie/die-funktion-der-warn-app-modern-erklaert-li.87342,https://twitter.com/JuerMueller/status/1272086508409929729,https://github.com/fidelthomet/privacy-preserving-proximity-tracing/,,,,"Thomas Haas, Fidel Thomet", Thomas and Fidel are based in Berlin and are both enrolled in the Urban Futures Masters programme at University of Applied Sciences Potsdam.   Thomas is an urban designer and writer focusing on contemporary perspectives. He works with Urban Catalyst Berlin and writes freelance for various magazines and publications.   Fidel is an interaction designer with focus on data visualisation and web-enginieering. He is a research associate at the visualisation research group ᴜᴄʟᴀʙ and a former Google News Lab Fellow at Frankfurter Allgemeine Zeitung. ,30 May 2020
United States,The Texas Tribune,Big,Texas coronavirus cases: Latest updates,"Investigation, Explainer, Database, Infographics, Chart, Map, Health","Scraping, D3.js, Json, Google Sheets, Python, Node.js"," The Texas Tribune is tracking how many people have tested positive for the novel coronavirus in Texas each day. From March to December, more than 1.5 million people in Texas have tested positive for the virus and at least 27,000 have died. The burden of the virus is not spread evenly across the state, and our tracker’s charts and maps make it easy for readers to see how the situation is unfolding in their area. It is our goal to make this data as simple to understand as possible, because it is so important. "," What distinguishes the Tribune’s coronavirus tracker from our competitors is its clear design and its focus on accountability. It is our goal to make this data as simple to understand as possible for our readers, most of whom (like us!) are not epidemiologists. As Texas rolled back its restrictions and cases surged multiple times in 2020, we wanted a clear, reliable place for readers to understand the situation and how state leaders’ decisions impacted their personal safety.   The Texas Tribune’s case tracker was the Tribune’s most-visited story of 2020, with 2.65 million page views from 834,000 users from April through December. Users also spent more than more than three minutes on the page, which is more than double the average time spent on all other Tribune stories.   We know that the story attracted a very loyal audience, because a large portion of readers get to the page via direct links, indicating they’ve bookmarked the tracker and check it regularly. Data from the tracker also led to dozens of related stories, and the tracker charts were embedded across our site and others. Charts from the tracker were republished by news organizations across the state. "," This story was created by a team of data reporters and developers, and updating the story every day requires innovation and creative storytelling techniques. When updating we use a Python scraper to fetch the state’s data from multiple sites and insert it into our own database. Then, we use that data to publish a page with interactive maps and charts built using D3, a javascript charting library. All of this is based on the Tribune’s open-source, dynamic page publishing platform. Because the data changes every day as the pandemic situation changes, the charts must be dynamic and flexible. The maps and charts can also be embedded in Tribune stories or by our media partners across Texas. "," Throughout the pandemic, the state has repeatedly changed the way it reports the data, and the team of journalists on this story responded to those changes on the fly. For instance, the state added new, more accurate data as pandemic upfolded, as well as introduced errors that needed to be explained to readers. We rebuilt our data scrapers and redesigned the presentation of the tracker again and again as the story — and the virus — evolved. This story also requires a high level of team coordination and communication to pull off updates every day of the week for nine months. We take turns and share the load, and we all have the state health department’s press officers on speed dial to resolve frequent problems with the data on their end. "," A data tracker that updates every day and has no set end point is a huge undertaking — especially when it features data that is brand new to everyone involved. Everyone on the team producing the story needs to be ready to collaborate, adapt and support each other.   ",https://apps.texastribune.org/features/2020/texas-coronavirus-cases-map/,https://web.archive.org/web/20200415030216/https://apps.texastribune.org/features/2020/texas-coronavirus-cases-map/,https://web.archive.org/web/20201229180827/https://apps.texastribune.org/features/2020/texas-coronavirus-cases-map/,,,,,,"Chris Essig, Mandi Cai, Carla Astudillo, Anna Novak and Darla Cameron"," Chris Essig, Mandi Cai, Carla Astudillo, Anna Novak and Darla Cameron are the data visuals team for this project at the Texas Tribune, a nonprofit newsroom covering state politics and policy. They use data reporting, web development and data visualization tools to tell stories about Texas. ",14 Apr 2020
United Kingdom,Financial Times,Big,Covid-19: The global crisis in data,"Explainer, Long-form, Cross-border, Infographics, Chart, Map, Health, Economy","Animation, D3.js, QGIS, Json, Adobe Creative Suite, CSV, R, RStudio, Node.js"," As part of a major series examining whether the world could have been spared from hte pandemic, the Financial Times Visual and Data Journalism team compiled and analysed data from around the world to produce a definitive, visually immersive analysis of the Covid-19 pandemic. Presented as a series of chronological chapters, the story used cutting-edge web design and data analysis/visualisation techniques to take readers on an authoritative journey from Wuhan, China in January, through to Europe's second wave in October. "," As part of the FT's key coverage of the crisis, the story was made free-to-read and achieved outstanding reader engagement metrics. Most importantly, with an average viewing duration of over six minutes across hundreds of thousands of page views, this piece engaged and informed our readers. It was widely shared to enormously positive reviews on social media, including by one member of the UK government's own Covid-19 science advisory committe (SAGE).   We delievered a compelling, tightly integrated combination of rich storytelling and in-depth analysis. The piece was cited by noted SEO expert Rand Fishkin as an example of '10x content' - ""content that is at least ten times better than the next best piece of content available online on that same topic"".   The story concentrated on describing paradoxes of the crisis or debunking myths including     The false trade-off between protecting citizens and protecting economies   How China contained the virus amid the miggest mass human migration on the planet   How nowhere in the US is like the US (the problem of aggregates)    Each one of these chapters, supporting by data and visuals, was crafted to better inform the public about the biggest threat to global health in a century. We think we succeeded - the piece became a much-cited reference point in critical debates in the early weeks of Europe's second wave.     "," Firstly, I want to give some emphasis to the design process ahead of the tools used.   The story was designed from the ground up to provide a positive user experience, led by our User Experience Editor. We agreed in early team design sessions that we wanted to give the reader 'intelligent incentives to keep scrolling'. Care and attention was given to every user interface element and how it would support the story. For example, an autoplaying video topper on the page was created to generate an emotional entry point for the reader based on particularly memorable moments from an unforgettable year. Meanwhile, a scrollytelling intro that provided some tension/teasing ahead of the detail that was to follow. Then, a scrollytelling approach deliberately connected each unfolding regional chapter to the bigger picture of the global death toll.   Next I want to talk about how we used data.   Data is often accused of being 'sterile', but we wanted the piece to generate an emotional response. We achieved this through careful design, not just with the intro, but with elements such as the gradual animated unfolding of the central streamgraph's horrific death toll and the use of 'active titles' on each chart: data became visual rhetoric.   In terms of tools, we used an understandably broad range: the page itself was constructed with customised React and D3; R, QGIS and Adobe Illustrator were used to create the inline graphics within chapters, which were then rendered as SVG on the page. Video elements were prepped with Adobe AfterEffect/sElements. Zeplin was used for bespoke page design. "," Without doubt, the hardest part of this project was planning and executing a hugely ambitious piece of work around a story that was still evolving at a rapid pace.   Our solution to this challenge was to run a parallel development process with user experience and page design being carried out in parallel with story development: Early in the project, the entire team worked together to agree a narrative vehicle for the piece: that it would be a combination of chronological and geographical. With the design work addressing that broad remit, individual chapter authors were then drawn from across the Visual and Data Journalism team who would research, visualise and write individual sections of the story. Earlier chapters were created first, providing a model for later chapters to follow. This approach meant that, althoug the piece took around two months from conception to publication, it still incorprated up-to-date information and analysis at the time it was published.   Another big advantage to the project's workflow was that it drew on work that had already been carried out by the team earlier in the crisis;     The prominent 'streamgraph' that provided the visual spine of the story — the terrible human toll of the crisis — drew data directly from the team's coronavirus tracker project.    The individual chapters were produced by data journalists who had already been intimately involved in data reporting on the pandemic from its early stages    We feel that the piece exudes a confidence that was created by six months of constant effort tracking, analysing and reporting the biggest global story of this century.     "," That it pays to put charts and maps on a page first and write around them! So often, newsroom workflows are dominated by an intertial compulsion to produce copy first, with graphics to follow as decoration. We feel that the cohesive narrative presented by the 2,000+ words in the story was entirely down to the way that the graphical analysis was used to compose the chapters and broader narrative structure of the piece first. The words followed — and were more confident because of it.   Having said that, we also feel that this piece reflects how important it is that data journalists should be encouraged to actually  write  longer stories — all too often, data jouranlists are thought of as 'the graphics people'. In this piece, the words were finely crafted by writers who knew the data best — and it shows. It was particularly pleasing that entire sentences from the story were shared on social media almost as much as the graphics were, proving this was a story with substance and something to say. ",https://ig.ft.com/coronavirus-global-data/,,,,,,,,"Steven Bernard, John Burn-Murdoch, Tom Hannen, Bob Haslett, Caroline Nevitt, Jane Pong, Ændrew Rininsland, Alan Smith, Martin Stabe, Cale Tilford, Aleksandra Wiśniewska, Claire Manibog"," Steve Bernard: Senior Visual Journalist at the FT, a specialist in cartography, data visualisation and video production.   John Burn-Murdoch: The FT's award-winning Chief Data Reporter, previously with the Guardian.   Tom Hannen: Executive producer for video at the Financial Times, previously worked in BBC Global News   Bob Haslett: Visual journalist, fond of design & typography. And whippets.   Caroline Nevitt: The FT's User Experience Editor, formerly of Wolff Olins   Jane Pong: Now at Bloomberg, Jane was the FT's data visualisation specialist in our Hong Kong newsroom   Ændrew Rininsland: Senior developer. Before starting at the FT in 2016, he was an interactive journalist at The Times and Sunday Times.   Alan Smith: Leads the FT's newsroom team of data reporters and visual journalists.   Martin Stabe: The FT's data editor, studied journalism at City University of London, joined the FT in 2010 after stints with Press Gazette, Retail Week and Drapers.   Cale Tilford: Senior newsroom developer started awho started on the visual and data desk in 2018. Applies computer science to data journalism.   Aleksandra Wiśniewska: Visual projects editor at the FT. Joined the newsroom in 2014 as a graduate trainee.   Claire Manibog: The series producer, a digital editor specialising in multimedia storytelling.     ",18 Oct 2020
United States,"ProPublica, The New York Times",Big,New Climate Maps Show a Transformed United States,"Explainer, Multiple-newsroom collaboration, Database, News application, Infographics, Chart, Map, Environment, Agriculture","D3.js, QGIS, Json, Adobe Creative Suite, PostGIS"," We analyzed data from the Rhodium Group and a study published in the Proceedings of the National Academy of Sciences to show how climate change will profoundly change the way we live in the United States by mid-century. The never-before-seen data shows how heat and humidity will push the South and Gulf Coast almost to be almost unlivable, while the upper midwest will become a more ideal place to live and farm. Together this is one of the most complete views of what our climate future looks like. "," This project was one of our most-viewed single pages on the website all year. We’ve heard from many, many readers that by stacking the information in an additive way -- showing how these individual problems will all affect the US in different ways, together -- we’ve driven home a key set of problems in the climate crisis that hadn’t been as clear before.  "," We used QGIS and Postgres/PostGIS for data analysis, and d3, Svelte.js, Illustrator and Photoshop for presentation. "," The hardest part of this project was figuring out how to combine seemingly disparate climate data sets into a complete portrait of the future of climate in the United States. From temperature to sea level rise to economic damages in dollars to the very concept of a ""niche"" that represents the ideal human habitation zone, we needed to show how individual counties ranked across the criteria. To solve this, the graphic includes three different visualizations: a globe, a set of county maps and a ranked table to let the reader explore these variables individually and then how they stack up together. Rather than attempt to create a combined index, the sortable table lets readers see how certain variables work together to amplify climate risks. "," A big lesson that can be learned from this piece is that organizing and designing data visualizations from existing datasets effectively can be more impactful and revelatory than publishing wholly new datasets. While this project did have some exclusive data, the lead visualization and a number of county datasets were surfaced from existing sources. The niche dataset had been out in the world as a paper, but had not been translated into an effective visual yet. We worked with the paper's authors to surface parts of the work most applicable to the US to create a piece that grabbed millions of eyeballs.  ",https://projects.propublica.org/climate-migration/,,,,,,,,"Al Shaw, Abrahm Lustgarten, Jeremy W. Goldsmith"," Al Shaw is a senior news applications developer at ProPublica where he uses data and interactive graphics to cover environmental issues, natural disasters and politics.   Abrahm Lustgarten is a senior environmental reporter at ProPublica, with a focus at the intersection of business, climate and energy.    Jeremy Goldsmith is a Geographer. He specializes in Cartography and GIS Analysis/Processing. His experience centers around clean, detailed, and informative maps. ",15 Sep 2020
Singapore,Reuters,Big,How Joe Biden won the U.S. presidential election,"Explainer, Infographics, Chart, Map, Elections, Politics","Scraping, D3.js, Google Sheets"," Democrat Joe Biden captured the U.S. presidency by expanding his party’s appeal among suburban voters, in middle- and upper-income communities and in places where a large share of people graduated from college. On the Republican side, Donald Trump made surprising gains among Latino populations in South Florida and in Texas along the border.    Reuters looked closely at these margins of victories across demographics, population densities, and historical outcomes to shed further light on the changing electoral landscape of the U.S. "," This project was widely viewed on Reuters.com and shared across social media. A number of news organizations made similar “peak” maps to look at some aspects of voting in the wake of the election, but this page was unique in its comprehensive look across multiple vectors in an easily digested scrolling story. "," This project benefited greatly from foresight of Jason Lange, who wrote a Python script to combine live county-level results with Census, Labor Dept. and other data sources for filtering. We would not have been able to turn around this sophisticated analysis without his planning and preparation.   The map was built with D3.js. The text was fed from Google Docs using the ArchieML markup language, which allowed us to quickly write and edit the piece. ", The hardest part was building it while preparing for the election. The same team that built this page was also responsible for live election results and other stories that ran before and after the Nov. 3 election. We had the basic frame built the day after the election and were able to write our analysis based on incoming results up until Nov. 7 when the race was finally called. ," When you do your job well, a complicated story can be easy to read. We tend to like charts with multiple layers of complexity, but it’s important to keep the reader in mind and break ideas down into digestible pieces, highlighting the aspects fo the data that speak to each idea as you flow through a story. All of the maps on this page are technically contained in the overview map, but by filtering and highlighting with text we can focus the reader on the bits of data that illuminate aspects of the story. ",https://graphics.reuters.com/USA-ELECTION/RESULTS/jznvnjyjzvl/,,,,,,,,"Chris Canipe, Gurman Bhatia, Jason Lange and Brad Heath", Chris Canipe is a graphics journalist with Reuters   Gurman Bhatia is a graphics journalist with Reuters  Jason Lange is a politics data reporter with Reuters  Brad Heath covers crime and justice for Reuters ,7 Nov 2020
Germany,"Der Tagesspiegel, FixMyBerlin",Big,Straßencheck,"Solutions journalism, Open data, Infographics, Environment","3D modelling, D3.js, Json, Python"," Although mobility revolution has been politically proclaimed after the 2016 election in Berlin, it still shows little on the streets. When bike lanes are built, cyclists often complain, that these lanes have little to do with their needs. That is partly because traffic planners have hardly ever conducted structured surveys of what types of bike lanes feel safe. Motorists and pedestrians haven’t been asked what kind of cycling infrastructure feels safe for them. The Tagesspiegel Innovation Lab teamed up with the startup FixMyCity and researchers to build a visual intuitive survey that allows to evaluate different types of road design.  "," More than 22.000 people participated in the survey, which was only possible thanks to the cooperation of the daily newspaper and experts, mixed with a playful design of the articles and the survey. The survey was shared nationally and internationally on social media.     The results attracted international attention and have since been as hotly debated in public as among experts. Furthermore, they became part of planning concepts between politicians and traffic planners. The generated data was published as open data for city planners to work with.   "," For the survey, thousands of photorealistic renderings were automatically generated by the research project. These simulated scenes all presented different combinations of bike lane features – such as width, color, demarcation or the position of parking spaces or amount of traffic on the street. This guaranteed that the individual factors could be separated in the evaluation afterwards.     For the data analysis we used Python pandas next to several other kinds of self-developed algorithms. Both teams worked together on the analysis – while working in close exchange with research experts and planners.   "," Straßencheck is a good example of how science and journalists can benefit from collaborations. While FixMyBerlin brought the biggest part of expertise, our team gave a lot of input on the different scenarios from a user's point of view.     It was a challenge to show more than thousands of images without boring the reader. Therefore, we were able to use our experience in UX-design and presenting complex data analysis to our readers. I the end, we successfully found an approach of gamification for a scientific survey, which is proven by the result of the survey- thousands of readers clicked through thousands of images.    The constant exchange over the entire duration of this longterm project allowed the project to balance between technical accuracy and the interest of the general public.   "," Even if journalists and scientists do not always have the same goal at the beginning, it is worth creating joint projects. Both sides benefit from the knowledge of the other. Journalists get exclusive results, scientists in return a great platform to present their research.  ",https://interaktiv.tagesspiegel.de/lab/strassencheck-ergbnisse-diese-strassen-will-berlin/,https://interaktiv.tagesspiegel.de/lab/strassencheck-das-stoert-im-berliner-verkehr-am-meisten/,https://interaktiv.tagesspiegel.de/lab/strassencheck/,,,,,,"Martin Baaske, Manuel Kostrzynski, Hendrik Lehmann, David Meidinger, Michael Gegg, Helena Wittlich, Nora Binnig, Vincent Ahrend, Boris Hekele, Heiko Rintelen, Felix Sistenich, Stefan Freudenberg, Philipp Schiedel, Tümer Tosik, Webkid, David Wegner","    Tagesspiegel Innovation Lab  develops new storytelling formats to explore the immense potential of digital journalism. In addition to the development of new modes of presentation such as interactive graphics and mixed media stories, the team is initially focusing on citizen research, data analysis, sensor journalism and evaluations with the help of machine learning. The Innovation Lab is staffed with software developers, editors and experts in artificial intelligence.      FixMyBerlin  is a project by the Berlin FixMyCity team which is part of the city's own innovation laboratory, researching the digitalization of Berlins administration and possibilities of the urban infrastructure for tomorrow. FixMyCity supports cities transforming into a modern and open administration with a focus on sustainability. The overall goal is to have an open and constructive dialogue between administration and citizens: to create inside. Behind FixMyCity is a team of developers, transport planners, designers and data specialists.  ",6 Jun 2020
Portugal,Público,Big,What are the coffee shops and restaurants 300m from schools? Check out the map,"Breaking news, Database, News application, Map, Business, Health","Scraping, Json, Google Sheets, R, RStudio"," At the start of the school year, and amid the covid-19 pandemic, the Portuguese government ruled that every coffee shops, bakeries, and restaurants at 300 meters by any school would have a limit of four people by a table. But how many businesses were affected by this rule? No one seemed to know - not even the government.   Scrapping a geolocated database of Portuguese schools, and using Google Places API, we manage to find out that at least 21 thousand businesses were affected. ", This story was very successful because it answered the questions that the government was not able to answer when they announced the law: how many businesses were affected by it and how could a business owner (and their clients) know if that business was under that rule. ," The first challenge was to find a geolocated database of all schools and universities in the country. I had a list of all schools and I've added the university campus to it and was ready to geolocate it all. But then I've found that the Portuguese government <a class=""editor-rtfLink"" href=""http://www.igefe.mec.pt/PesquisaRede"" style=""color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;; color: #4a6ee0;"" target=""_blank""><u>had that database   but you had to use a complex system of dropdowns to get the geolocation of any school. Fortunately, I was able to find the json file that was feeding that website which had all the coordinates for all schools. Using R, I've converted that json file to a data frame.   Because this website lacked all college institutions, I had to use <a class=""editor-rtfLink"" href=""https://www.dgeec.mec.pt/np4/38/?form"" style=""color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;; color: #4a6ee0;"" target=""_blank""><u>another database   and then geolocate all of them by hand.   With all the data loaded, I've used Google Places API to request all coffee shops and restaurants 300meters close to any of those coordinates. Because the API returned results that were not 300 meters close to that point, I then calculated the euclidean distance to the point, filtering out those cases that were too far away.   Then, while a reporter contacted some of the businesses that were affected by the rules, I built an interactive map using Mapbox that allowed the readers to explore the business affected by the law. "," Time and the bill. This was a kind of breaking news story and we had to answer fast to it. Finding all schools coordinates took me half a day, but I wasn’t considering the time that asking that data would take me.  I also had a little problem with Google Cloud. I was using the free credit they give you when you create an account and was expecting it to only give me the results that were precisely 300m from those points. After doing quick math based on the results from the first 200 schools, I thought that I would hardly pass that amount. But I forgot that the big cities were not yet collected, where there are more schools and more business - also some businesses were under the radar of two schools, which meant that I would have to delete those duplicates later - but they counted as an API request anyway. It was 4 am when I went to sleep, expecting the data to be all collected in the morning. It was indeed, but with a 300 euros bill. "," This kind of project is the kind that shows pretty well how data journalism can work on breaking news and how a data journalist's brain works. Most people thought it would be impossible to measure how many businesses were affected, but I thought about the possibility of using the school's coordinates and Google Places API to answer it. Of course, it is an estimate - I was not able to request Bars and Bakeries because of my bill problem. And we need to take into consideration the fact that there’s a percentage of businesses that are not on Google Maps. ",https://www.publico.pt/interactivo/quais-cafes-restaurantes-300-metros-das-escolas-consulte-mapa,,,,,,,,"Rui Barros, Claudia Carvalho Silva, Inês Moura Pinto"," Rui Barros is a data journalist/ journocoder/ news nerd currently working at PÚBLICO, a daily newspaper in Portugal. Being the only data journalist in the newsroom, he does everything from doing data-driven investigations, news applications or simply helping someone by scraping a website.  He uses R - mostly the tidyverse family of packages - to do everything data-related and uses HTML, CSS, and JavaScript on his interactive works and data visualizations.     ",15 Sep 2020
"Hong Kong S.A.R., China",Initium Media,Small,The Mass Prosecution,"Investigation, Explainer, Human rights","Animation, D3.js, Json, OpenStreetMap, Node.js","   The anti-extradition bill movement has brought about an unprecedented governance crisis that has completely transformed Hong Kong. Freedoms were stripped from people overnight in the huge wave of prosecutions that followed. Society began to cast doubt on the fairness and independence of the judiciary.      By cross referencing court records, news reports, and police numbers, we at The Initium Media produced an interactive documentation of this mass prosecution.      We also conducted in-depth interviews with those prosecuted and their lawyers, shining light on the pressures they face after being arrested, and issues such as excessive police powers and injustice.  ","   Initium Media targets the readers coming from Hong Kong, Taiwan and Mainland China who may have different impressions or attitudes towards the protesters in the anti-extradition bill movement. The access-free interactive website depicted a whole picture of the arrest and prosecution the protesters faced.      Most of the interviewees had concerns about confidentiality and wanted to be anonymous. It was a challenge for the storyteller. What we chose was to record their voices and merge their stories with words, figures, and graphics like an online Human Library.       Unlike the traditional practice, the project did not just focus on the personal experience of the interviewees which may be a bit biased, but tried to balance with a thorough explanation of the legal proceedings and supporting data. For instance, one interviewee had disclosed the verbal abuse of Hong Kong police. In order to show that was not a tip of the iceberg, our team had recorded all the complaints the counsel filed in the courts.       The impact of such a balanced and organized way to tell a story with both emotional elements and objective data would well convince the readers and challenge their prejudices.  "," The project is based on React.js and Webpack, using Sass as the CSS preprocessor. As a data-driven project, it chooses D3.js to create diagrams and infographics. The data is imported in the format of Json. The map layout is created with leaflet.js. Also, with the help of gsap, some infographics become animated and interactive.","   Data Collection is the hardest part of the project. The process of acquiring data was full of hindrances. The project intended to give the readers a whole picture of how the protesters of Anti E-lab movement undergo legal proceedings and comprehensive analysis of the cases.       While the Hong Kong judiciary erases the court list within 2-3 days' time. Firstly, we have to manually archive the documents on a daily basis in order to keep a tracking record. The information we needed for the project was scattered. There was no shortcut to gather the relevant information. What we did was to enter every single data from the court list and several media reports to build an all-inclusive database.       Moreover, the government refused to provide our team with certain figures, though the information was generally regarded as not sensitive. For example, more than half defendants were being charged with riot. But when and at what occasion were they being arrested? Unfortunately, there is no formal Archives Law in Hong Kong but only a non-binding ‘Code on the Access to Information’. Our journalist had once proceeded with an application; in the end, the officials still refused to disclose the information. Our team had to cross-check with all cases reported in the newspaper day by day to figure out the data we needed.       Hong Kong is now full of tension and distrust. Under such circumstances, it was extremely difficult to find interviewees than in the normal time. Most of the protesters had concerns about confidentiality even though they wanted their stories to be heard. Our journalist had been to different magistracies and contracted several protesters directly to search for a story that can represent a relatively comprehensive picture.  The efforts our team had devoted to this project should be well recognized.  "," The project embodies what ‘never says never’ is. The team had made a great effort to break through the limitations. Data collection is never an easy task, especially in the times that the government is holding a hostile attitude towards the media. What we can choose was either to pick the incomplete or even biased information from the officials or to find another way to develop our own database to convey a comprehensive angle of view. We journalists shall never give up on asking more and more details even if the path is much more difficult.       Another breakthrough is to turn the crisis into opportunity. When the interviewees rejected to disclose their identities, it was extremely hard to tell a good story with only texts and images. We then made use of sound as an impressive medium, in a way to balance the concern of confidentiality but still iIndulge the readers with the personal experience of the interviewees.    The skills may not be innovative for other journalists but the attitude to think out of-the-box is the spirit which the industry is looking for. ",https://theinitium.com/project/20200724-hongkong-anti-elab-movement-prosecutions/,,,,,,,,"Gemini Cheng Pui Shan, Irene Chan, Kexin Lin, Lam Chun Tung, Victoria Jin, Tseng Lee-Yu"," Initium Media, debuted in August 2015 and headquartered in Hong Kong, is Hong Kong’s Largest Native Online Media Company as measured by numbers of professional journalists. Initium Media has been widely acclaimed by international media industry and is the first Chinese media partner of The Wall Street Journal. Initium Media is also a multi-award winning including SOPA and Human Rights Press Awards.      Initium Media, founded in December 2014 and headquartered in Hong Kong, is a media company serving the Chinese-speaking population worldwide. We will launch mobile applications, websites and social media platforms in August 2015.  ",24 Jul 2020
Singapore,Reuters,Big,Assessing Australia's ecological disaster,"Investigation, Explainer, Illustration, Infographics, Map, Satellite images, Environment",Adobe Creative Suite," Australia’s government called the bushfires crisis of 2020 “an ecological disaster.”  Reuters delivered the first data-driven analysis of fires and habitat data, showing how hundreds of species suffered.    We processed massive amounts of satellite-derived fire data and habitat information. By calculating the intersection of those datasets we were able to reveal the animals hardest hit by bushfires.     Many species, including some that are critically endangered, have seen large swathes of their environment destroyed. Some of these species have had the majority of their territory wiped out, raising fears of extinction. "," This was the most in-depth analysis of habitat damage published at the time. Other news organisations and government agencies were publishing estimates and approximate headline figures, but we were able to give a detailed account of every animal individually.    After publication we were contacted by individuals and wildlife organisations asking about access to the raw data and analysis. It was seen as important information as attention turned to rebuilding habitats and protecting the species that were most vulnerable. "," We processed massive amounts of satellite-derived fire data and slowly built up our own “burned area” analysis. We then took more than 1,400 habitat spatial files and ran batch calculations in order to find the intersection of the two data sets, revealing the burned percentage and acreage of each habitat. This exclusive dataset showed us which animals were hardest hit by bushfires.     There was also a large amount of detailed cartography, satellite imagery analysis, and hand drawn illustration to tie the whole piece together as an immersive experience. ", The preparation and processing of all of the data was a monumental task. There was a lot of work to do before processing the calculations. ," This heavy data journalism exercise could have been presented as a straight forward exclusive story, but this level of production and the visual experience helped bring the data to life. ",https://graphics.reuters.com/AUSTRALIA-BUSHFIRES-WILDLIFE/0100B5672VM/index.html,,,,,,,,"Simon Scarr, Manas Sharma, Marco Hernandez"," The Reuters graphics desk publishes visual stories and data visualisations to accompany Reuters news coverage. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and produces many of the visual stories published. ",21 Jan 2020
Singapore,Reuters,Big,The Korean clusters: How coronavirus cases exploded in South Korean churches and hospitals,"Explainer, Infographics, Map, Health","Adobe Creative Suite, Microsoft Excel, CSV"," In February 2020, South Korea announced thousands of coronavirus cases in the space of only a few days, an outbreak that initially pushed South Korea’s tally of confirmed cases much higher than anywhere else outside of China.   Reuters manually combed through daily government press releases to build a data-driven account of the spread and the emergence of patient 31, a single super spreader believed to be the source of thousands of infections. "," Our forensic reporting showed how one person could have a ripple effect in spreading the novel coronavirus. The story was lauded because of how we were able to explain the chain of transmission early in the pandemic.    The piece went viral far outside of Asia, with many on social media in the U.S. sharing it as an example of what can happen if social distancing isn’t adhered to. ", The Korea Centers for Disease Control & Prevention put out a detailed health bulletin every day. We had to sift through and look for specific details piecing together how “patient 31” became the inflection point for the virus in South Korea.    We were then able to build custom visualisations in the browser locally which could be exported and styled in Adobe Illustrator and placed within the story page using ai2html.     ," This story was built on manual data collection and old school reporting. Although not alien to our team who often rely on computer assisted reporting, it was an equally rewarding experience to handle a project in this way. Technology helped us piece some of the parts together and visualise what we were seeing in the data but whiteboards and markers were definitely involved too. "," Press releases and health bulletins may look mundane at surface level, but are often packed with valuable information which can be reworked to show patterns or reveal stories. ",https://graphics.reuters.com/CHINA-HEALTH-SOUTHKOREA-CLUSTERS/0100B5G33SB/index.html,,,,,,,,"Marco Hernandez, Simon Scarr, Manas Sharma"," The Reuters graphics desk publishes visual stories and data visualisations to accompany Reuters news coverage. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and produces many of the visual stories published. ",20 Mar 2020
Bosnia and Herzegovina,OCCRP,Big,Europe's COVID-19 Spending Spree Unmasked,"Investigation, Explainer, Solutions journalism, Cross-border, Multiple-newsroom collaboration, Database, Open data, News application, Fact-checking, Infographics, Map, Corruption, Economy","Scraping, Json, Microsoft Excel, Google Sheets, CSV, PostgreSQL, Python","   As countries across Europe fought the COVID-19 pandemic and rushed to obtain critical supplies — such as PPE and ventilators — many countries suspended their usual public procurement rules, resulting in billions of euros of spending largely hidden from the public. Along with media partners in 37 countries, OCCRP followed the money and collected information from over 37,800 COVID-19 related tenders and contracts worth over 21 billion euros (U.S. $24.9 billion). The data gives an unprecedented view of just what Europe’s governments have been spending their billions on — and where things may have gone astray.  ","   The project was the first collaborative effort by European journalists to pull together COVID-19 procurement data. It revealed that the majority of the continent's known procurement deals were signed as direct awards without tender, and that large, established companies were the major beneficiaries of the deals. The data also discovered massive discrepancies in prices paid for key items such as FFP2 masks, and identified countries — such as Ukraine and Czechia — where the prices paid varied most wildly.  The project also comprehensively documented which countries have been the most transparent about sharing data, and which countries have remained “black holes” for COVID-19 procurement information.      After publication, the authors were invited to speak on various events ranging from journalistic seminars through procurement specialist calls to the annual Transparency International Anti-Corruption Conference. At these occasions, colleagues could access the data and grievances were shared about the lack of transparency over irregular public spending (little competition) with expert audiences. One of the authors, Adriana Homolova, was also approached by the Dutch government to help design a more transparent system for public contracts publication, a project that was already in the pipeline of the government. Adriana is still being approached to speak about the data at universities and with procurement and open data researchers. The data collected will also be integrated into OpenContracting's “Emergency Procurement Explorer.”  ",   We used Python and Jupyter Notebooks to write scraping scripts for various public procurement websites and for data cleaning. This data was then fed into a Google Spreadsheet that was openly editable by anybody from the team and in the end was also used for data publication. Data visualisations for the publication were done with Flourish. Aleph was used to upload and search contracts and documents that were not public before.  ,"   The hardest part of this project was its sheer size. This project involved the coordination of dozens of individual journalists, all working to balance the demands of their own newsrooms, wrestling with a massive amount of data. Journalists stayed in regular contact via the Signal app, and over a period of months collaborated on a strategy to obtain and process extremely large and diverse data, regularly updating their findings in central spreadsheets and a wiki. This could have easily collapsed into chaos, but a core team worked diligently to make sure all data was properly inputted and the team was kept abreast of updates. Instead, the project managed to keep going thanks to a spirit of public service and constant communication and feedback from all involved. As for the data, the hardest part was to standardize very different data sources (official websites in many countries, FOIA requests, data leaks, TED data etc) into one coherent data set. This also unfortunately made it nearly impossible to keep it up to date properly.  ","   This one-of-a-kind trove of data gives journalists a starting point to further investigate government spending in their own countries. Compiling this data in one place allowed reporters to compare government spending and ask questions that held their governments to account: Why did mask prices fluctuate so wildly from country to country? Why isn't my government releasing this data when others are? OCCRP encouraged journalists to investigate COVID-19 related records in their countries and offered original source documents and assistance.          Information about government procurement is notoriously arcane for reporters to analyze: not only is the data itself incredibly complex in its structure (a tabular version of TED awards, the European procurement system, might have 150 columns), but it also requires a deep understanding of the legal and political process required for public institutions to spend billions of Euros. The pandemic compelled governments to deviate from their legal rules in ways that still needed to be proportional to the severity of the emergency. Holding power to account in this context requires a tight integration of data analysis and policy analysis.      We hope that now we’ve learned how to analyse EU procurement data in a crisis that put a special spotlight on the process, we can also encourage more reporters to dig into the data.  ",https://www.occrp.org/en/coronavirus/europes-covid-19-spending-spree-unmasked,https://www.occrp.org/en/coronavirus/interactive-and-map-europes-covid-19-procurement,https://docs.google.com/spreadsheets/d/1VXURZlKH-_GeNvPrytgJOeTUH3hXf0r_veIXWJp1K20/edit#gid=0,https://docs.google.com/spreadsheets/d/10VL5FpviSXctagcoQM_pr0xP4Lsmzzc3-i7mEyCE2kw/edit#gid=0,https://docs.google.com/spreadsheets/d/1lsaA4Dy5Tivg0ZV8J2u_AguIXFcKA4aeE_EdbdruKtM/edit#gid=0,https://docs.google.com/spreadsheets/d/13-Ml9LjRnerOXZ8MJRvTAimSZXwd2xWTNE97gVIoWbA/edit#gid=0,https://www.occrp.org/en/coronavirus/in-europes-scramble-to-buy-COVID-19-supplies-anti-corruption-measures-fall-away,,"Adriana Homolova, Dada Lyndell, Aubrey Belford with Sylke Gruhnwald (freelance, Switzerland), Ola Westerberg (freelance, Sweden), Michele Catanzaro (freelance, Spain) and Staffan Dahllöf (freelance, Denmark) contributed reporting.","   Adriana Homolova is a freelance data journalist, data analyst, and data skill teacher for international teams. Previously, she worked in multiple Dutch media as the nerd in the newsroom. She started analyzing procurement in 2013 and has taken every chance to do so ever since.           An award-winning data journalist, Dada Lyndell is an experienced trainer, specializing in data journalism and Open Source Intelligence methods. Lyndell works with OCCRP, training regional journalists in the use of open-source intelligence and supporting news investigations, with special focus on flight and ship tracking, satellite imagery, and other publicly sourced data.    Based in Kyiv, Ukraine, Aubrey Belford joined OCCRP in 2016 and is an Eastern Partnership editor. Aubrey covers global stories about the nexus of disinformation, crime, and corruption, and has led projects including <a href=""https://www.occrp.org/en/paradiseleased/"">Paradise Leased: the Theft of the Maldives , and <a href=""https://www.occrp.org/en/spooksandspin/"">Spooks and Spin: Information War in the Balkans . ",21 Oct 2020
United Kingdom,The Economist,Big,The Economist's covid-19 excess deaths tracker,"Explainer, Cross-border, Open data, Chart","Scraping, D3.js, R"," In early April 2020, we published the world’s first international comparison of covid-19 excess mortality, after tip-offs from people in Italy and France that official death tolls were undercounting the actual number of fatalities. Later that month, we published the first interactive tracking page for excess mortality, showing data for several countries. In May, we were the first organisation to publish all of this data on GitHub, along with our sources and code. We have updated the database regularly since then, while expanding our selection of countries. "," Excess mortality is the best way to compare the impact of covid-19 across countries. Varied levels of testing, especially in developing nations, can make official death tolls unreliable. In April, no organisation was collecting this data internationally.    So we decided to do it ourselves. We put the tracker in front of our paywall, to act as a public service. We also published all of our data, sources and code on GitHub. We wanted academics to trust our work, and to give them the tools they needed for their own research. According to <a href=""https://scholar.google.com/scholar?q=%22economist.com%22+%22excess+deaths%22+%22covid%22"">Google Scholar , our work on excess mortality has been cited in more than 120 academic articles.   This has shaped global awareness of excess mortality. After we published our tracking pages, other organisations (such as the New York Times and Financial Times) produced similar work. Governments also took notice. When we started out, only a handful published regular mortality data. Now, dozens do.    One notable example of our journalism encouraging greater transparency was in Mexico. The health ministry cited our tracker in official briefings (see this <a href=""https://twitter.com/jorgegavino/status/1303066030445604864"">link ). It then started to publish data in a similar format, replicating our heatmaps (see this <a href=""https://coronavirus.gob.mx/wp-content/uploads/2020/09/Bol_Excs_Mort_MX_SE31_09Sep2020_20h30-1.pdf#page=24"">link ). Mexico has one of the world's highest rates of excess mortality, which is 150% greater than the official covid-19 death toll. It would not have been possible to establish this without the health ministry releasing this data.    "," To gather the data, we used a combination of R scripts and lots of manual labour. For some countries, we could automatically download data and clean it into the right format. For others, we had to access spreadsheets by hand, or even copy charts from official websites into a machine-readable format.   Once we had data in clean CSVs, we trained regression models in R that could predict a baseline of expected deaths in each week or month of 2020, based on national trends from recent years.   The graphics were written with React and D3.js. We think that one of the things that sets our tracker apart from others published by our competitors is that we made every chart interactive. We believe that in a topic as complicated as this one it’s even more important to be transparent, so we added tooltips to every chart, showing expected and total deaths for each point. We also added a toggle to switch between deaths per 100,000 people and absolute figures.   The tracker has been changing too: at the beginning of the pandemic it was mostly a grid of line charts, but after doubling the number of countries we redesigned the page. We added a heatmap with country and regional data and a little explainer that walks the reader through the concept of expected and total deaths. "," There were several challenging aspects of this project. Gathering the data in the first place required lots of investigating of government websites, many of which are in languages other than English. We sent emails to several possible sources, and asked The Economist’s correspondents from around the world to help track down information. Wrangling the numbers from different countries into a consistent format took a lot of effort.   Refreshing the data and adding new countries has also proved tricky and time consuming. Throughout the pandemic, we have tried to keep the page as up-to-date as possible. We have also answered several queries from readers.   Even after spending dozens of hours making the data consistent, we spent a considerable amount of time getting the little details right on the visualisations. Most countries publish weekly data but some only publish monthly files. Some countries don’t have nationwide data available, only major cities. We ended up adding multiple footnotes and different code paths to account for these.    "," Hopefully this project has demonstrated the importance of publishing data and code on GitHub, since it has allowed academics to interrogate our work, and also to use it in their own research. This project has also shown the benefits of automating work with R scripts. Updating every single source each week by hand would have been very cumbersome; increasingly, we can gather most of the data by directly downloading CSVs from government websites. ",https://www.economist.com/graphic-detail/coronavirus-excess-deaths-tracker,https://github.com/TheEconomist/covid-19-excess-deaths-tracker,https://scholar.google.com/scholar?q=%22economist.com%22+%22excess+deaths%22+%22covid%22,https://twitter.com/jorgegavino/status/1303066030445604864,https://coronavirus.gob.mx/wp-content/uploads/2020/09/Bol_Excs_Mort_MX_SE31_09Sep2020_20h30-1.pdf#page=24,https://www.economist.com/graphic-detail/2020/04/03/covid-19s-death-toll-appears-higher-than-official-figures-suggest,,,"James Tozer, Martín González"," James Tozer is a journalist in The Economist’s data team. He contributes regularly to the Graphic Detail section, writing about any topic for which he can find numbers.    Martín González is an interactive visual journalist in The Economist’s data team. He makes interactive charts for the Graphic Detail section, with a special focus on politics.    ",4 Apr 2020
Serbia,KRIK,Small,"Database ""Judge who Judges"" (""Prosudi ko sudi"")","Investigation, Solutions journalism, Database, Open data, Fact-checking, Illustration, Politics, Corruption, Crime",Animation," Who are the judges who make decisions in the most important trials in Serbia? Why do many of those cases end up with the release of controversial businessmen, politicians, and criminals? How do judges progress in their careers, when they made illegal decisions, what property do they own?   KRIK's unique online database ""<a href=""http://prosudikosudi.rs/index.php"">Judge who judges ""  provides answers to all these questions.  Our new innovative database is the   only place  where you can read the complete profiles of 33 Serbian high-ranked judges , who act in the main departments for organized crime and corruption. "," Believe it or not, this is the first time that someone in Serbia has tackled this topic, no one before KRIK has dared to raise the issue of accountability and transparency of judges . And now,   it is finally possible for Serbian citizens to have insight in work of high-ranked judges, to have informed opinions as the first step in the fight against corruption in judiciary, since our readers can now “judge the judges” .   This investigation had a huge impact, on several levels – it has   launched a significant public debate on accountability of judges and transparency of their work and income . Findings from our database were republished dozens of times in other Serbian media, our reporters and editors were called to talk in several TV shows, dedicated exclusively to our investigation of Serbian judiciary. Database has attracted enormous public attention, so already in the first month since launching our database had half million of visits , which is remarkable result for a small country such as Serbia. Citizens were hungry for this information since they previously could not get data about judges anywhere and so they showered us with praise on social networks.   Some of the judges even subsequently contacted us to declare and explain their assets ! More importantly, <a href=""https://www.krik.rs/nakon-otkrica-krik-a-agencija-proverava-imovinu-sudije-savica/"">  state Agency  for the Prevention of Corruption has recently   launched its own investigation  <u> of the origin of huge property of one of the judges we wrote about (Zoran Savic) ,   precisely because we discovered the disproportion of wealth in relation to his income ! This state investigation is currently ongoing. "," In Serbia there was no such database which was dedicated exclusively to transparency of the work of judges. That is why KRIK’s database was made carefully and thoroughly, by digging and combining data from all available official sources and archives in one place .   Our Database now contains narrative information on seven different areas :   1) judges’ career and promotions, 2) court cases he/she worked on, 3) illegal decisions (verdicts) he/she made, 4) proceedings led against judges - disciplinary, misdemeanor, 5) reported and undeclared property (of course without publishing addresses), 6) judges’ professional engagement out of court and 7) compromising business alliances. All of these were collected from official sources and registers.   The process looks as following: the journalists first started to search basic information about the judges’: immediate family members’ names, potential companies, declared assets, political connections, and possible records of proceedings. One part of this is to do initial online research. Then we mostly had to communicate directly with the courts and other official archives, since most data about judges were not available online. We filed hundreds of requests for documents and data to Serbian courts and state registers .   Our team has done detailed analysis of every collected document. After that, we have contacted all judges which we investigated  and included their answers and reactions to our discoveries in their profiles in that database. "," As mentioned above, this is the first time that someone in Serbia has tackled this topic, no one before KRIK has dared to raise the issue of accountability and transparency of judges. Our database is new and unique, that is why it was especially difficult, first we had to convince judges to talk to us!     Despite the public importance of what they do, Serbian judges usually do not want to communicate with the media. Prior to the publication of the database, KRIK tried to interview all 33 judges, but most (two thirds) did not respond to the invitation. However, some contacted us later when they saw the database and gave us quotes, which we have added to their profiles.   Also, Serbian associations of judges were divided in commenting our database, ones stubbornly continued to defend their secrecy and  they had publicly attacked KRIK , saying that “ journalists have no rights to report about judges and their work ”, which is complete nonsense and testifies to their unprofessionalism. But the other Serbian association and several independent professionals pointed out how important it is to increase judicial transparency in such way as KRIK did. "," It took as more than a year to make this database and it was worth it, citizens were hungry for this data. It is always crucial to find a hole in a society that only quality investigative journalism can fill and then to be of service to its readers.      We will continue to update our database, KRIK team is immensely proud we have contributed to transparency in Serbian society! ",http://prosudikosudi.rs/index.php,http://prosudikosudi.rs/sudije.php,http://prosudikosudi.rs/predmeti.php,,,,,,"Bojana Jovanović, Stevan Dojčinović, Bojana Pavlović, Milica Vojinović, Jelena Radivojević, Marija Vučić, Vesna Radojević, Dragana Pećo, Miodrag Ćakić, Jelena Vasić and Snežana Petijević"," KRIK (Crime and Corruption Reporting Network) is the most popular investigative portal in Serbia. Our non-profit media was founded in 2015 by a small team of journalists, who for years have been engaged in exposing organized crime and corruption, and who have received many national and <a href=""https://www.krik.rs/en/krik-team/"">international awards  for their work. KRIK team is now known for its hard-hitting investigations, as well as for unique online databases, which have become an indispensable source of information for Serbian citizens. Please find more information about our reporters here: https://www.krik.rs/en/krik-team/  ",8 Dec 2020
United States,Bloomberg,Big,Bloomberg Vaccine Tracker,"Database, Infographics, Chart, Health","Scraping, D3.js, Json, Google Sheets, CSV, R, Node.js"," The Bloomberg Vaccine Tracker is the most up-to-date and comprehensive tally of vaccinations in the U.S. and around the globe, powered by a network of Bloomberg reporters in more than 50 countries who collect data from local sources unavailable in any other government or public dashboard. "," Bloomberg's tracker has been cited repeatedly by state governments (including Illinois, West Virginia, Connecticut and California), elected officials, international governments and news organizations (including MSNBC, PBS, Politico and Axios).  California, after falling behind most other states in its vaccination campaign on Bloomberg's rankings, launched a statewide data review and found that health-care providers were not properly submitting vaccination records. Maryland also moved to address what it said were problems with slow reporting because of its position on Bloomberg's ranking. In other states, citizens have used Bloomberg's dashboard and rankings to ask state officials why vaccine rollouts are moving ahead slowly.  In South Korea -- lauded for its pandemic response -- local media used Bloomberg's database of vaccine contracts to ask the government why it had not announced deals to acquire the shots. The deals were announced days later. "," The data entry and fact-checking are done through a massive and well-designed Google Sheets system, where reporters and editors take shifts working around the clock, entering data from government sources, and check each others' work. The data then goes through a processing and cleaning pipeline to eliminate questionable entries. This data pipeline is built in Node.js. Historical data is reconstructed through daily snapshots of the data and takes into consideration government data revises. "," The hardest part is to built and maintain this comprehensive vaccination database that doesn't exist anywhere. Because of the lack of international and even national data disclosure standards, it's challenging to deal with the data inconsistency issues. It is also challenging to build a clean and well maintained data pipeline, while coming up with the most relevant graphics that captures what people care about the most in the news "," The lesson that other journalists can learn from the project is that automation cannot replace reporting. At the very beginning, we considered making the process fully automated, scraping states' and countries' vaccine dashboards. We soon realized that it would highly limit the number of states and countries we are able to include in the tracker. The half-automated process we finally decided on allowed us to include as many states and countries as possible, and at the same time to have built-in measures to prevent human errors. ",https://www.bloomberg.com/graphics/covid-vaccine-tracker-global-distribution/,,,,,,,,"By: Tom Randall, Cedric Sam, Andre Tartar, Christopher Cannon and Paul Murray; Editors: Drew Armstrong and Yue Qiu"," By: Tom Randall, Cedric Sam, Andre Tartar, Christopher Cannon and Paul Murray; Editors: Drew Armstrong and Yue Qiu ",4 Dec 2020
United States,Bloomberg,Big,Deadliest Mass Shootings Are Often Preceded by Violence at Home,"Investigation, Infographics, Women, Crime, Gun violence","Scraping, D3.js, CSV, Python, Node.js"," This story examines the relationship between domestic violence and mass shootings, weaving together personal narratives, data analysis and policy failures on a local and national scale to investigate one of the human costs of this country’s lax gun control laws. It analyzes 749 shootings from 2014 to 2019—the largest and most comprehensive by far of an analysis of this kind--to find that 60% of shootings with four or more victims were either domestic violence incidents or committed by men with histories of domestic violence. "," The story was shared by researchers and organizations such as Everytown, the Trace, the National Network to End Domestic Violence, the Coalition to Stop Gun Violence, Giffords Courage, Moms Demand Action, and the Canadian Femicide Observatory.    It provided overwhelming evidence of the links between domestic violence and mass shootings. It was also the first large-scale study to expand the data beyond the deadlist mass shootings—reasoning that attempts upon lives should be taken just as seriously, whether victims ultimately survive or not. In doing so, it was the first study to find that mass shootings with domestic violence links were also more deadly. "," Mass shootings data was compiled and cross-verified from multiple datasets in order to achieve the most complete survey available. From there, details and metadata around each of the 2000+ incidents were researched by looking into news reports and criminal histories of the perpetrators.    The interactive story was built with a combination of d3, ai2html, and other custom html, css and javascript. "," Given the sensitivity and nuance around each incident, we didn't feel comfortable compiling and extracting data programmatically. So the most difficult aspect was the sheer number of stories to comb through—going over each one in graphic detail. Availability of criminal background history was spotty and varied by state, and other incident details were only known to us if local news covered it. "," A story is often the most powerful when personal narratives and quantitative journalism work together. The data analysis demonstrated widespread patterns, while tough interviews and Jackie's deft storytelling laid out the horrific patterns of abuse and failings in our justice system to heed the warning signs of violence. ",https://www.bloomberg.com/graphics/2020-mass-shootings-domestic-violence-connection/,,,,,,,,By Jackie Gu; Edited by Mira Rojanasakul and Rebecca Greenfield, Jackie Gu is a data journalist at Bloomberg. ,30 Jun 2020
Colombia,"La Paz en el Terreno, Rutas del Conflicto, Colombia2020 - El Espectador",Small,Risk maps: threats against communitarian leaders and Human Rights defenders in Colombia,"Investigation, Explainer, Solutions journalism, Long-form, Multiple-newsroom collaboration, Database, Illustration, Infographics, Map, Women, Crime, Gun violence, Human rights","D3.js, JQuery, Json, Adobe Creative Suite, Microsoft Excel, Google Sheets"," After the Peace Agreement between the Colombian Government and the former FARC guerilla in 2016, thousands of communitarian leaders have been killed, threatened and prosecuted for defending Human Rights and the Peace Agreement implementation, even during the COVID-19 pandemic. Given the inefficiency of the security measures granted by the Government, we, alongside these communitarian leaders, are building risk maps which warn of the dangers around their social work in the most violent regions of the country and point to urgent actions to mitigate that systematic violence.    "," Beside the fact that these maps have raised alert to the authorities responsible of fighting these risks, the most important impact of the project has been accompanying and recognizing the Colombian communitarian leaders and Human Rights defenders. In the meantime, the commercial press has limited itself to counting victims and has moved away from journalistic coverage in remote regions of Colombia, due to the difficulties that the pandemic has brought.   With a publishing strategy in multimedia and printed press —booklets and publications in El Espectador, the second-largest newspaper in Colombia— we have added to the positioning of this important issue on the country's news agenda, explaining the contexts around the risks and the interests of those behind this violence.   The quality of the investigations has been recognized by decision-making sectors in Colombian society, such as the Truth Commission and the Special Jurisdiction for Peace, institutions created after the Peace Agreement that have requested our work as input for their investigations in their mission to recognize the rights of victims and survivors of the conflict.   Also, we have encouraged the participation of the audiences of this journalistic alliance in clarifying these contexts of violence. La Paz en el Terreno (The Peace on the Ground) has a citizen communication channel (Your Memory Counts) that receives complaints, additional information and proposals for new topics to map.   Finally, the impact has also been evidenced in the interest of international cooperation organizations to support our work. Today we have the support of the UNDP (United Nations Development Programme) and the German Friedrich-Ebert-Stiftung foundation, with whom we have worked on the development of these maps since 2019 and we will continue with other mappings in 2021.    "," Through workshops with cartographies, interviews, and safe spaces —both physical and virtual— to which groups of Human Rights defenders from the same regions are invited, it is possible to determine these risk maps, digitalized and published in multimedia and printed formats. Unique data journalistic pieces which show this issue in a way that could not be possible otherwise.   These risk maps are constructed under a team methodology and seek, through cartography, to involve the communities affected by the armed conflict in the construction of their information, so that it can then be contrasted, verified and contextualized. We adapted our methodology to remote, protected video communication due to the COVID-19 pandemic.    In terms of journalistic practices, this initiative of risk maps is characterized by an empathic approach with the sources or participants of the cartographic workshops.   Once we collect the information from the physical maps, they are systematized in databases, digitized, illustrated and built with the D3 Javascript library. These maps are contrasted with expert and official sources, especially the Army and the Police Force, many times responsible for these risk contexts.   Additionally, information from the databases of social organizations, state institutions and our records on violence against communitarian leaders and Human Rights defenders in each region is cross-checked to establish patterns of this violence, obtain investigation leads and build infographics that clarify the dimension of these attacks in each territory.   The material is gathered and explained in interactive reports built in HTML5, published on the websites of  Rutas del Conflicto (Routes of the Conflict), Colombia2020 by El Espectador and La Paz en Terreno, as well as in radio and print.    "," When community leaders are asked how others can contribute to the protection of their lives, they have repeated the answer in every possible public platform: by responsibly making their situation visible. The experience of La Paz en el Terreno has shown us that showcasing the leadership of some attacked Human Rights defenders can work as a protection measure, but it is something that must be done with great care and responsibly handled information.    In most cases, we exercise anonymity, reliable contact channels and physical, digital, emotional and gender-sensitive security protocols. The Foundation for Press Freedom has endorsed these protocols. In this way, we shield the security of our journalists, allies and sources.   On the other hand, due to the contexts of violence and inequality that surround the majority of community leaders, there are great difficulties in obtaining information, not only for security reasons but also because of the difficult access to communities, often far from the big cities. There, the collaboration of allies such as local social organizations and organizations from international cooperation has been key. They have a long history working with these populations, a wide recognition in these regions and are an important source of trust. These organizations have served as a bridge between us and groups of community leaders.   Another major challenge to accomplish this work in 2020 was the COVID-19 pandemic. Given the impossibility of meeting in person with our sources in the first months of the pandemic, it was necessary to adapt the methodology of the cartographic workshops to the virtual plane. Once the emergency somehow diminished, we were able to travel and meet, but keeping the respective biosecurity measures.    "," A belief that we share with many colleagues around the world is that journalism must go beyond reporting what is happening; it must also be deeply committed to social change. This project seeks to be useful for the dignity and recognition of the victims of the armed conflict, as well as an active agent in the construction of peace.   In particular, this project teaches that:     There are tools for social research, typical of Humanities and Social Sciences, that can be extremely useful for journalism, especially when it seeks to closely understand the voices and views of those who generally do not participate in the public sphere, like victims. One of these is social cartography, which proves quite valuable for journalistic research aimed to understand the territory through the eyes of our sources, identify and rethink social problems through maps, images and stories, and involve the protagonists of the stories in the construction of their own information. All of this material can also be subject to data journalism techniques, as we do in our work.   If we are small, it will always be better to find allies. This journalistic alliance shows that the union between journalistic projects that have similar interests can produce relevant and useful synergies for people. Rutas del Conflicto is a specialized media-outlet in research and data journalism and Colombia2020 covers day to day news and has a range of reach of tens of thousands of people. Both projects saw the need to come together to monitor violence against communitarian leaders, one of the most critical issues in terms of serious Human Rights violations in the country today.   After treating the information, you should give it back to the sources. The booklets are sent to the leaders and their communities.   A pandemic cannot stop good journalism.  ",https://docs.google.com/document/d/1QblP49E0dOxI9QPvozVIetQJGqzP_5n4tu9JVIIPfvo/edit?usp=sharing,https://lapazenelterreno.com/mapas-de-riesgo/antioquia-silenciada/,https://lapazenelterreno.com/mapas-de-riesgo/amenazas-asesinan-lideres-sur-cordoba/,https://lapazenelterreno.com/mapas-de-riesgo/lideres-sociales-catatumbo/,,,,,"Juan Gómez, Silvia Corredor, Valeria Arias, Carlos Mayorga, Nicolás Sánchez, Natalia Pinilla, Alejandro Ballesteros"," Juan Gómez, Silvia Corredor, Valeria Arias and Carlos Mayorga: investigative journalists for La Paz en el Terreno and Rutas del Conflicto — winner of the Data Journalism Awards in 2017 as Data journalism website of the year—. In 2019, Juan Gómez was shortlisted for the same awards in the category Student and young data journalist of the year.   Nicolás Sánchez: investigative journalist for La Paz en el Terreno and Colombia2020 - El Espectador..   Natalia Pinilla: illustrator and graphic designer for La Paz en el Terreno.   Alejandro Ballesteros: journalist and web developer for La Paz en el Terreno and Rutas del Conflicto.   La Paz en el Terreno is a platform developed by the teams of Rutas el Conflicto and Colombia2020 - El Espectador, which produces journalistic content and tools with databases and in-depth investigations that allow to measure and monitor the implementation of the Peace Agreement between the FARC ex-guerrilla and the Colombian government in two items: the threats against social leaders and the reincorporation of ex-combatants. The data is published on the platform, social media and other formats (multimedia, podcast and printed) in order to reach the populations most affected by the conflict.",8 May 2020
Kyrgyzstan,Kloop.kg,Small,Rings of Corruption. How Kyrgyz businessmen fakes competition for 83 million soms,"Investigation, Database, Video, Corruption, Economy","Json, Python","   Kyrgyz businessmen imitate competition and win state tenders without any obstacles, because there are no other participants besides them. The law prohibits entering into contracts with such companies and calls it a ""conflict of interest.""      For this project we scrapped and combined the databases of state procurements and legal entities registered with the Ministry of Justice.   "," The whole project was not just publishing story, we conducted a series of online training sessions for all who were interested in learning Neo4J graph database (around 20 participants not only from Kyrgyzstan).   The explaining video has watched more than 200 000 times (both on Facebook and Instagram).    As a result of this project, after combining two databases we received a huge amount of our own data. Since we are small media we do not have enough resources to verify and work on everything.    So we decided to launch online investigation school, where we teach people journalism and investigations, while they train by working on with our data. Now we have around 100 students (althoug around 33 of them are actively participate, make all tasks and attend online classes). "," We scrapped two databases: state procurements and legal entities, combined them and used Neo4J graph database to show the connection between companies involved in state procurements. We also created an interactive interface where everyone can try and see how links are built in our database. "," Three challenging things:   1. The site of Kyrgyz state procurements itself: everything was filled in manually by employees (and often with errors and mistakes), so we double-checked the information   2. Every case needs to be veryfied to make sure that the companies are really controlled by the same people   3. For some reasons at the final point we decided to create our own interface for exploring connections in our database.  ", This is the unordinary approach for a journalistic stories and investigations. Such a data-oriented approach might be useful for other journalists. ,https://kloop.kg/blog/2020/07/15/kolca-korrupcii-goszakupki/,https://docs.google.com/document/d/1Qt9RO_EPg7feRWC8eTJHJFCxvJD5jZoWk8MKvqX-MEs/edit?usp=sharing,https://kloop.kg/blog/category/koltsa-korruptsii-v-goszakupkah-kyrgyzstana/,https://www.facebook.com/kloop.kg/videos/906721719838845,,,,,"Ekaterina Reznikova, Rinat Tuhvatshim, Khakim Davurov, Aziza Raimberdieva, Alexey Gulyaev, Atai Narynov, Anna Boiko, Arseniy Mamashev"," This project was created by a team of at least 8 people, who are independent journalists and editors from Kyrgyzstan and Crimea, as well as programmers, who are an inseperable part of Kloop's data team.  ",15 Jul 2020
"Hong Kong S.A.R., China",Stand News,Small,"Not Just Statistics, But Human Lives — A Remembrance of Hong Kong Covid Victims","Database, Illustration, Infographics, Chart","D3.js, Google Sheets"," In Hong Kong, almost 200 people have passed away from covid since January 2020. In the daily news updates, those who passed away are reported as case numbers #595, #1487, #1348, #1776, yet their lives are more than just that. This project serves as a remembrance of those lost to the pandemic, along with as many personal anecdotes as we could collect, from the lives that were lived.    "," As the pandemic got worse in Hong Kong, we felt like it was important to provide a record of those who have fallen victim to the disease. Beyond just collecting data, we were able to collect stories from those who have passed away.    One 88-year-old man visited his wife at a nursing home daily, before he succumbed to the disease. One daughter told us that she kept his 86-year-old dad’s dentures after he passed away. Another daughter told us that she wasn’t able to see his 80-year-old dad before he died because of miscommunication with quarantine officials.   Data doesn’t have to be impersonal. In these lives that were lost, there are many stories of affection, regrets, struggles. This project helped provide a chance for readers to remember those who were lost in this pandemic. "," We collected case data through various government sources, which is published in pdf format, in a google spreadsheet, and added additional anecdotal information as we collected it. We drew the illustrations of the figures representing people  with charcoal. We used d3.js for data visualization and web interactivity. "," As there is limited public information about those who have passed away, beyond just age, gender and place of residence, we had some difficulty in collecting anecdotes at first. We pieced together small bits of information from the daily coronavirus briefing when officials would announce new deaths. Along with the initial publish, we made a public call for those who have lost loved ones to reach out to us. We luckily received some responses and we were able to interview family members of several coronavirus victims, who shared details with us, as well as complaints against government arrangements that made it difficult for them to say goodbye.  "," To find families of those who have passed away from covid, we made public calls to action and were able to reach people this way. The feedback we got from this project have also taught us the importance and effectiveness of providing a more humanized approach to data stories.    ",https://www.thestandnews.com/society/%E4%B8%8D%E6%98%AF%E6%95%B8%E5%AD%97-%E6%98%AF%E4%BA%BA%E5%91%BD-%E6%AD%A6%E6%BC%A2%E8%82%BA%E7%82%8E%E9%A6%99%E6%B8%AF%E7%97%85%E9%80%9D%E8%80%85%E7%9A%84%E8%A8%98%E9%8C%84/,,,,,,,,"K.K. Rebecca Lai, Kris Lau"," K.K. Rebecca Lai is a graphics and data journalist who specializes in telling visual stories. Kris Lau is an experienced journalist, with a specialty in health and medical issues.    ",11 Aug 2020
United States,The Marshall Project,Big,The impact of COVID-19 on incarcerated populations in the U.S.,"Investigation, Explainer, Solutions journalism, Long-form, Multiple-newsroom collaboration, Database, Open data, Infographics, Chart, Crime","R, RStudio","   As coronavirus began to take hold across the country, The Marshall Project recognized the outsize impact it would have on the people living and working in America’s prisons and jails—where social distancing is impossible and many basic preventative measures are against the rules. An astonishing amount of data on criminal justice already goes uncounted or is deliberately obscured from the public. During the course of the pandemic, the need for strong data reporting behind bars has only grown more urgent and we focused on telling those stories.   ","     We were able to draw on our reporters’ expertise to write groundbreaking stories on COVID-19’s impact on the juvenile justice system, courts, policing and juries. We exposed how North Carolina prisons—despite being on lockdown—were still allowing hundreds of incarcerated people to work for local industries like chicken-processing plants, potentially bringing the virus back with them. After that exposé ran in local papers, the state closed the program. Our story on how Texas prison meals had become even more disgusting than usual prompted local lawmakers to press corrections officials to  improve the quality of the food.          Working with the Associated Press on covid tracking project  means that our data is sent to newsrooms across the country, with more than 670 citing our work. Sen. Amy Klobuchar used our data in a <a href=""https://www.klobuchar.senate.gov/public/_cache/files/a/9/a9a3faa6-2122-4fd1-aebd-aaddf21921f5/1F113BD5AE7C957C6868525BBFF6B031.0508lettertoag.pdf"">letter  to former-Attorney General William Barr, while Sen. Cory Booker and Rep. Ayanna Pressley relied on our numbers in a letter to governors of five hard-hit states, urging them to release people from prison who are over 50 or have pre-existing health conditions.           Our reporting also reveals how little the Federal Bureau of Prisons has done to protect vulnerable prisoners. Rep. Hakeem Jeffries referred to our work while questioning BOP director during a House hearing, our  work  was cited in a<a href=""https://www.opn.ca6.uscourts.gov/opinions.pdf/20a0365p-06.pdf""> federal court ruling  allowing district courts to reduce sentences for incarcerated people in “extraordinary and compelling” circumstances—essentially, to counteract the Bureau of Prison’s reluctance.           Our August investigation into the role of the U.S. Marshals in spreading COVID-19 while transferring federal prisoners around the country led Senators Elizabeth Warren and Cory Booker, and Congressman Ted Deutch, to write <a href=""https://www.warren.senate.gov/newsroom/press-releases/senators-warren-booker-and-rep-deutch-demand-information-on-the-safety-of-prisoner-transportation-by-the-us-marshals-service-and-contractor-prisoner-transportation-services-amid-covid-19-pandemic"">letters  to both the Marshals Service and Prisoner Transportation Services, urging them to immediately start COVID-testing before transferring incarcerated people between facilities.  ","   For our weekly tracker, we monitor websites for all of our state and federal prison agencies using <a href=""https://newsklaxon.org/"">Klaxon , an open-source reporting tool built by The Marshall Project. This alerts us to changes in data for many of the states. We have also built our own program that takes screenshots of most of the websites’ published data, such as it is.       We manually log all of the figures from the web and our shoe-leather reporting in a shared Google Spreadsheet, accessed by reporters at both The Marshall Project and The Associated Press. After the data has been manually checked, we run it through another battery of scripts to check for any discrepancies that need to be examined more closely.       Ultimately, our data analysis for all the projects is written in R, run through the RStudio IDE.  ","   To build and maintain the database that informs our covid tracking project has been enormously difficult. Most states provide only a fraction of the information we’re seeking. They might post positive cases and deaths, but not testing information, which makes it hard to put the number of cases into context. They might not report deaths at all. Most are not publicly reporting vaccine numbers. And some will report one set of numbers on their website with a current snapshot, without revealing that certain subsets of prisoners are being excluded. Only one or two of the departments post historic data on their site, so it’s extremely difficult to track changes over time. Very often numbers will be revised without any indication of why, for example, the number of recovered prisoners is 1,000 fewer today than it was a week ago. It requires creating our own historic snapshots and a series of checks to ensure the data’s integrity.      We try to surmount these problems by heavily employing the most powerful tool in a data journalist’s arsenal: the telephone. We contact the departments themselves every week and ask them to fill in the blanks and to answer questions about strange changes in any of the numbers. This requires persistence and a good system for examining the data as it comes in.  ","   By design, prisons and jails are among the most hidden institutions in American life, and an astonishing amount of data on criminal justice already goes uncounted or is deliberately obscured from the public. Even journalists who manage to gain access are only ever shown a highly edited version of life inside.           When we are given data, it’s often incomplete or comes out years after it’s relevant. We’ll receive thousands of pages of scanned, hand-written reports, forcing us to find time-consuming and laborious ways to pull the information into digital databases that we can use to make sense of it all and to share our knowledge with readers.          These stories, which rely on creating new data that doesn’t exist anywhere, have been incredibly demanding on our staff and our AP partners as well. To fill out the latest data from 51 agencies that are generally reticent to answer questions and have a habit of changing their answers when they do give them, demands a great deal of time and effort. To do it every week for ten months and counting is a huge undertaking. But without this data, we, and journalists and policymakers everywhere, would be operating completely in the dark as we try to assess conditions in prisons systems, and thus believe it’s crucially important that we continue to create the data and to make it available to everyone.  ",https://www.themarshallproject.org/2020/05/01/a-state-by-state-look-at-coronavirus-in-prisons,https://www.themarshallproject.org/2020/08/21/covid-19-s-toll-on-people-of-color-is-worse-than-we-knew,https://www.themarshallproject.org/2020/12/18/1-in-5-prisoners-in-the-u-s-has-had-covid-19,https://www.themarshallproject.org/2020/05/01/a-state-by-state-look-at-coronavirus-in-prisons,https://www.themarshallproject.org/2020/03/31/why-jails-are-so-important-in-the-fight-against-coronavirus,https://www.themarshallproject.org/2020/03/19/north-carolina-prisoners-still-working-in-chicken-plants-despite-coronavirus-fears,https://www.themarshallproject.org/2020/10/07/thousands-of-sick-federal-prisoners-sought-compassionate-release-98-percent-were-denied,,Staff of The Marshall Project," The staff of The Marshall Project are listed <a href=""https://www.themarshallproject.org/people"">here . ",31 Mar 2020
"Hong Kong S.A.R., China",Stand News,Small,Detainment of 12 Hong Kongers in China —— Reconstructing Government Flying Service Flight Paths the Day of Capture,"Investigation, Breaking news, Infographics, Chart, Map, Politics","Scraping, D3.js, QGIS, R, RStudio"," In August of 2020, 12 Hong Kongers were arrested at sea by Chinese police for illegal border crossing and sent to detention in mainland China. Hong Kong officials at first claimed to have no knowledge about the arrest, yet it was later revealed that the Hong Kong Government Flying Service flew two missions that coincided with the time and place of the arrest. This project reconstructed flight paths of government aircrafts the day of the arrest as well as months of flight records to point out suspicions around the event. "," The police and the Government Flying Service had refused to give details of the mission from the day of the arrest. Chief Executive Carrie Lam, head of the Security Bureau and the Commissioner of the Police all claimed that the Hong Kong government had nothing to do with the arrest of the 12 Hong Kongers. But through reconstruction of the flight paths, analysis of flight data, it became clear that there were irregularities in the deployment of government aircrafts the day of the arrest, building more suspicion around the event.   Hong Kong protests that began in 2019 were mounted against a proposed law that allowed extradition to China where the justice system is highly politicized. The arrest and subsequent detainment of the 12 Hong Kongers was highly significant to the protest movement in Hong Kong. The way with which the government handled information around the arrests raised even more suspicions. This project served as an important record to counter the government narrative that claimed no collusion with Chinese authorities in the arrest of the twelve Hong Kongers. "," We scraped flight data from flight tracking sites, and used R to process the data. We also utilized open source software mapshaper.js and QGIS to plot the map data. Finally we used a combination of illustrator and d3.js to visualize the information for the web. "," There was a huge amount of data associated with this story as we tried to piece together months of flight records from the Government Flying Service aircrafts. Flight tracking data is also somewhat inconsistent because of the limits of radar technology. Even after the initial effort of scraping the flight data, it was a difficult task to comb through the data and chain together flight information for individual flights in order to spot irregularities in deployment.    We also faced a quick turnaround on the stories as news was breaking fast. Each of the two visual stories was turned around in a day and involved a lot of technical challenges and reporting to pull together.    The political situation in Hong Kong has been on edge since protests began in 2019. Stories like these serve an important role in continuing to offer a spotlight on people involved in the protests as they face a government intent on stomping down opposition voices. "," Flightradar24 and FlightAware are two of the world’s largest flight tracking data companies. There are troves of information available on the sites and are an important resource in investigative journalism that involves tracking flights. Beyond just scraping data from the sites, we were also able to reach out to these companies to help answer questions in order to untangle the data. It was important lesson in not handling the data in a vacuum but to interview the data provider when possible especially to better understand the shortcomings of the data.  ",https://bit.ly/3ji5d4B,https://bit.ly/2LjSDoM,,,,,,,"K.K. Rebecca Lai, Shunhei Chan", K.K. Rebecca Lai is a graphics and data journalist who specializes in telling visual stories. Shunhei Chan is an experienced investigative journalist.    ,7 Oct 2020
United States,The University of Maryland's Howard Center for Investigative Journalism reporting consortium,Small,Nowhere to Go,"Investigation, Explainer, Solutions journalism, Long-form, Database, Open data, Infographics, Chart, Video, Business, Women, Health, Crime, Economy","Scraping, Json, Microsoft Excel, Google Sheets, CSV, R, RStudio, Python","   Last year, the Howard Center for Investigative Journalism at the University of Maryland collaborated with other universities to investigate the impact of homelessness and the threat of homelessness posed by the pandemic for a project called “Nowhere to Go.”      The initial stories this spring documented the criminalization of homelessness in some of the country’s least affordable cities. As the pandemic caused millions of Americans to lose their jobs, the consortium pivoted over the summer to determine whether the federal moratorium on evictions was working. This fall, it investigated evictions by public housing authorities, often the last stop before the street.  ","   The series focused attention on the plight of some of the nation’s most vulnerable people, with distribution by the Associated Press and USA Today.  By highlighting stories of individual tenants at risk of eviction, our work also led directly to some getting necessary assistance to remain housed.           It also drove the creation of two major journalistic collaborations that were launched for the project, collaborations that have continued even after this particular reporting project ended.           First, to tell these stories, we brought together data journalists and faculty at seven universities: the University of Maryland, University of Oregon, Boston University, Stanford University, the University of Florida, the University of Arkansas and Arizona State University. This multiple-university collaboration was, we believe, unprecedented in scope.  This collaboration is now working together on a new reporting project, and recruiting others to join.       Second, it kickstarted a collaborative effort to obtain hard-to-get court records in more than a dozen cities.  Data journalists working for the Howard Center at Maryland, the Big Local News project at Stanford University, USA Today and others collaboratively wrote custom, open-source software to scrape court records on evictions from online court record management systems in more than a dozen cities.  This effort was onerous -- but necessary -- because eviction records we needed to tell these stories were unavailable through other means.  This data collection effort has continued, and is even expanding.        We have open-sourced the court scraping software package we developed for this project and we continue to improve. We are also adding additional court systems to the list of jurisdictions from which we regularly collect records.  And we are starting to gather new case types, beyond just evictions.  We are releasing all the data we’ve collected on Stanford’s Big Local News data sharing platform for use by other journalists.   ","   We used several tools and technologies to bring this project to life, including:          Web scraping with Python, R and a suite of open source software libraries, including selenium.           Data analysis with Python, R and SQL, primarily working in Jupyter notebooks and R Studio.           GitHub for version control and open source tool sharing.           Open Refine for data cleaning.           ","        The hardest part of this project by far was on the data acquisition front.  Obtaining bulk court data in the U.S. is needlessly complicated.  Most state and county judicial systems in the U.S. are exempt from public records laws that would compel the release of bulk data.  When court systems do make bulk data available, they often sell access at prices unaffordable for most news organizations.          In most states, access to court records is made available through a public web application that typically allows users to look up individual cases, linked to a database back end that contains the bulk data we were after.  By writing custom web scraping software to programmatically access these sites, we were able to gather court data in bulk.           This was not an easy effort. No two web applications were alike. Though many sites employ software from a common technology vendor to power their sites, each site had its own quirks, making it difficult to write a single tool to gather all the data we needed.  And many sites had defenses designed to prevent automated tools like ours from gathering records at scale, but we developed legal techniques to defeat these systems.    ","   There are several things we think other journalists could benefit from that we discovered while working on this project.            Collaboration is a great way to take on journalism projects that are beyond the scope of one individual newsroom.  But it takes work to organize and manage correctly, especially on projects that combine student journalists with professionals.           Through this project, we have done more than just produce a package of meaningful journalism.  We have also provided other news organizations several opportunities to build on our reporting.               We have released the underlying data on evictions in more than a dozen cities for use by other journalists.           We have produced an open-source software package that they can modify to obtain court records in their local jurisdiction, and have opened the door to others who wish to join our collaboration and work alongside us.           We have also tested and implemented a stable, low-cost method for legally defeating captcha technology that could be of broad use to other journalists involved in scraping government websites.      ",https://homeless.cnsmaryland.org/,https://github.com/biglocalnews/court-scraper,https://biglocalnews.org/#/open_projects,,,,,,Howard Center for Investigative Journalism reporting consortium,"   The Howard Center for Investigative Journalism, launched in 2019, gives University of Maryland Philip Merrill College of Journalism students the opportunity to work with news organizations across the country to report stories of national or international importance to the public. For this project, we also collaborated with journalism students and faculty at the University of Oregon, Boston University, Stanford University, the University of Florida, the University of Arkansas and Arizona State University.      ",2 Aug 2020
United States,The New York Times,Big,Tracking the Coronavirus,"Investigation, Long-form, Multiple-newsroom collaboration, Database, Open data, Fact-checking, Infographics, Map, Health","Personalisation, Scraping, D3.js, Json, Microsoft Excel, Google Sheets, CSV, R, Node.js"," In the vacuum left by the lack of a coordinated federal effort to disseminate data on the pandemic, we launched a project that would become the newsroom’s most ambitious data-tracking effort ever. We were one of the first organizations to provide county-level data on cases and deaths. We also compiled authoritative databases on case clusters at nursing homes, food-processing facilities, prisons and colleges. Our efforts focused on topics of public importance where timely, reliable data wasn’t available from the government. The data were presented on over 80 tracking pages, used in stories and made available to the public on Github. "," The project had a tremendous impact on many levels. Readers tell us about how they check the tracking pages every day, which are among the most-viewed pages we have ever published.   We decided early to make the county-level database freely available, extending the reach of the data far beyond the pages of The Times. It has been cited in more than 60 peer-reviewed scientific papers. It has been cited by federal agencies, like the Council of Economic Advisers, CDC and Department of Veterans Affairs, which said it would be used to provide better acute care in rural areas. Local and state officials cited the data in formulating policies. Health care companies said they were using the data to manage patients. Other companies said they were using the data to help inform when to return to the office.    The data has been cited in many news publications, and Google uses the data to power its U.S. and state dashboards whenever anyone searches for “covid cases.”   Our cluster data revealed how colleges drove Covid into areas that had been relatively virus free. It has revealed that about a third of Covid deaths are linked to nursing homes. And it has shown the wide racial gap in who gets sick and who dies.   When we began tracking nursing home cases, most states were not identifying affected facilities. We filed dozens of public records requests to surface this information. When we began tracking college cases, very few were proactively disclosing numbers. We filed more than 200 public records requests, and by mid-fall most colleges were sharing data. To analyze racial disparities, we filed a Freedom of Information Act request and successfully sued the CDC for access to its internal database. In total, we filed more than 400 public records requests for virus data. "," The county dataset of cases and deaths are stored in a PostgreSQL database. Node.js was used to create a data admin for doing QA and managing multiple sources of data for every geography, as well as to create a suite of more than 300 scrapers to collect the data. The nursing home system extends Google Sheets to use as a relational database, with both automated scrapers and news assistants entering data for individual states and facilities. After new data is collected, normalized, and fact checked, a custom Node.js script integrates the new data into the database.    The data are frequently pushed to thousands of pages on nytimes.com, which use JavaScript (D3 and Svelte) to dynamically generate text and visualizations, which explain the latest state of the outbreak in every state and county, in nursing homes, colleges, prisons and more. Google Drive, Google Documents, Microsoft Excel, Microsoft Access and R were also used to manage data and do analysis.    Some of the data collected through The Times’s survey process has also been analyzed by Times journalists and joined with other large datasets to produce investigative stories. Journalists joined The Times’s long-term care database to the federal government’s database, as well as to data on the racial make-up of nursing homes. They used regression analysis to determine whether there were patterns in the homes that had cases or deaths. Journalists joined the county-level database with Census data to identify counties where college students comprised at least 10 percent of the population. The journalists then took that list of about 200 counties and cross-referenced it with The Times’s college cases database. The resulting stories showed that college campuses were driving the spike in cases in the early fall, and that college outbreaks likely led to deaths in the wider community. "," The Times has several internal databases dedicated to tracking U.S. coronavirus cases at the county level and clusters at facilities. Each database has strict methodology for entry and verification, developed by Times journalists.    The county-level case database, vaccine database and nursing home database were initially manual operations, with a team of journalists checking public websites and Twitter feeds or contacting state and county governments and individual facilities. Both are now powered both by manual collection and computerized processes created by Times developers. The college, prison and cluster databases use manual collection only, and often involve extensive email or phone correspondence with government officials and business representatives. The collection effort includes dozens of journalists, who survey government entities and private facilities, collect and fact-check data, build and run automated collection systems and present the work in text and visually.    With little federal guidance for reporting, nursing homes, prisons, colleges and even local health departments often define and report cases and deaths in different ways. The Times has developed strict methodology to make sure cases and deaths are counted in a consistent way. For example, some colleges report positive tests rather than unique positive cases. Because it is possible for individuals to test positive multiple times, journalists have asked every college that reports positive tests (there are hundreds) to confirm the number of unique cases. If the college is unable to confirm a number or refuses to respond, those cases are removed from the total number of cases tied to colleges overall, and those colleges have been marked clearly in the online tracker as having potential duplicate test results. The Times also has strict fact-checking rules in place to make sure cases and deaths are sourced properly and checked by multiple people before publication. "," Problems in the data frequently led to story ideas. The county-level data did not contain enough information to analyze racial disparities on a national level. So, The Times filed a Freedom of Information Act request and eventually sued the C.D.C. for an anonymized database of individual confirmed cases along with characteristics of each infected person. The C.D.C. provided data on 1.45 million cases reported to the agency by states through the end of May. Many of the records were missing critical information The Times requested, like the race and home county of an infected person, so the analysis was based on the nearly 640,000 cases for which the race, ethnicity and home county of a patient was known. The data allowed journalists to measure racial disparities across 974 counties, accounting for about 55 percent of the nation’s population, a far wider look than had been possible previously.    Like many newsrooms across the country, The Times undertook this sprawling data collection and reporting effort remotely, with dozens of journalists working from laptops in their homes on shared databases. As cases and deaths have swelled, the databases housing this information have buckled under their weight multiple times, requiring creative solutions. In mid-March, the database, a shared Google spreadsheet, suddenly stopped functioning because it had too many cases and too many people working in it. Journalists quickly had to redesign the database, splitting it in two parts, in order to keep the tracking effort going. The nursing home database alone has undergone multiple redesigns because there have been so many cases and deaths at so many facilities. ",https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html,"Nursing Home Covid Tracker - first published May 11, 2020, and updated regularly since then: https://www.nytimes.com/interactive/2020/us/coronavirus-nursing-homes.html","College Covid Tracker - first published July 28, 2020, updated regularly since then: https://www.nytimes.com/interactive/2020/us/covid-college-cases-tracker.html","The Fullest Look Yet at the Racial Inequity of Coronavirus - published July 5, 2020: https://www.nytimes.com/interactive/2020/07/05/us/coronavirus-latinos-african-americans-cdc-data.html","Track Coronavirus Cases in Places Important to You - first published Nov. 24, 2020: https://www.nytimes.com/interactive/2020/us/covid-cases-deaths-tracker.html","How Full Are Hospital I.C.U.s Near You? - first published Dec. 16, 2020: https://www.nytimes.com/interactive/2020/us/covid-hospitals-near-you.html","See How the Vaccine Rollout is Going in Your State - first published Dec. 11, 2020, updated regularly since then: https://www.nytimes.com/interactive/2020/us/covid-19-vaccine-doses.html",,The New York Times," This was a cross-department effort from Graphics, National, Interactive News, International and Digital Design. ",3 Mar 2020
United States,The New York Times,Big,An Incalculable Loss,"Database, Illustration, Infographics, Health","Scraping, Canvas, Adobe Creative Suite, Google Sheets, Node.js"," To mark the grim milestone of 100,000 coronavirus deaths in the United States, we collected obituaries of 1,000 people and distilled a brief excerpt for each to try to capture the humanity of the lives lost. The project sought to turn the death data into a more humanistic representation of the scale of the loss. Online, we combined the excerpts with 100,000 illustrated figures and an essay. In print, we took over the entire front page and several pages inside with the 1,000 names and obituary excerpts — still just 1 percent of the toll. "," The project drew widespread attention at a difficult moment in the pandemic. The decision to take over the front page of the print paper was especially resonant and ultimately drew people into the project to spend time with the names and their stories.   Many readers, including those who lost loved ones to Covid, expressed their gratitude for the piece, some in particularly emotional terms. One note: “I will cherish your echoing the pains of lives lost to Covid-19, and thereby helping us to grapple with what it means to be the America to which my parents each emigrated from eastern Europe just over a century ago.”   People who saw their own loved ones in the list and reached out; others shared the piece or left comments with their loved ones' names and a short sentence, adding them to the list and making them part of the project, too.    The all-type format of the front page inspired many derivative art projects, including one striking take amid the racial justice protests that sprung up not long afterward listing names of those killed by police. "," This was not a typical data project because a significant portion of the source material was qualitative and had to be collected and qualitatively reviewed for inclusion, then turned into readable data for display. The editing of the 1,000 excerpts was perhaps the most time-consuming and emotionally challenging part of the project.   To gather the obituaries we searched news reference sites like Factiva and Nexis as well as the obituary site Legacy.com for mentions of deaths related to coronavirus. We also searched Spanish- and Chinese-language media and did some more targeted searching on the sites of specific news outlets to make sure our list was as inclusive as possible.    All the material was poured into a series of 17 Google docs with about 100 pages of obits each. A team of researchers and editors read through them creating metadata (name, age, location) in ArchieML and choosing excerpts for publication. We created a Node script to download the data from the 17 docs and merge them into a single Google spreadsheet that powered the interactive.   For the graphic visualization we used an HTML canvas element with a series of small illustrations. We added aspects of randomness that would make the display feel more organic while also retaining a sense of order. For the typographic elements we chose to use real HTML text for crisp display. The final piece of the puzzle was to sync the two elements, which happens as the page loads and responds to the size of the viewport. "," It was emotionally taxing work on a tight deadline. At the time, such a scale of death was hard to fathom (though we’ve since nearly quintupled it), and to build the piece we had to confront it directly.    From initial concept to publication was less than two weeks, and it required an enormous team to pull off the research, editing and production. It was difficult to get the whole thing done from a practical standpoint and also difficult because of the subject matter.    Once we had the initial list of obituaries, a half-dozen editors read every one to choose the very short lines that went into the piece. Most are direct quotes, or lightly edited. Because we were keeping everything to a sentence or less, we ended up leaving so much out. It’s difficult to capture a whole life in a handful of words, or even the couple hundred words we were drawing from. But the hope was that collectively, the short descriptions would help readers better comprehend the enormous gravity of the deaths in a more visceral way than just a chart or data visualization.    It was also challenging to strike the right tone visually and in text, again remembering we were dealing with real people’s lives and wanting to honor their loved ones’ memories of them. We went through many iterations of the illustrations and design to make it feel at once personal and to convey the scale — a balance that is difficult to achieve in most data work. "," It’s important to humanize numbers, especially big ones, but it’s not easy. On this story, getting to the underlying source of the quantitative information helped us reveal a lot more than a summary statistic.   We started out feeling like the impact of 100,000 deaths was this awful, unknowable thing. How could we ever know what it meant? We tried to answer the question by breaking it down. Each death was a life, each life was a person, each person had a story, a community, an impact. That story was knowable, too. All together, they made up a big, overlapping story about our country.   One lesson we learned on the process: It was valuable for everyone involved to have a sense of the final work product early on. We did a proof-of-concept sketch of the front page that helped gain buy-in from leadership, and also gave the researchers and editors combing through the obits a tangible goal. Sometimes we do the reporting and then start designing, but in this case we needed both to happen on parallel tracks and we were lucky that they could inform each other. ",https://www.nytimes.com/interactive/2020/05/24/us/us-coronavirus-deaths-100000.html,Print A1 - https://twitter.com/nytimes/status/1264427825639063553,,,,,,,The New York Times," This was a cross-department effort from Graphics, National, Digital Design, Research and the Print Hub.    By Dan Barry, Larry Buchanan, Clinton Cargill, Annie Daniel, Alain Delaqueriere, Lazaro Gamio, Gabriel Gianordoli, Richard Harris, Barbara Harvey, John Haskins, Jonathan Huang, Simone Landon, Juliette Love, Grace Maalouf, Alex Matthews, Farah Mohamed, Steven Moity, Destinée-Charisse Royal, Matt Ruby and Eden Weingart.    Additional research by Yuriria Avila, Alex Lemonides and Chi Zhang. Additional editing by Jason Bailey, Eric Morse and Alison Peterson.  ",24 May 2020
United States,The New York Times,Big,Who Gets to Breathe Clean Air in New Delhi?,"Documentary, Database, News application, Infographics, Chart, Video, Map, Environment, Health","Sensor, D3.js, JQuery, Json, Adobe Creative Suite, Google Sheets, CSV, Node.js", We measured the air pollution that two children in New Delhi breathed as we followed them around the city on a normal day to see how wealth inequality affected their exposure. ," Everyone does not breathe the same air. This project went to extraordinary lengths to make visible this dangerous reality.    While Delhi’s poor air quality is well-known, disparities in individual exposure based on class or circumstances are poorly understood. Few researchers have collected this data. Many residents are unaware of the risks. The most harmful pollutants are commonplace, legal and largely invisible.   We showed, moment to moment, what that exposure looks like. Monu, who lives in a slum and attends school outside, is exposed to about four times as much pollution as Aamya, whose school and home are guarded by air purifiers. We watch as both children brush their hair, hang out with friends and sit down for dinner, and see overlaid the spikes and valleys of their real-time pollution exposure.    The visual contrast reflects a dispiriting reality: A long-term, consistent disparity like we observed that day could steal around five years more life from someone in Monu’s position, compared with an upper-middle-class child like Aamya.   Arden Pope, one of the world’s foremost experts on health and air pollution, called the piece “an engaging, important, and sobering story.” Scott Murray called it “the finest piece of data-driven visual journalism I have seen, ever, hands-down.” Most importantly, it is impossible to address health inequalities if they are not understood, and this piece provides an opening. "," We worked with researchers from ILK Labs in Bangalore to design a data collection and processing protocol involving three types of portable air quality sensors and custom software running on a battery-powered Raspberry Pi computer.    We recorded the air pollution data as CSV files. The video files included sidecar metadata generated by the cameras. We used Google Sheets and a custom Node.js app to parse the various metadata and sync it all via their timecodes. This new, combined data allowed us to generate all the time code-based visuals you see in the story.    The timelines and side-by-side photos, using this new data, were made interactive in the browser with JavaScript, leaning heavily on the D3 JavaScript library.    It is important to note that just as important as specific technologies used were the months of on-the-ground reporting from reporters in the New Delhi bureau to identify appropriate families, gain entry to their homes and schools, and understand the broader social context behind Monu’s and Aamya’s lives. The combination of deeply informed on-the-ground reporting and creative data-gathering and visualization helps make the project distinctive. "," It’s unusual for The Times to collect data for a project like this. We usually build charts and other graphics with data given to us by outside experts. But the data we wanted didn’t exist, so we had to collect it.    To add to the challenge, there wasn’t a turnkey, commercial project that did what we needed. Most air quality monitoring hardware is big and stationary.  We needed a small, mobile solution that let us keep up with our subjects. We settled on the Arduino-based Airbeam 2 sensor by HabitatMap, which took exposure measurements of the kids every five seconds as we documented their day with our cameras.    The sensor in the Airbeam 2 is well-regarded, but shortcomings in its mobile app led us to fork an open source project that reads real-time data off the sensor. Our heavily modified app ran on a tiny, battery powered Raspberry Pi Zero computer tethered to the sensor’s serial port. Our app allowed us to control the sensor and manage the data it collected, in the field, using only our phones.    To assist us in data collection and post-reporting analysis, we partnered with Adithi Upadhya and Meenakshi Kushwaha, pollution researchers with ILK Labs who have experience measuring pollution in India using lower-cost sensors like the Airbeam 2. They also collected stationary samples with their own equipment to help verify and normalize the data on our smaller, portable sensors. "," There is tremendous potential for sensor journalism and rigorous fieldwork to show aspects of the world that cannot be revealed in an interview or official records. With the guidance of researchers, we can design experiments that give us direct access to scientific data and far more visualization possibilities than if we relied on existing studies. At the same time, none of the data will matter if we can’t make it feel intuitive and personal, and this means empathetic, deeply informed beat reporting will be as critical to this journalism as it is to written work. ",https://www.nytimes.com/interactive/2020/12/17/world/asia/india-pollution-inequality.html,,,,,,,,The New York Times," By Jin Wu, Derek Watkins, Josh Williams, Shalini Venugopal Bhagat, Hari Kumar and Jeffrey Gettleman Cinematography by Karan Deep Singh and Omar Adam Khan.    Field production by Sidrah Fatma Ahmed.    Meenakshi Kushwaha and Adithi Upadhya from ILK Labs helped collect and analyze data.    Produced by Rumsey Taylor, Leslye Davis and Josh Keller.  ",17 Dec 2020
United States,The New York Times,Big,How the Virus Got Out and How the Virus Won,"Explainer, Illustration, Infographics, Chart, Map, Health","Animation, 3D modelling, Three.js, QGIS, Json, Google Sheets, CSV, Node.js"," Clear and compelling visualizations illuminated two major public health failures. The first analyzed the movements of hundreds of millions of people to show why the most extensive global travel restrictions in history didn’t stop the epidemic. The second showed a vast wave of infection that swarmed across the United States before we knew about how bad it was, and how American officials chose to underestimate, downplay and ultimately ignore the threat. "," These two stories wove together emerging science and data analysis to create animated explanations of how the pandemic spread beyond our grasp. Combining data journalism with cutting-edge visualizations, the stories assembled the evidence and showed who was at fault.   The reporting for each story was immense. For “How the Virus Got Out,” we worked with many of the world’s top disease researchers to assess what fueled the epidemic. We scraped cell-phone tracking data from Baidu and matched it to global flight traffic, allowing us to describe, for the first time, how people moved around China and the world in the early months. The analysis found that about seven million people left Wuhan before the Chinese government locked it down, seeding outbreaks in 26 countries. This story was published in mid-March, when many Americans were becoming aware of the threat. It became The Times’s most-read international story of the year.   For How the Virus Won, we spent months analyzing genetic data, case reports, disease modeling and travel patterns. We partnered with epidemiologists and geneticists, who provided exclusive research on the genetic traces of the outbreak and the central role of travel from New York City. We visualized the hidden spread of the virus in the early months of the year, when the public didn’t know where the virus was and officials waited to act. Those delays, we estimated with Columbia University researchers, cost tens of thousands of lives.   Tom Inglesby, one of country’s top public health researchers, tweeted: “@CDCgov should be doing analyses like this NYT story to understand pandemic. This kind of analysis should be encouraged and permitted by Administration. Information needs to flow freely to understand exactly what happened, is happening, as it has in past outbreaks.” President Trump called the estimates of lives lost a “political hit job.” "," Both stories used WebGL to visualize the spread of the virus. The Three.js javascript library allowed us to bring our data into a three dimensional space where we animated hundreds of thousands of particles representing healthy and infected populations, virus mutations, virus deaths and travel distances. We used our internal Node-based publishing system to constantly parse each dataset while we were making design decisions, to ensure we were always sketching with the latest data.   In order to visualize such a large number of data objects, we used a technique that bypassed the computer’s CPU by sending animation information directly to the graphics processor. The animation calculations, which would have normally created a bottleneck in the Three.js library, were computed by embedding values in our custom WebGL shaders. The performance of the reader’s browser was far better using this technique.   Adobe Illustrator, Autodesk Maya and QGIS were used to create the maps and 3D models. "," Epidemics are difficult to understand. Viruses are invisible. Exponential growth is confusing. Testing is spotty. The hardest part about this project was finding a visualization form that could connect together what was happening in an intuitive way. And what was happening was terrifying.   We pushed the boundaries of what is possible in the browser, using technologies that would not have been possible just a few years ago. And we did it for our most ambitious telling of the world’s biggest story. But it was not visualization merely to track the virus — it was to help readers understand what went wrong.   Once you can visualize the invisible — we used dots of infections visiting new cities, growing quickly — then it’s easier to see why our various strategies didn’t work. Lockdowns were too late. New York City Mayor Bill DeBlasio’s March call to “get out of the town” was dangerous. Restrictions on travel from China meant little when cases were streaming in from Europe. We were limited by what we didn’t track and couldn’t see. "," The projects were cohesive because we knew what we wanted to say. This came out of extensive conversations with researchers about the dynamics of outbreaks and the data we would need to illustrate them. It allowed us to be very specific about which data we would go after, the forms we would explore and — just as important — the data we would ignore. We spent weeks negotiating with flight data firms, for instance, arguing that their data could play an important public service. Every aspect — reporting, data gathering, visualizations, writing — had to support the central message we wanted to convey.    But beyond what the pieces explicitly say, the animations here helped convey a visceral understanding of what happened when words were insufficient. Fear that the virus is streaming into new cities. Awe at the explosive power of exponential growth. Outrage that officials refused to see what was all around them. Sadness at the invisible, uncounted lives that could have been saved. Ultimately, it is these messages that leave a lasting impact and show the unique power  of data visualization in an all-consuming crisis. ","How the Virus Got Out - March 22, 2020: https://www.nytimes.com/interactive/2020/03/22/world/coronavirus-spread.html","How the Virus Won - June 24, 2020: https://www.nytimes.com/interactive/2020/us/coronavirus-spread.html",Print for How the Virus Got Out: https://twitter.com/DanZedekDesign/status/1243239304710836226,Print for How the Virus Won: https://twitter.com/Josh_H/status/1279717732586598400,Tom Inglesby: https://twitter.com/T_Inglesby/status/1276131896955670532,,,,The New York Times," How the Virus Got Out - By Jin Wu, Weiyi Cai, Derek Watkins and James Glanz.    How the Virus Won - By Derek Watkins, Josh Holder, James Glanz, Weiyi Cai, Benedict Carey and Jeremy White.  ",22 Mar 2020
Nigeria,"Code for Africa, The Guardian Nigeria, Pulitzer Centre on Crisis Reporting, Humanitarian Open Street Map Team, AfricanDRONE, Uhuru Labs",Big,Mapping Makoko,n-a,"Drone mapping, OpenDataKit, Shorthand, Flourish data visualisations, Google Earth Pro, Google Sheets"," Neglected and Unmapped was produced through a partnership between Code for Africa, the Guardian Nigeria, The Pulitzer Centre on Crisis Reporting, Humanitarian OpenStreetMap, MakokoDreams, AfricanDRONE and UhuruLabs.    Code for Africa (CfA) is the continent’s largest network of civic technology and data journalism labs, with teams in 12 countries. CfA builds digital democracy solutions that give citizens unfettered access to actionable information that empowers them to make informed decisions, and that strengthens civic engagement for improved public governance and accountability. This includes building infrastructure like the continent’s largest open data portals at openAFRICA and sourceAFRICA, as well as incubating initiatives as diverse as the africanDRONE network, the PesaCheck fact-checking initiative and the sensors.AFRICA air quality sensor network.   In this project CfA contributed conceptualised the project, managed and coordinated all parties involved (including the local community in Lagos and the local NGO partner, MakokoDreams), ran the field data collection and developed the data journalism campaign around it.   The Guardian Nigeria contributed to reporting on the mapping project conducted my CfA and partners in Makoko.    The Pulitzer Centre on Crisis Reporting was the project funder.    The Humanitarian OpenStreetMap team contributed to project funding. Volunteers from HOTOSM network helped to digitise maps through their crowdsourcing mapping platform.   AfricanDRONE and its subsidiary - Uhuru Labs - contributed to drone training, drone mapping and uploading drone footage to OpenStreetMap open data portal.    "," Makoko is one of Africa’s most unique inner-city slums, with a third of the community built on stilts in a lagoon off the Lagos mainland, accessible only by canoe. The rest of the settlement is on swampy land with little sanitation and few public services. Makoko is estimated to be home to 300,000 people, but government and residents claim the figure is higher. The numbers are hazy because the area appears as a near-blank space on maps — with little information about structures, density, or streets. This means it is almost impossible to properly track land ownership, plan infrastructure, optimize services, plan for emergencies, or support development.   Being a blank spot on the map means authorities never adequately allocate resources to Makoko, or — worse — exploit the lack of awareness to grab the land and displace dwellers. Neglected and Unmapped explored a bottom-up mapping project that helped the community fight for their land rights. The project combines data, satellite images, on-the-ground multimedia journalism, and long-form storytelling.   The project began with strategic partnerships and townhalls to introduce the project to the community and get their buy-in. This was a critical step, as years of neglect from officials, and individuals claiming to be working for their benefit had affected their perceptions of outsiders working in Makoko.    Once the community was brought on board, our team worked with people in Makoko to build their capacity and empower them to join the project. Fifteen local women and seventeen men were trained by the coalition partners to map Makoko using drones, canoes and open data platforms. Throughout the project 990 images captured by drones were uploaded to the OpenStreetMap and Java OpenStreetMap. The Humanitarian Open Street Map volunteers assisted in digitising the images. Additionally, 80 Points of Interest (POIs) were captured using The Open Data Toolkit (ODK) and dataset uploaded to openAFRICA. These POIs include sources of clean water, schools, healthcare facilities, places of worship, townhalls and more.    The mapping project was captured in a flagship story produced by media partners the Guardian Nigeria. This piece married drone journalism, geo-journalism, data visualisations and multimedia with first hand narratives of lives in Makoko to expose the challenges faced in the slum to an international audience. The project then received considerable international traction, receiving features in Agency France Press, Aljazeera, CNN, the BBC and the Guardian UK.    Mapping Makoko is an ongoing process. In 2020 the project team returned to Makoko and conducted fresh water samples for analysis, mapping a further 20 clean water sources as POIs in Makoko. We hope to continue mapping Makoko through future grant opportunities, as well as to use the collaborative model developed there as a blueprint for future data journalism projects. ",,"Accessing the Makoko informal settlements in Lagos
Getting the buy-in from the local chiefs to start putting the points on the map, and then run an entire data journalism campaign","Data journalism projects can help put invisible communities or settlements into open maps
Combining drones, geo data and data visualisation can produce impact on the geographic perception of communities
The importance of digitising infrastructure that can aid in decision making including service delivery such as healthcare. ",https://guardian.ng/stories/makoko-neglected-and-unmapped/,https://pulitzercenter.org/projects/mapping-makoko,http://www.cnn.com/2020/02/26/africa/nigeria-makoko-mapping-intl/index.html,https://www.bbc.co.uk/programmes/p084qdn9,https://pulitzercenter.org/blog/behind-story-jacopo-ottaviani-john-eromosele-mapping-makoko,https://web.facebook.com/watch/?v=973781016334126,https://www.openstreetmap.org/search?query=Makoko%20Nigeria#map=18/6.49538/3.39065,https://africaopendata.org/dataset/point-of-interest-in-makoko-2019&sa=D&source=editors&ust=1612345075126000&usg=AFQjCNFJQO0LRNHzgIo8rUx76hym_jhRQg,"Code for Africa, The Guardian Nigeria, Pulitzer Centre on Crisis Reporting, Humanitarian Open Street Map Team, AfricanDRONE, Uhuru Labs",,26 Feb 2020
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,